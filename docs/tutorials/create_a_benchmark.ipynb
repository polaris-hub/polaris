{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polaris explicitly distinguished datasets from benchmarks. A benchmark defines the ML task and evaluation logic (e.g. split and metrics) for a dataset. Because of this, a single dataset can be the basis of multiple benchmarks.\n",
    "\n",
    "## Create a Benchmark\n",
    "\n",
    "To create a benchmark, you need to instantiate the `BenchmarkV2Specification` class. This requires you to specify: \n",
    "\n",
    "1. The **dataset**, which can be stored either locally or on the Hub.\n",
    "1. The **task**, where a task is defined by input and target columns.\n",
    "2. The **split**, where a split is defined by a bunch of indices.\n",
    "3. The **metric**, where a metric needs to be officially supported by Polaris.\n",
    "4. The **metadata** to contextualize your benchmark.\n",
    "\n",
    "### Define the dataset\n",
    "To learn how to create a dataset, see [this tutorial](./create_a_dataset.html). \n",
    "\n",
    "Alternatively, we can also load an existing dataset from the Hub.\n",
    "\n",
    "<div class=\"admonition warning\">\n",
    "    <p class=\"admonition-title\">Not all Hub datasets are supported</p>\n",
    "    <p>You can only create benchmarks for DatasetV2 instances, not for DatasetV1 instances. Some of the datasets stored on the Hub are still V1 datasets.</p>\n",
    "</div>\n",
    "\n",
    "### Define the task\n",
    "Currently, Polaris only supports predictive tasks. Specifying a predictive task is simply done by specifying the input and target columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_columns = [\"SMILES\"]\n",
    "target_columns = [\"LOG_SOLUBILITY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we specified just a single input and target column, but a benchmark can have multiple (e.g. a multi-task benchmark).\n",
    "\n",
    "### Define the split\n",
    "\n",
    "To ensure reproducible results, Polaris represents a split through a bunch of sets of indices.\n",
    "\n",
    "_But there is a catch_: We want Polaris to scale to extra large datasets. If we are to naively store millions of indices as lists of integers, this would impose a significant memory footprint. We therefore use bitmaps, more specifically [roaring bitmaps](https://roaringbitmap.org/) to store the splits in a memory efficient way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polaris.benchmark._split_v2 import IndexSet\n",
    "\n",
    "# To specify a set of integers, you can directly pass in a list of integers\n",
    "# This will automatically convert the indices to a BitMap\n",
    "training = IndexSet(indices=[0, 1])\n",
    "test = IndexSet(indices=[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyroaring import BitMap\n",
    "\n",
    "# Or you can create the BitMap manually and iteratively\n",
    "indices = BitMap()\n",
    "indices.add(0)\n",
    "indices.add(1)\n",
    "\n",
    "training = IndexSet(indices=indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polaris.benchmark._split_v2 import SplitV2\n",
    "\n",
    "# Finally, we create the actual split object\n",
    "split = SplitV2(training=training, test=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the metrics\n",
    "Even something as widely used as Mean Absolute Error (MAE) can be implemented in subtly different ways. Some people apply a log transform first, others might clip outliers, and sometimes an off-by-one or a bug creeps in. Over time, these variations add up. We decided to codify each metric for a Polaris benchmark in a single, transparent implementation. Our priority here is eliminating “mystery differences” that have nothing to do with actual model performance. Learn more [here](https://polarishub.io/blog/reproducible-machine-learning-in-drug-discovery-how-polaris-serves-as-a-single-source-of-truth).\n",
    "\n",
    "Specifying a metric is easy. You can simply specify its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"mean_absolute_error\", \"mean_squared_error\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify a main metric, which will be the metric used to rank the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_metric = \"mean_absolute_error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a list of all support metrics, you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polaris.evaluate._metric import DEFAULT_METRICS\n",
    "\n",
    "DEFAULT_METRICS.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create more complex metrics that wrap these base metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polaris.evaluate import Metric\n",
    "\n",
    "mae_agg = Metric(label=\"mean_absolute_error\", config={\"group_by\": \"UNIQUE_ID\", \"on_error\": \"ignore\", \"aggregation\": \"mean\"})\n",
    "metrics.append(mae_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition info\">\n",
    "    <p class=\"admonition-title\">What if my metric isn't supported yet?</p>\n",
    "    <p>Using a metric that's not supported yet, currently requires adding it to the <a href=\"https://github.com/polaris-hub/polaris\">Polaris codebase</a>. We're always looking to improve support. Reach out to us over Github and we're happy to help!</p>\n",
    "</div>\n",
    "\n",
    "### Bringing it all together\n",
    "Now we can create the `BenchmarkV2Specification` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polaris.benchmark._benchmark_v2 import BenchmarkV2Specification\n",
    "\n",
    "benchmark = BenchmarkV2Specification(\n",
    "    # 1. The dataset\n",
    "    dataset=dataset,\n",
    "    # 2. The task\n",
    "    input_cols=input_columns,\n",
    "    target_cols=target_columns,\n",
    "    # 3. The split\n",
    "    split=split,\n",
    "    # 4. The metrics\n",
    "    metrics=metrics,\n",
    "    main_metric=main_metric,\n",
    "    # 5. The metadata\n",
    "    name=\"my-first-benchmark\",\n",
    "    owner=\"your-username\", \n",
    "    description=\"Created using the Polaris tutorial\",\n",
    "    tags=[\"tutorial\"], \n",
    "    user_attributes={\"Key\": \"Value\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Share your benchmark\n",
    "Want to share your benchmark with the community? Upload it to the Polaris Hub!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark.upload_to_hub(owner=\"your-username\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to upload a new version of your benchmark, you can specify its previous version with the `parent_artifact_id` parameter. Don't forget to add a changelog describing your updates!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark.artifact_changelog = \"In this version, I added...\"\n",
    "\n",
    "benchmark.upload_to_hub(\n",
    "  owner=\"your-username\",\n",
    "  parent_artifact_id=\"your-username/my-first-benchmark\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The End."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
