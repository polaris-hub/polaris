{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "172ae3e5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"admonition abstract highlight\">\n",
    "    <p class=\"admonition-title\">In short</p>\n",
    "    <p>This tutorial walks you through the dataset and benchmark data-structures. After creating our own custom dataset and benchmark, we will learn how to upload it to the Hub!</p>\n",
    "</div>\n",
    "\n",
    "We have already seen how easy it is to load a benchmark or dataset from the Polaris Hub. Let's now learn a bit more about the underlying data model by creating our own dataset and benchmark!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "833650d7",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Note: Cell is tagged to not show up in the mkdocs build\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2621b09b",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Note: Cell is tagged to not show up in the mkdocs build\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece200dc",
   "metadata": {},
   "source": [
    "## Create the dataset\n",
    "\n",
    "A dataset in Polaris is at its core a tabular data-structure in which each row stores a single datapoint. For this example, we will process a multi-task DMPK dataset from [Fang et al.](https://doi.org/10.1021/acs.jcim.3c00160). For the sake of simplicity, we don't do any curation and will just download the dataset as-is from their Github.\n",
    "\n",
    "<div class=\"admonition warning highlight\">\n",
    "    <p class=\"admonition-title\">The importance of curation</p>\n",
    "    <p>While we do not address it in this tutorial, data curation is essential to an impactful benchmark. Because of this, we have not just made several high-quality benchmarks readily available on the Polaris Hub, but also open-sourced <a href=\"https://github.com/polaris-hub/auroris\">some of the tools</a> we've built to curate these datasets.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ecc653f-ec84-4102-aa22-cada0377c964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Internal ID</th>\n",
       "      <th>Vendor ID</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>CollectionName</th>\n",
       "      <th>LOG HLM_CLint (mL/min/kg)</th>\n",
       "      <th>LOG MDR1-MDCK ER (B-A/A-B)</th>\n",
       "      <th>LOG SOLUBILITY PH 6.8 (ug/mL)</th>\n",
       "      <th>LOG PLASMA PROTEIN BINDING (HUMAN) (% unbound)</th>\n",
       "      <th>LOG PLASMA PROTEIN BINDING (RAT) (% unbound)</th>\n",
       "      <th>LOG RLM_CLint (mL/min/kg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mol1</td>\n",
       "      <td>317714313</td>\n",
       "      <td>CNc1cc(Nc2cccn(-c3ccccn3)c2=O)nn2c(C(=O)N[C@@H...</td>\n",
       "      <td>emolecules</td>\n",
       "      <td>0.675687</td>\n",
       "      <td>1.493167</td>\n",
       "      <td>0.089905</td>\n",
       "      <td>0.991226</td>\n",
       "      <td>0.518514</td>\n",
       "      <td>1.392169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mol2</td>\n",
       "      <td>324056965</td>\n",
       "      <td>CCOc1cc2nn(CCC(C)(C)O)cc2cc1NC(=O)c1cccc(C(F)F)n1</td>\n",
       "      <td>emolecules</td>\n",
       "      <td>0.675687</td>\n",
       "      <td>1.040780</td>\n",
       "      <td>0.550228</td>\n",
       "      <td>0.099681</td>\n",
       "      <td>0.268344</td>\n",
       "      <td>1.027920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mol3</td>\n",
       "      <td>304005766</td>\n",
       "      <td>CN(c1ncc(F)cn1)[C@H]1CCCNC1</td>\n",
       "      <td>emolecules</td>\n",
       "      <td>0.675687</td>\n",
       "      <td>-0.358806</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.027920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mol4</td>\n",
       "      <td>194963090</td>\n",
       "      <td>CC(C)(Oc1ccc(-c2cnc(N)c(-c3ccc(Cl)cc3)c2)cc1)C...</td>\n",
       "      <td>emolecules</td>\n",
       "      <td>0.675687</td>\n",
       "      <td>1.026662</td>\n",
       "      <td>1.657056</td>\n",
       "      <td>-1.158015</td>\n",
       "      <td>-1.403403</td>\n",
       "      <td>1.027920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mol5</td>\n",
       "      <td>324059015</td>\n",
       "      <td>CC(C)(O)CCn1cc2cc(NC(=O)c3cccc(C(F)(F)F)n3)c(C...</td>\n",
       "      <td>emolecules</td>\n",
       "      <td>0.996380</td>\n",
       "      <td>1.010597</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.015611</td>\n",
       "      <td>1.092264</td>\n",
       "      <td>1.629093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Internal ID  Vendor ID                                             SMILES  \\\n",
       "0        Mol1  317714313  CNc1cc(Nc2cccn(-c3ccccn3)c2=O)nn2c(C(=O)N[C@@H...   \n",
       "1        Mol2  324056965  CCOc1cc2nn(CCC(C)(C)O)cc2cc1NC(=O)c1cccc(C(F)F)n1   \n",
       "2        Mol3  304005766                        CN(c1ncc(F)cn1)[C@H]1CCCNC1   \n",
       "3        Mol4  194963090  CC(C)(Oc1ccc(-c2cnc(N)c(-c3ccc(Cl)cc3)c2)cc1)C...   \n",
       "4        Mol5  324059015  CC(C)(O)CCn1cc2cc(NC(=O)c3cccc(C(F)(F)F)n3)c(C...   \n",
       "\n",
       "  CollectionName  LOG HLM_CLint (mL/min/kg)  LOG MDR1-MDCK ER (B-A/A-B)  \\\n",
       "0     emolecules                   0.675687                    1.493167   \n",
       "1     emolecules                   0.675687                    1.040780   \n",
       "2     emolecules                   0.675687                   -0.358806   \n",
       "3     emolecules                   0.675687                    1.026662   \n",
       "4     emolecules                   0.996380                    1.010597   \n",
       "\n",
       "   LOG SOLUBILITY PH 6.8 (ug/mL)  \\\n",
       "0                       0.089905   \n",
       "1                       0.550228   \n",
       "2                            NaN   \n",
       "3                       1.657056   \n",
       "4                            NaN   \n",
       "\n",
       "   LOG PLASMA PROTEIN BINDING (HUMAN) (% unbound)  \\\n",
       "0                                        0.991226   \n",
       "1                                        0.099681   \n",
       "2                                        2.000000   \n",
       "3                                       -1.158015   \n",
       "4                                        1.015611   \n",
       "\n",
       "   LOG PLASMA PROTEIN BINDING (RAT) (% unbound)  LOG RLM_CLint (mL/min/kg)  \n",
       "0                                      0.518514                   1.392169  \n",
       "1                                      0.268344                   1.027920  \n",
       "2                                      2.000000                   1.027920  \n",
       "3                                     -1.403403                   1.027920  \n",
       "4                                      1.092264                   1.629093  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "PATH = (\n",
    "    \"https://raw.githubusercontent.com/molecularinformatics/Computational-ADME/main/ADME_public_set_3521.csv\"\n",
    ")\n",
    "table = pd.read_csv(PATH)\n",
    "table.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b330cf1a-bcb8-44d1-a6d2-54f468749083",
   "metadata": {},
   "source": [
    "While not required, a good dataset will specify additional meta-data to give further explanations on the data is contained within the dataset. This can be done on both the column level and on the dataset level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3145fc25-e670-413a-8926-8ab9d6fcb3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from polaris.dataset import ColumnAnnotation\n",
    "\n",
    "# Additional meta-data on the column level\n",
    "# Of course, for a real dataset we should annotate all columns.\n",
    "annotations = {\n",
    "    \"LOG HLM_CLint (mL/min/kg)\": ColumnAnnotation(\n",
    "        desription=\"Microsomal stability\",\n",
    "        user_attributes={\"unit\": \"mL/min/kg\"},\n",
    "    ),\n",
    "    \"SMILES\": ColumnAnnotation(desription=\"Molecule SMILES string\", modality=\"molecule\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e606547-f711-4e4d-8c93-97002a8a2236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from polaris.dataset import Dataset\n",
    "from polaris.utils.types import HubOwner\n",
    "\n",
    "dataset = Dataset(\n",
    "    # The table is the core data-structure required to construct a dataset\n",
    "    table=table,\n",
    "    # Additional meta-data on the dataset level.\n",
    "    name=\"Fang_2023_DMPK\",\n",
    "    description=\"120 prospective data sets, collected over 20 months across six ADME in vitro endpoints\",\n",
    "    source=\"https://doi.org/10.1021/acs.jcim.3c00160\",\n",
    "    annotations=annotations,\n",
    "    tags=[\"DMPK\", \"ADME\"],\n",
    "    owner=HubOwner(user_id=\"cwognum\", slug=\"cwognum\"),\n",
    "    license=\"CC-BY-4.0\",\n",
    "    user_attributes={\"year\": \"2023\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bfee67-cda4-43d8-b9fd-31e8bf09de68",
   "metadata": {},
   "source": [
    "## Save and load the dataset \n",
    "\n",
    "We can now save the dataset either to a local path or directly to the hub!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c7766c1-ae85-4d0e-9757-aa5c495bc95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "temp_dir = tempfile.TemporaryDirectory().name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff8fff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datamol as dm\n",
    "\n",
    "save_dir = dm.fs.join(temp_dir, \"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b8d6aa6-9ca7-41dd-b219-9fbee813bc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = dataset.to_json(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45881174",
   "metadata": {},
   "source": [
    "Looking at the save destination, we see this created two files: A JSON with all the meta-data and a `.parquet` file with the tabular data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "114f8ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/var/folders/1y/1v1blh6x56zdn027g5g9bwph0000gr/T/tmpe_g26lrl/dataset/table.parquet',\n",
       " '/var/folders/1y/1v1blh6x56zdn027g5g9bwph0000gr/T/tmpe_g26lrl/dataset/dataset.json']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs = dm.fs.get_mapper(save_dir).fs\n",
    "fs.ls(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a0d35e",
   "metadata": {},
   "source": [
    "Loading the dataset can be done through this JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6161d30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polaris as po\n",
    "\n",
    "dataset = po.load_dataset(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637525f8-5d00-4937-8c0c-29876a309e46",
   "metadata": {},
   "source": [
    "We can also upload the dataset to the hub!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e98d9a66-dcd2-451e-8fb2-b603c344e87f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from polaris.hub.client import PolarisHubClient\n",
    "\n",
    "# NOTE: Commented out to not flood the DB\n",
    "# with PolarisHubClient() as client:\n",
    "#     client.upload_dataset(dataset=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e146f9a6",
   "metadata": {},
   "source": [
    "## Create the benchmark specification\n",
    "A benchmark is represented by the `BenchmarkSpecification`, which wraps a `Dataset` with additional data to produce a benchmark.\n",
    "\n",
    "It specifies:\n",
    "1. Which dataset to use (see Dataset);\n",
    "2. Which columns are used as input and which columns are used as target;\n",
    "3. Which metrics should be used to evaluate performance on this task;\n",
    "4. A predefined, static train-test split to use during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3313a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from polaris.benchmark import SingleTaskBenchmarkSpecification\n",
    "\n",
    "# For the sake of simplicity, we use a very simple, ordered split\n",
    "split = (np.arange(3000).tolist(), (np.arange(521) + 3000).tolist())  # train  # test\n",
    "\n",
    "benchmark = SingleTaskBenchmarkSpecification(\n",
    "    dataset=dataset,\n",
    "    target_cols=\"LOG SOLUBILITY PH 6.8 (ug/mL)\",\n",
    "    input_cols=\"SMILES\",\n",
    "    split=split,\n",
    "    metrics=\"mean_absolute_error\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560c105b-0a01-41ed-a5da-08fcf0db820d",
   "metadata": {},
   "source": [
    "Metrics should be supported in the polaris framework.\n",
    "\n",
    "For more information, see the `Metric` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "103f73d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Metric.mean_absolute_error: MetricInfo(fn=<function mean_absolute_error at 0x169779c60>, is_multitask=False)>,\n",
       " <Metric.mean_squared_error: MetricInfo(fn=<function mean_squared_error at 0x16977a020>, is_multitask=False)>,\n",
       " <Metric.accuracy: MetricInfo(fn=<function accuracy_score at 0x169758540>, is_multitask=False)>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from polaris.evaluate import Metric\n",
    "\n",
    "list(Metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0ee4c2",
   "metadata": {},
   "source": [
    "To support the vast flexibility in specifying a benchmark, we have different classes that correspond to different types of benchmarks. Each of these subclasses makes the data-model or logic more specific to a particular case. For example, trying to create a multitask benchmark with the same arguments as we used above will throw an error as there is just a single target column specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a036795",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for MultiTaskBenchmarkSpecification\ntarget_cols\n  Value error, A multi-task benchmark should specify at least two target columns [type=value_error, input_value='LOG SOLUBILITY PH 6.8 (ug/mL)', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/Users/cas.wognum/Documents/repositories/polaris/docs/tutorials/custom_dataset_benchmark.ipynb Cell 25\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cas.wognum/Documents/repositories/polaris/docs/tutorials/custom_dataset_benchmark.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpolaris\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbenchmark\u001b[39;00m \u001b[39mimport\u001b[39;00m MultiTaskBenchmarkSpecification\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/cas.wognum/Documents/repositories/polaris/docs/tutorials/custom_dataset_benchmark.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m benchmark \u001b[39m=\u001b[39m MultiTaskBenchmarkSpecification(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cas.wognum/Documents/repositories/polaris/docs/tutorials/custom_dataset_benchmark.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     dataset\u001b[39m=\u001b[39;49mdataset,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cas.wognum/Documents/repositories/polaris/docs/tutorials/custom_dataset_benchmark.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     target_cols\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mLOG SOLUBILITY PH 6.8 (ug/mL)\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cas.wognum/Documents/repositories/polaris/docs/tutorials/custom_dataset_benchmark.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     input_cols\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mSMILES\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cas.wognum/Documents/repositories/polaris/docs/tutorials/custom_dataset_benchmark.ipynb#X33sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     split\u001b[39m=\u001b[39;49msplit,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cas.wognum/Documents/repositories/polaris/docs/tutorials/custom_dataset_benchmark.ipynb#X33sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     metrics\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmean_absolute_error\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cas.wognum/Documents/repositories/polaris/docs/tutorials/custom_dataset_benchmark.ipynb#X33sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m )\n",
      "File \u001b[0;32m~/micromamba/envs/polaris/lib/python3.11/site-packages/pydantic/main.py:164\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    163\u001b[0m __tracebackhide__ \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m __pydantic_self__\u001b[39m.\u001b[39;49m__pydantic_validator__\u001b[39m.\u001b[39;49mvalidate_python(data, self_instance\u001b[39m=\u001b[39;49m__pydantic_self__)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for MultiTaskBenchmarkSpecification\ntarget_cols\n  Value error, A multi-task benchmark should specify at least two target columns [type=value_error, input_value='LOG SOLUBILITY PH 6.8 (ug/mL)', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error"
     ]
    }
   ],
   "source": [
    "from polaris.benchmark import MultiTaskBenchmarkSpecification\n",
    "\n",
    "benchmark = MultiTaskBenchmarkSpecification(\n",
    "    dataset=dataset,\n",
    "    target_cols=\"LOG SOLUBILITY PH 6.8 (ug/mL)\",\n",
    "    input_cols=\"SMILES\",\n",
    "    split=split,\n",
    "    metrics=\"mean_absolute_error\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1bae4e",
   "metadata": {},
   "source": [
    "## Save and load the benchmark\n",
    "Saving the benchmark is easy and can be done with a single line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfe454ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = dm.fs.join(temp_dir, \"benchmark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adbaf57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = benchmark.to_json(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06ca03cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/var/folders/1y/1v1blh6x56zdn027g5g9bwph0000gr/T/tmpe_g26lrl/benchmark/table.parquet',\n",
       " '/var/folders/1y/1v1blh6x56zdn027g5g9bwph0000gr/T/tmpe_g26lrl/benchmark/benchmark.json',\n",
       " '/var/folders/1y/1v1blh6x56zdn027g5g9bwph0000gr/T/tmpe_g26lrl/benchmark/dataset.json']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs = dm.fs.get_mapper(save_dir).fs\n",
    "fs.ls(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a691ae",
   "metadata": {},
   "source": [
    "This created three files. Two `json` files and a single `parquet` file. The `parquet` file saves the tabular structure at the base of the `Dataset` class, whereas the `json` files save all the meta-data for the `Dataset` and `BenchmarkSpecification`.\n",
    "\n",
    "As before, loading the benchmark can be done through the JSON file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71e3fe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = po.load_benchmark(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a88c7b9-63ab-4d33-aa46-614aa0f2bda9",
   "metadata": {},
   "source": [
    "And as before, we can also upload the benchmark directly to the hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca2d1529-3773-4caa-b12d-62865c477e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Commented out to not flood the DB\n",
    "# with PolarisHubClient() as client:\n",
    "#     client.upload_benchmark(dataset=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16789db",
   "metadata": {},
   "source": [
    "The End. \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
