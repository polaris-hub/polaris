{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Introduction","text":"<p>Welcome to the Polaris documentation!</p>"},{"location":"index.html#what-is-polaris","title":"What is Polaris?","text":"<p>Our mission</p> <p>Polaris is on a mission to bring innovators and practitioners closer together to develop methods that matter.</p> <p>Polaris is an optimistic community that fundamentally believes in the ability of Machine Learning to radically improve lives by disrupting the drug discovery process. However, we recognize that the absence of standardized, domain-appropriate datasets, guidelines, and tools for method evaluation is limiting its current impact.</p> <p>Polaris is a Python library designed to interact with the Polaris Hub. Our aim is to build the leading benchmarking platform for drug discovery, promoting the use of high-quality resources and domain-appropriate evaluation protocols. Learn more through our blog posts.</p>"},{"location":"index.html#where-to-next","title":"Where to next?","text":"<p>  Quickstart</p> <p>If you are entirely new to Polaris, this is the place to start! Learn about the essential concepts and partake in your first benchmark.</p> <p> Let's get started</p> <p>  Tutorials</p> <p>Dive deeper into the Polaris code and learn about advanced concepts to create your own benchmarks and datasets. </p> <p> Let's get started</p> <p>  API Reference</p> <p>This is where you will find the technical documentation of the code itself. Learn the intricate details of how the various methods and classes work.</p> <p> Let's get started</p> <p>  Community</p> <p>Whether you are a first-time contributor or open-source veteran, we welcome any contribution to Polaris. Learn more about our community initiatives.</p> <p> Let's get started</p>"},{"location":"quickstart.html","title":"Quickstart","text":"<p>Welcome to the Polaris Quickstart guide! This page will introduce you to core concepts and you'll submit a first result to a benchmark on the Polaris Hub.</p>"},{"location":"quickstart.html#installation","title":"Installation","text":"<p><code>polaris-lib</code> vs <code>polaris</code></p> <p>Be aware that the package name differs between pip and conda.</p> <p>Polaris can be installed via pip:</p> <pre><code>pip install polaris-lib\n</code></pre> <p>or conda:  <pre><code>conda install -c conda-forge polaris\n</code></pre></p>"},{"location":"quickstart.html#core-concepts","title":"Core concepts","text":"<p>Polaris explicitly distinguished datasets and benchmarks. </p> <ul> <li>A dataset is simply a tabular collection of data, storing datapoints in a row-wise manner. </li> <li>A benchmark defines the ML task and evaluation logic (e.g. split and metrics) for a dataset.</li> </ul> <p>One dataset can therefore be associated with multiple benchmarks. </p>"},{"location":"quickstart.html#login","title":"Login","text":"<p>To interact with the Polaris Hub from the client, you must first authenticate yourself. If you don't have an account yet, you can create one here.</p> <p>You can do this via the following command in your terminal:</p> <pre><code>polaris login\n</code></pre> <p>or in Python:  <pre><code>from polaris.hub.client import PolarisHubClient\n\nwith PolarisHubClient() as client:\n    client.login()\n</code></pre></p>"},{"location":"quickstart.html#benchmark-api","title":"Benchmark API","text":"<p>To get started, we will submit a result to the <code>polaris/hello-world-benchmark</code>.</p> <pre><code>import polaris as po\n\n# Load the benchmark from the Hub\nbenchmark = po.load_benchmark(\"polaris/hello-world-benchmark\")\n\n# Get the train and test data-loaders\ntrain, test = benchmark.get_train_test_split()\n\n# Use the training data to train your model\n# Get the input as an array with 'train.inputs' and 'train.targets'  \n# Or simply iterate over the train object.\nfor x, y in train:\n    ...\n\n# Work your magic to accurately predict the test set\npredictions = [0.0 for x in test]\n\n# Evaluate your predictions\nresults = benchmark.evaluate(predictions)\n\n# Submit your results\nresults.upload_to_hub(owner=\"dummy-user\", access=\"public\")\n</code></pre> <p>Through immutable datasets and standardized benchmarks, Polaris aims to serve as a source of truth for machine learning in drug discovery. The limited flexibility might differ from your typical experience, but this is by design to improve reproducibility. Learn more here.</p>"},{"location":"quickstart.html#dataset-api","title":"Dataset API","text":"<p>Loading a benchmark will automatically load the underlying dataset. We can also directly access the <code>polaris/hello-world</code> dataset.</p> <pre><code>import polaris as po\n\n# Load the dataset from the Hub\ndataset = po.load_dataset(\"polaris/hello-world\")\n\n# Get information on the dataset size\ndataset.size()\n\n# Load a datapoint in memory\ndataset.get_data(\n    row=dataset.rows[0],\n    col=dataset.columns[0],\n)\n\n# Or, similarly:\ndataset[dataset.rows[0], dataset.columns[0]]\n\n# Get an entire data point\ndataset[0]\n</code></pre> <p>Drug discovery research involves a maze of file formats (e.g. PDB for 3D structures, SDF for small molecules, and so on). Each format requires specialized knowledge to parse and interpret properly. At Polaris, we wanted to remove that barrier. We use a universal data format based on Zarr. Learn more here.</p>"},{"location":"quickstart.html#where-to-next","title":"Where to next?","text":"<p>Now that you've seen how easy it is to use Polaris, let's dive into the details through a set of tutorials!</p>"},{"location":"resources.html","title":"Resources","text":""},{"location":"resources.html#publications","title":"Publications","text":"<ul> <li>Correspondence in Nature Biotechnology: 10.1038/s42256-024-00911-w.</li> <li>Preprint on Method Comparison Protocols: 10.26434/chemrxiv-2024-6dbwv-v2.</li> </ul>"},{"location":"resources.html#talks","title":"Talks","text":"<ul> <li>PyData London (June, 2024): https://www.youtube.com/watch?v=YZDfD9D7mtE</li> <li>MoML (June, 2024): https://www.youtube.com/watch?v=Tsz_T1WyufI</li> </ul>"},{"location":"api/adapters.html","title":"Adapters","text":""},{"location":"api/adapters.html#polaris.dataset._adapters","title":"polaris.dataset._adapters","text":""},{"location":"api/adapters.html#polaris.dataset._adapters.Adapter","title":"Adapter","text":"<p>               Bases: <code>Enum</code></p> <p>Adapters are predefined callables that change the format of the data. Adapters are serializable and can thus be saved alongside datasets.</p> <p>Attributes:</p> Name Type Description <code>SMILES_TO_MOL</code> <p>Convert a SMILES string to a RDKit molecule.</p> <code>BYTES_TO_MOL</code> <p>Convert a RDKit binary string to a RDKit molecule.</p> <code>ARRAY_TO_PDB</code> <p>Convert a Zarr arrays to PDB arrays.</p>"},{"location":"api/base.html","title":"Base classes","text":""},{"location":"api/base.html#polaris._artifact.BaseArtifactModel","title":"polaris._artifact.BaseArtifactModel","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for all artifacts on the Hub. Specifies metadata that is used by the Hub.</p> Optional <p>Despite all artifacts basing this class, note that all attributes are optional. This ensures the library can be used without the Polaris Hub. Only when uploading to the Hub, some of the attributes are required.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>SlugCompatibleStringType | None</code> <p>A slug-compatible name for the artifact. Together with the owner, this is used by the Hub to uniquely identify the artifact.</p> <code>description</code> <code>str</code> <p>A beginner-friendly, short description of the artifact.</p> <code>tags</code> <code>list[str]</code> <p>A list of tags to categorize the artifact by. This is used by the Hub to search over artifacts.</p> <code>user_attributes</code> <code>dict[str, str]</code> <p>A dict with additional, textual user attributes.</p> <code>owner</code> <code>HubOwner | None</code> <p>A slug-compatible name for the owner of the artifact. If the artifact comes from the Polaris Hub, this is the associated owner (organization or user). Together with the name, this is used by the Hub to uniquely identify the artifact.</p> <code>polaris_version</code> <code>str</code> <p>The version of the Polaris library that was used to create the artifact.</p>"},{"location":"api/base.html#polaris._artifact.BaseArtifactModel.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(path: str) -&gt; Self\n</code></pre> <p>Loads an artifact from a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to a JSON file containing the artifact definition.</p> required"},{"location":"api/base.html#polaris._artifact.BaseArtifactModel.to_json","title":"to_json","text":"<pre><code>to_json(path: str) -&gt; None\n</code></pre> <p>Saves an artifact to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to save the artifact definition as JSON.</p> required"},{"location":"api/benchmark.html","title":"Benchmark","text":""},{"location":"api/benchmark.html#polaris.benchmark.BenchmarkV2Specification","title":"polaris.benchmark.BenchmarkV2Specification","text":"<p>               Bases: <code>SplitSpecificationV2Mixin</code>, <code>BenchmarkSpecification[BenchmarkResultsV2]</code></p>"},{"location":"api/benchmark.html#polaris.benchmark.BenchmarkV2Specification.get_train_test_split","title":"get_train_test_split","text":"<pre><code>get_train_test_split(\n    featurization_fn: Callable | None = None,\n) -&gt; tuple[Subset, dict[str, Subset]]\n</code></pre> <p>Construct the train and test sets, given the split in the benchmark specification.</p> <p>Returns <code>Subset</code> objects, which offer several ways of accessing the data and can thus easily serve as a basis to build framework-specific (e.g. PyTorch, Tensorflow) data-loaders on top of.</p> <p>Parameters:</p> Name Type Description Default <code>featurization_fn</code> <code>Callable | None</code> <p>A function to apply to the input data. If a multi-input benchmark, this function expects an input in the format specified by the <code>input_format</code> parameter.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Subset, dict[str, Subset]]</code> <p>A tuple with the train <code>Subset</code> and test <code>Subset</code> objects. If there are multiple test sets, these are returned in a dictionary and each test set has an associated name. The targets of the test set can not be accessed.</p>"},{"location":"api/benchmark.html#polaris.benchmark.BenchmarkV2Specification.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    y_pred: IncomingPredictionsType | None = None,\n    y_prob: IncomingPredictionsType | None = None,\n) -&gt; BenchmarkResultsV2\n</code></pre> <p>Execute the evaluation protocol for the benchmark, given a set of predictions.</p> What about <code>y_true</code>? <p>Contrary to other frameworks that you might be familiar with, we opted for a signature that includes just the predictions. This reduces the chance of accidentally using the test targets during training.</p> <p>For this method, we make the following assumptions:</p> <ol> <li>There can be one or multiple test set(s);</li> <li>There can be one or multiple target(s);</li> <li>The metrics are constant across test sets;</li> <li>The metrics are constant across targets;</li> <li>There can be metrics which measure across tasks.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>y_pred</code> <code>IncomingPredictionsType | None</code> <p>The predictions for the test set, as NumPy arrays. If there are multiple targets, the predictions should be wrapped in a dictionary with the target labels as keys. If there are multiple test sets, the predictions should be further wrapped in a dictionary     with the test subset labels as keys.</p> <code>None</code> <code>y_prob</code> <code>IncomingPredictionsType | None</code> <p>The predicted probabilities for the test set, formatted similarly to predictions, based on the number of tasks and test sets.</p> <code>None</code> <p>Returns:</p> Type Description <code>BenchmarkResultsV2</code> <p>A <code>BenchmarkResultsV2</code> object. This object can be directly submitted to the Polaris Hub.</p> <p>Examples:</p> <ol> <li>For regression benchmarks:     pred_scores = your_model.predict_score(molecules) # predict continuous score values     benchmark.evaluate(y_pred=pred_scores)</li> <li>For classification benchmarks:<ul> <li>If <code>roc_auc</code> and <code>pr_auc</code> are in the metric list, both class probabilities and label predictions are required:     pred_probs = your_model.predict_proba(molecules) # predict probablities     pred_labels = your_model.predict_labels(molecules) # predict class labels     benchmark.evaluate(y_pred=pred_labels, y_prob=pred_probs)</li> <li>Otherwise:     benchmark.evaluate(y_pred=pred_labels)</li> </ul> </li> </ol>"},{"location":"api/benchmark.html#polaris.benchmark.BenchmarkSpecification","title":"polaris.benchmark.BenchmarkSpecification","text":"<p>               Bases: <code>PredictiveTaskSpecificationMixin</code>, <code>BaseArtifactModel</code>, <code>BaseSplitSpecificationMixin</code>, <code>ABC</code>, <code>Generic[BenchmarkResultsType]</code></p> <p>This class wraps a dataset with additional data to specify the evaluation logic.</p> <p>Specifically, it specifies:</p> <ol> <li>Which dataset to use;</li> <li>A task definition (we currently only support predictive tasks);</li> <li>A predefined, static train-test split to use during evaluation.</li> </ol> <p>Examples:</p> <p>Basic API usage: <pre><code>import polaris as po\n\n# Load the benchmark from the Hub\nbenchmark = po.load_benchmark(\"polaris/hello-world-benchmark\")\n\n# Get the train and test data-loaders\ntrain, test = benchmark.get_train_test_split()\n\n# Use the training data to train your model\n# Get the input as an array with 'train.inputs' and 'train.targets'\n# Or simply iterate over the train object.\nfor x, y in train:\n    ...\n\n# Work your magic to accurately predict the test set\npredictions = [0.0 for x in test]\n\n# Evaluate your predictions\nresults = benchmark.evaluate(predictions)\n\n# Submit your results\nresults.upload_to_hub(owner=\"dummy-user\")\n</code></pre></p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>BaseDataset</code> <p>The dataset the benchmark specification is based on.</p> <code>readme</code> <code>str</code> <p>Markdown text that can be used to provide a formatted description of the benchmark. If using the Polaris Hub, it is worth noting that this field is more easily edited through the Hub UI as it provides a rich text editor for writing markdown.</p> <p>For additional metadata attributes, see the base classes.</p>"},{"location":"api/benchmark.html#polaris.benchmark.BenchmarkSpecification.upload_to_hub","title":"upload_to_hub","text":"<pre><code>upload_to_hub(\n    settings: PolarisHubSettings | None = None,\n    cache_auth_token: bool = True,\n    access: AccessType = \"private\",\n    owner: HubOwner | str | None = None,\n    **kwargs: dict,\n)\n</code></pre> <p>Very light, convenient wrapper around the <code>PolarisHubClient.upload_benchmark</code> method.</p>"},{"location":"api/benchmark.html#polaris.benchmark.BenchmarkSpecification.to_json","title":"to_json","text":"<pre><code>to_json(destination: str) -&gt; str\n</code></pre> <p>Save the benchmark to a destination directory as a JSON file.</p> Multiple files <p>Perhaps unintuitive, this method creates multiple files in the destination directory as it also saves the dataset it is based on to the specified destination.</p> <p>Parameters:</p> Name Type Description Default <code>destination</code> <code>str</code> <p>The directory to save the associated data to.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The path to the JSON file.</p>"},{"location":"api/competition.evaluation.html","title":"Competition.evaluation","text":""},{"location":"api/competition.evaluation.html#polaris.evaluate.CompetitionPredictions","title":"polaris.evaluate.CompetitionPredictions","text":"<p>               Bases: <code>BenchmarkPredictions</code>, <code>ResultsMetadataV1</code></p> <p>Predictions for competition benchmarks.</p> <p>This object is to be used as input to <code>PolarisHubClient.submit_competition_predictions</code>. It is used to ensure that the structure of the predictions are compatible with evaluation methods on the Polaris Hub. In addition to the predictions, it contains metadata that describes a predictions object.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>SlugCompatibleStringType</code> <p>A slug-compatible name for the artifact. It is redeclared here to be required.</p> <code>owner</code> <code>HubOwner</code> <p>A slug-compatible name for the owner of the artifact. It is redeclared here to be required.</p> <code>report_url</code> <code>HubOwner</code> <p>A URL to a report/paper/write-up which describes the methods used to generate the predictions.</p>"},{"location":"api/competition.html","title":"Competition","text":""},{"location":"api/competition.html#polaris.competition.CompetitionSpecification","title":"polaris.competition.CompetitionSpecification","text":"<p>               Bases: <code>DatasetV2</code>, <code>PredictiveTaskSpecificationMixin</code>, <code>SplitSpecificationV1Mixin</code></p> <p>An instance of this class represents a Polaris competition.</p> <p>Examples:</p> <p>Basic API usage: <pre><code>import polaris as po\n\n# Load the benchmark from the Hub\ncompetition = po.load_competition(\"dummy-user/dummy-name\")\n\n# Get the train and test data-loaders\ntrain, test = competition.get_train_test_split()\n\n# Use the training data to train your model\n# Get the input as an array with 'train.inputs' and 'train.targets'\n# Or simply iterate over the train object.\nfor x, y in train:\n    ...\n\n# Work your magic to accurately predict the test set\nprediction_values = np.array([0.0 for x in test])\n\n# Submit your predictions\ncompetition.submit_predictions(\n    prediction_name=\"first-prediction\",\n    prediction_owner=\"dummy-user\",\n    report_url=\"REPORT_URL\",\n    predictions=prediction_values,\n)\n</code></pre></p> <p>Attributes:</p> Name Type Description <code>start_time</code> <code>datetime</code> <p>The time at which the competition starts accepting prediction submissions.</p> <code>end_time</code> <code>datetime</code> <p>The time at which the competition stops accepting prediction submissions.</p> <code>n_classes</code> <code>dict[ColumnName, int | None]</code> <p>The number of classes within each target column that defines a classification task.</p> <p>For additional metadata attributes, see the base classes.</p>"},{"location":"api/competition.html#polaris.competition.CompetitionSpecification.get_train_test_split","title":"get_train_test_split","text":"<pre><code>get_train_test_split(\n    featurization_fn: Callable | None = None,\n) -&gt; tuple[Subset, Subset | dict[str, Subset]]\n</code></pre> <p>Construct the train and test sets, given the split in the competition specification.</p> <p>Returns <code>Subset</code> objects, which offer several ways of accessing the data and can thus easily serve as a basis to build framework-specific (e.g. PyTorch, Tensorflow) data-loaders on top of.</p> <p>Parameters:</p> Name Type Description Default <code>featurization_fn</code> <code>Callable | None</code> <p>A function to apply to the input data. If a multi-input benchmark, this function expects an input in the format specified by the <code>input_format</code> parameter.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Subset, Subset | dict[str, Subset]]</code> <p>A tuple with the train <code>Subset</code> and test <code>Subset</code> objects. If there are multiple test sets, these are returned in a dictionary and each test set has an associated name. The targets of the test set can not be accessed.</p>"},{"location":"api/competition.html#polaris.competition.CompetitionSpecification.submit_predictions","title":"submit_predictions","text":"<pre><code>submit_predictions(\n    predictions: IncomingPredictionsType,\n    prediction_name: SlugCompatibleStringType,\n    prediction_owner: str,\n    report_url: HttpUrlString,\n    contributors: list[HubUser] | None = None,\n    github_url: HttpUrlString | None = None,\n    description: str = \"\",\n    tags: list[str] | None = None,\n    user_attributes: dict[str, str] | None = None,\n) -&gt; None\n</code></pre> <p>Convenient wrapper around the <code>PolarisHubClient.submit_competition_predictions</code> method. It handles the creation of a standardized predictions object, which is expected by the Hub, automatically.</p> <p>Parameters:</p> Name Type Description Default <code>prediction_name</code> <code>SlugCompatibleStringType</code> <p>The name of the prediction.</p> required <code>prediction_owner</code> <code>str</code> <p>The slug of the user/organization which owns the prediction.</p> required <code>predictions</code> <code>IncomingPredictionsType</code> <p>The predictions for each test set defined in the competition.</p> required <code>report_url</code> <code>HttpUrlString</code> <p>A URL to a report/paper/write-up which describes the methods used to generate the predictions.</p> required <code>contributors</code> <code>list[HubUser] | None</code> <p>The users credited with generating these predictions.</p> <code>None</code> <code>github_url</code> <code>HttpUrlString | None</code> <p>An optional URL to a code repository containing the code used to generated these predictions.</p> <code>None</code> <code>description</code> <code>str</code> <p>An optional and short description of the predictions.</p> <code>''</code> <code>tags</code> <code>list[str] | None</code> <p>An optional list of tags to categorize the prediction by.</p> <code>None</code> <code>user_attributes</code> <code>dict[str, str] | None</code> <p>An optional dict with additional, textual user attributes.</p> <code>None</code>"},{"location":"api/converters.html","title":"Converters","text":""},{"location":"api/converters.html#polaris.dataset.converters.Converter","title":"polaris.dataset.converters.Converter","text":"<p>               Bases: <code>ABC</code></p>"},{"location":"api/converters.html#polaris.dataset.converters.Converter.convert","title":"convert  <code>abstractmethod</code>","text":"<pre><code>convert(path: str, append: bool = False) -&gt; FactoryProduct\n</code></pre> <p>This converts a file into a table and possibly annotations</p>"},{"location":"api/converters.html#polaris.dataset.converters.Converter.get_pointer","title":"get_pointer  <code>staticmethod</code>","text":"<pre><code>get_pointer(column: str, index: Union[int, slice]) -&gt; str\n</code></pre> <p>Creates a pointer.</p> <p>Parameters:</p> Name Type Description Default <code>column</code> <code>str</code> <p>The name of the column. Each column has its own group in the root.</p> required <code>index</code> <code>Union[int, slice]</code> <p>The index or slice of the pointer.</p> required"},{"location":"api/converters.html#polaris.dataset.converters.SDFConverter","title":"polaris.dataset.converters.SDFConverter","text":"<p>               Bases: <code>Converter</code></p> <p>Converts a SDF file into a Polaris dataset.</p> Binary strings for serialization <p>This class converts the molecules to binary strings (for ML purposes, this should be lossless). This might not be the most storage efficient, but is fastest and easiest to maintain. See this Github Discussion for more info.</p> <p>Properties defined on the molecule level in the SDF file can be extracted into separate columns or can be kept in the molecule object.</p> <p>Parameters:</p> Name Type Description Default <code>mol_column</code> <code>str</code> <p>The name of the column that will contain the pointers to the molecules.</p> <code>'molecule'</code> <code>smiles_column</code> <code>Optional[str]</code> <p>The name of the column that will contain the SMILES strings.</p> <code>'smiles'</code> <code>use_isomeric_smiles</code> <code>bool</code> <p>Whether to use isomeric SMILES.</p> <code>True</code> <code>mol_id_column</code> <code>Optional[str]</code> <p>The name of the column that will contain the molecule names.</p> <code>None</code> <code>mol_prop_as_cols</code> <code>bool</code> <p>Whether to extract properties defined on the molecule level in the SDF file into separate columns.</p> <code>True</code> <code>groupby_key</code> <code>Optional[str]</code> <p>The name of the column to group by. If set, the dataset can combine multiple pointers to the molecules into a single datapoint.</p> <code>None</code>"},{"location":"api/converters.html#polaris.dataset.converters.ZarrConverter","title":"polaris.dataset.converters.ZarrConverter","text":"<p>               Bases: <code>Converter</code></p> <p>Parse a .zarr archive into a Polaris <code>Dataset</code>.</p> Loading from <code>.zarr</code> <p>Loading and saving datasets from and to <code>.zarr</code> is still experimental and currently not fully supported by the Hub.</p> <p>A <code>.zarr</code> file can contain groups and arrays, where each group can again contain groups and arrays. Within Polaris, the Zarr archive is expected to have a flat hierarchy where each array corresponds to a single column and each array contains the values for all datapoints in that column.</p>"},{"location":"api/converters.html#polaris.dataset.converters.PDBConverter","title":"polaris.dataset.converters.PDBConverter","text":"<p>               Bases: <code>Converter</code></p> <p>Converts PDB files into a Polaris dataset based on fastpdb.</p> Only the most essential structural information of a protein is retained <p>This conversion saves the 3D coordinates, chain ID, residue ID, insertion code, residue name, heteroatom indicator, atom name, element, atom ID, B-factor, occupancy, and charge. Records such as CONECT (connectivity information), ANISOU (anisotropic Temperature Factors), HETATM (heteroatoms and ligands) are handled by <code>fastpdb</code>. We believe this makes for a good ML-ready format, but let us know if you require any other information to be saved.</p> PDBs as ND-arrays using <code>biotite</code> <p>To save PDBs in a Polaris-compatible format, we convert them to ND-arrays using <code>fastpdb</code> and <code>biotite</code>. We then save these ND-arrays to Zarr archives. For more info, see fastpdb and biotite</p> <p>Parameters:</p> Name Type Description Default <code>pdb_column</code> <code>str</code> <p>The name of the column that will contain the pointers to the pdbs.</p> <code>'pdb'</code> <code>n_jobs</code> <code>int</code> <p>The number of jobs to run in parallel.</p> <code>1</code> <code>zarr_chunks</code> <code>Sequence[Optional[int]]</code> <p>The chunk size for the Zarr arrays.</p> <code>(1,)</code>"},{"location":"api/converters.html#polaris.dataset.converters.PDBConverter.convert","title":"convert","text":"<pre><code>convert(path, factory: DatasetFactory, append: bool = False) -&gt; FactoryProduct\n</code></pre> <p>Convert one or a list of PDB files into Zarr</p>"},{"location":"api/dataset.html","title":"Dataset","text":""},{"location":"api/dataset.html#polaris.dataset.DatasetV2","title":"polaris.dataset.DatasetV2","text":"<p>               Bases: <code>BaseDataset</code></p> <p>Second version of a Polaris Dataset.</p> <p>This version gets rid of the DataFrame and stores all data in a Zarr archive.</p> <p>V1 stored all datapoints in a Pandas DataFrame. Because a DataFrame is always loaded to memory, this was a bottleneck when the number of data points grew large. Even with the pointer columns, you still need to load all pointers into memory. V2 therefore switches to a Zarr-only format.</p> <p>Attributes:</p> Name Type Description <code>zarr_root_path</code> <code>str</code> <p>Required path to a Zarr archive.</p> <p>For additional metadata attributes, see the base classes.</p> <p>Raises:</p> Type Description <code>InvalidDatasetError</code> <p>If the dataset does not conform to the Pydantic data-model specification.</p>"},{"location":"api/dataset.html#polaris.dataset.DatasetV2.n_rows","title":"n_rows  <code>property</code>","text":"<pre><code>n_rows: int\n</code></pre> <p>Return all row indices for the dataset</p>"},{"location":"api/dataset.html#polaris.dataset.DatasetV2.rows","title":"rows  <code>property</code>","text":"<pre><code>rows: Iterable[int]\n</code></pre> <p>Return all row indices for the dataset This feature is added for completeness' sake, but does not provide any performance benefits.</p>"},{"location":"api/dataset.html#polaris.dataset.DatasetV2.columns","title":"columns  <code>property</code>","text":"<pre><code>columns: list[str]\n</code></pre> <p>Return all columns for the dataset</p>"},{"location":"api/dataset.html#polaris.dataset.DatasetV2.dtypes","title":"dtypes  <code>property</code>","text":"<pre><code>dtypes: dict[str, dtype]\n</code></pre> <p>Return the dtype for each of the columns for the dataset</p>"},{"location":"api/dataset.html#polaris.dataset.DatasetV2.zarr_manifest_md5sum","title":"zarr_manifest_md5sum  <code>property</code> <code>writable</code>","text":"<pre><code>zarr_manifest_md5sum: str\n</code></pre> <p>Lazily compute the checksum once needed.</p> <p>The checksum of the DatasetV2 is computed from the Zarr Manifest and is not deterministic.</p>"},{"location":"api/dataset.html#polaris.dataset.DatasetV2.has_zarr_manifest_md5sum","title":"has_zarr_manifest_md5sum  <code>property</code>","text":"<pre><code>has_zarr_manifest_md5sum: bool\n</code></pre> <p>Whether the md5sum for this dataset's zarr manifest has been computed and stored.</p>"},{"location":"api/dataset.html#polaris.dataset.DatasetV2.load_zarr_root_from_hub","title":"load_zarr_root_from_hub","text":"<pre><code>load_zarr_root_from_hub()\n</code></pre> <p>Loads a Zarr archive from the Hub.</p>"},{"location":"api/dataset.html#polaris.dataset.DatasetV2.get_data","title":"get_data","text":"<pre><code>get_data(\n    row: int, col: str, adapters: dict[str, Adapter] | None = None\n) -&gt; np.ndarray | Any\n</code></pre> <p>Indexes the Zarr archive.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>int</code> <p>The index of the data to fetch.</p> required <code>col</code> <code>str</code> <p>The label of a direct child of the Zarr root.</p> required <code>adapters</code> <code>dict[str, Adapter] | None</code> <p>The adapters to apply to the data before returning it. If None, will use the default adapters specified for the dataset.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray | Any</code> <p>A numpy array with the data at the specified indices. If the column is a pointer column, the content of the referenced file is loaded to memory.</p>"},{"location":"api/dataset.html#polaris.dataset.DatasetV2.upload_to_hub","title":"upload_to_hub","text":"<pre><code>upload_to_hub(\n    access: AccessType = \"private\", owner: HubOwner | str | None = None\n)\n</code></pre> <p>Uploads the dataset to the Polaris Hub.</p>"},{"location":"api/dataset.html#polaris.dataset.DatasetV2.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(path: str)\n</code></pre> <p>Loads a dataset from a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the JSON file to load the dataset from.</p> required"},{"location":"api/dataset.html#polaris.dataset.DatasetV2.to_json","title":"to_json","text":"<pre><code>to_json(\n    destination: str | Path, if_exists: ZarrConflictResolution = \"replace\"\n) -&gt; str\n</code></pre> <p>Save the dataset to a destination directory as a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>destination</code> <code>str | Path</code> <p>The directory to save the associated data to.</p> required <code>if_exists</code> <code>ZarrConflictResolution</code> <p>Action for handling existing files in the Zarr archive. Options are 'raise' to throw an error, 'replace' to overwrite, or 'skip' to proceed without altering the existing files.</p> <code>'replace'</code> <p>Returns:</p> Type Description <code>str</code> <p>The path to the JSON file.</p>"},{"location":"api/dataset.html#polaris.dataset.DatasetV2.cache","title":"cache","text":"<pre><code>cache(\n    destination: str | PathLike | None = None,\n    if_exists: ZarrConflictResolution = \"replace\",\n) -&gt; str\n</code></pre> <p>Caches the dataset by downloading the Zarr archive to a local directory.</p> <p>Parameters:</p> Name Type Description Default <code>destination</code> <code>str | PathLike | None</code> <p>The directory to cache the data to. If None, will use the default cache directory.</p> <code>None</code> <code>if_exists</code> <code>ZarrConflictResolution</code> <p>Action for handling existing files at the destination. Options are 'raise' to throw an error, 'replace' to overwrite, or 'skip' to proceed without altering the existing files.</p> <code>'replace'</code> <p>Returns:</p> Type Description <code>str</code> <p>The path to the directory where data has been cached to.</p>"},{"location":"api/dataset.html#polaris.dataset.DatasetV2.should_verify_checksum","title":"should_verify_checksum","text":"<pre><code>should_verify_checksum(strategy: ChecksumStrategy) -&gt; bool\n</code></pre> <p>Determines whether to verify the checksum of the dataset based on the strategy.</p>"},{"location":"api/dataset.html#polaris.dataset._base.BaseDataset","title":"polaris.dataset._base.BaseDataset","text":"<p>               Bases: <code>BaseArtifactModel</code>, <code>ABC</code></p> <p>Base data-model for a Polaris dataset, implemented as a Pydantic model.</p> <p>At its core, a dataset in Polaris can conceptually be thought of as tabular data structure that stores data-points in a row-wise manner, where each column correspond to a variable associated with that datapoint.</p> <p>A Dataset can have multiple modalities or targets, can be sparse and can be part of one or multiple benchmarks.</p> <p>Attributes:</p> Name Type Description <code>default_adapters</code> <code>dict[str, Adapter]</code> <p>The adapters that the Dataset recommends to use by default to change the format of the data for specific columns.</p> <code>zarr_root_path</code> <code>str | None</code> <p>The data for any pointer column should be saved in the Zarr archive this path points to.</p> <code>readme</code> <code>str</code> <p>Markdown text that can be used to provide a formatted description of the dataset. If using the Polaris Hub, it is worth noting that this field is more easily edited through the Hub UI as it provides a rich text editor for writing markdown.</p> <code>annotations</code> <code>dict[str, ColumnAnnotation]</code> <p>Each column can be annotated with a <code>ColumnAnnotation</code> object. Importantly, this is used to annotate whether a column is a pointer column.</p> <code>source</code> <code>HttpUrlString | None</code> <p>The data source, e.g. a DOI, Github repo or URI.</p> <code>license</code> <code>SupportedLicenseType | None</code> <p>The dataset license. Polaris only supports some Creative Commons licenses. See <code>SupportedLicenseType</code> for accepted ID values.</p> <code>curation_reference</code> <code>HttpUrlString | None</code> <p>A reference to the curation process, e.g. a DOI, Github repo or URI.</p> <p>For additional metadata attributes, see the base classes.</p> <p>Raises:</p> Type Description <code>InvalidDatasetError</code> <p>If the dataset does not conform to the Pydantic data-model specification.</p>"},{"location":"api/dataset.html#polaris.dataset._base.BaseDataset.uses_zarr","title":"uses_zarr  <code>property</code>","text":"<pre><code>uses_zarr: bool\n</code></pre> <p>Whether any of the data in this dataset is stored in a Zarr Archive.</p>"},{"location":"api/dataset.html#polaris.dataset._base.BaseDataset.zarr_data","title":"zarr_data  <code>property</code>","text":"<pre><code>zarr_data\n</code></pre> <p>Get the Zarr data.</p> <p>This is different from the Zarr Root, because to optimize the efficiency of data loading, a user can choose to load the data into memory as a numpy array</p> General purpose dataloader. <p>The goal with Polaris is to provide general purpose datasets that serve as good options for a wide variety of use cases. This also implies you should be able to optimize things further for a specific use case if needed.</p>"},{"location":"api/dataset.html#polaris.dataset._base.BaseDataset.zarr_root","title":"zarr_root  <code>property</code>","text":"<pre><code>zarr_root: Group | None\n</code></pre> <p>Get the zarr Group object corresponding to the root.</p> <p>Opens the zarr archive in read-write mode if it is not already open.</p> Different to <code>zarr_data</code> <p>The <code>zarr_data</code> attribute references either to the Zarr archive or to a in-memory copy of the data. See also <code>dataset.load_to_memory()</code>.</p>"},{"location":"api/dataset.html#polaris.dataset._base.BaseDataset.n_rows","title":"n_rows  <code>property</code>","text":"<pre><code>n_rows: int\n</code></pre> <p>The number of rows in the dataset.</p>"},{"location":"api/dataset.html#polaris.dataset._base.BaseDataset.n_columns","title":"n_columns  <code>property</code>","text":"<pre><code>n_columns: int\n</code></pre> <p>The number of columns in the dataset.</p>"},{"location":"api/dataset.html#polaris.dataset._base.BaseDataset.rows","title":"rows  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>rows: Iterable[str | int]\n</code></pre> <p>Return all row indices for the dataset</p>"},{"location":"api/dataset.html#polaris.dataset._base.BaseDataset.columns","title":"columns  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>columns: list[str]\n</code></pre> <p>Return all columns for the dataset</p>"},{"location":"api/dataset.html#polaris.dataset._base.BaseDataset.dtypes","title":"dtypes  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>dtypes: dict[str, dtype]\n</code></pre> <p>Return the dtype for each of the columns for the dataset</p>"},{"location":"api/dataset.html#polaris.dataset._base.BaseDataset.load_zarr_root_from_hub","title":"load_zarr_root_from_hub  <code>abstractmethod</code>","text":"<pre><code>load_zarr_root_from_hub()\n</code></pre> <p>Loads a Zarr archive from the Polaris Hub.</p>"},{"location":"api/dataset.html#polaris.dataset._base.BaseDataset.load_zarr_root_from_local","title":"load_zarr_root_from_local","text":"<pre><code>load_zarr_root_from_local()\n</code></pre> <p>Loads a locally stored Zarr archive.</p> <p>We use memory mapping by default because our experiments show that it's consistently faster</p>"},{"location":"api/dataset.html#polaris.dataset._base.BaseDataset.load_to_memory","title":"load_to_memory","text":"<pre><code>load_to_memory()\n</code></pre> <p>Load data from zarr files to memeory</p> Make sure the uncompressed dataset fits in-memory. <p>This method will load the uncompressed dataset into memory. Make sure you actually have enough memory to store the dataset.</p>"},{"location":"api/dataset.html#polaris.dataset._base.BaseDataset.get_data","title":"get_data  <code>abstractmethod</code>","text":"<pre><code>get_data(\n    row: str | int, col: str, adapters: dict[str, Adapter] | None = None\n) -&gt; np.ndarray | Any\n</code></pre> <p>Since the dataset might contain pointers to external files, data retrieval is more complicated than just indexing the <code>table</code> attribute. This method provides an end-point for seamlessly accessing the underlying data.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>str | int</code> <p>The row index in the <code>Dataset.table</code> attribute</p> required <code>col</code> <code>str</code> <p>The column index in the <code>Dataset.table</code> attribute</p> required <code>adapters</code> <code>dict[str, Adapter] | None</code> <p>The adapters to apply to the data before returning it. If None, will use the default adapters specified for the dataset.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray | Any</code> <p>A numpy array with the data at the specified indices. If the column is a pointer column, the content of the referenced file is loaded to memory.</p>"},{"location":"api/dataset.html#polaris.dataset._base.BaseDataset.upload_to_hub","title":"upload_to_hub  <code>abstractmethod</code>","text":"<pre><code>upload_to_hub(\n    access: AccessType = \"private\", owner: HubOwner | str | None = None\n)\n</code></pre> <p>Uploads the dataset to the Polaris Hub.</p>"},{"location":"api/dataset.html#polaris.dataset._base.BaseDataset.from_json","title":"from_json  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>from_json(path: str)\n</code></pre> <p>Loads a dataset from a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the JSON file to load the dataset from.</p> required"},{"location":"api/dataset.html#polaris.dataset._base.BaseDataset.to_json","title":"to_json  <code>abstractmethod</code>","text":"<pre><code>to_json(\n    destination: str | Path, if_exists: ZarrConflictResolution = \"replace\"\n) -&gt; str\n</code></pre> <p>Save the dataset to a destination directory as a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>destination</code> <code>str | Path</code> <p>The directory to save the associated data to.</p> required <code>if_exists</code> <code>ZarrConflictResolution</code> <p>Action for handling existing files in the Zarr archive. Options are 'raise' to throw an error, 'replace' to overwrite, or 'skip' to proceed without altering the existing files.</p> <code>'replace'</code> <p>Returns:</p> Type Description <code>str</code> <p>The path to the JSON file.</p>"},{"location":"api/dataset.html#polaris.dataset.ColumnAnnotation","title":"polaris.dataset.ColumnAnnotation","text":"<p>               Bases: <code>BaseModel</code></p> <p>The <code>ColumnAnnotation</code> class is used to annotate the columns of the  object. This mostly just stores metadata and does not affect the logic. The exception is the <code>is_pointer</code> attribute.</p> <p>Attributes:</p> Name Type Description <code>is_pointer</code> <code>bool</code> <p>Annotates whether a column is a pointer column. If so, it does not contain data, but rather contains references to blobs of data from which the data is loaded.</p> <code>modality</code> <code>Modality</code> <p>The data modality describes the data type and is used to categorize datasets on the Hub and while it does not affect logic in this library, it does affect the logic of the Hub.</p> <code>description</code> <code>str | None</code> <p>Describes how the data was generated.</p> <code>user_attributes</code> <code>dict[str, str]</code> <p>Any additional metadata can be stored in the user attributes.</p> <code>content_type</code> <code>KnownContentType | str | None</code> <p>Specify column's IANA content type. If the the content type matches with a known type for molecules (e.g. \"chemical/x-smiles\"), visualization for its content will be activated on the Hub side</p>"},{"location":"api/dataset.html#polaris.dataset.zarr","title":"polaris.dataset.zarr","text":""},{"location":"api/dataset.html#polaris.dataset.zarr.ZarrFileChecksum","title":"ZarrFileChecksum","text":"<p>               Bases: <code>BaseModel</code></p> <p>This data is sent to the Hub to verify the integrity of the Zarr archive on upload.</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>str</code> <p>The path of the file relative to the Zarr root.</p> <code>md5sum</code> <code>str</code> <p>The md5sum of the file.</p> <code>size</code> <code>int</code> <p>The size of the file in bytes.</p>"},{"location":"api/dataset.html#polaris.dataset.zarr.MemoryMappedDirectoryStore","title":"MemoryMappedDirectoryStore","text":"<p>               Bases: <code>DirectoryStore</code></p> <p>A Zarr Store to open chunks as memory-mapped files. See also this Github issue.</p> <p>Memory mapping leverages low-level OS functionality to reduce the time it takes to read the content of a file by directly mapping to memory.</p>"},{"location":"api/dataset.html#polaris.dataset.zarr.compute_zarr_checksum","title":"compute_zarr_checksum","text":"<pre><code>compute_zarr_checksum(\n    zarr_root_path: str,\n) -&gt; Tuple[_ZarrDirectoryDigest, List[ZarrFileChecksum]]\n</code></pre> <p>Implements an algorithm to compute the Zarr checksum.</p> This checksum is sensitive to Zarr configuration. <p>This checksum is sensitive to change in the Zarr structure. For example, if you change the chunk size, the checksum will also change.</p> <p>To understand how this works, consider the following directory structure:</p> <pre><code>       . (root)\n      / \\\n     a   c\n    /\n   b\n</code></pre> <p>Within zarr, this would for example be:</p> <ul> <li><code>root</code>: A Zarr Group with a single Array.</li> <li><code>a</code>: A Zarr Array</li> <li><code>b</code>: A single chunk of the Zarr Array</li> <li><code>c</code>: A metadata file (i.e. .zarray, .zattrs or .zgroup)</li> </ul> <p>To compute the checksum, we first find all the trees in the node, in this case b and c. We compute the hash of the content (the raw bytes) for each of these files.</p> <p>We then work our way up the tree. For any node (directory), we find all children of that node. In an sorted order, we then serialize a list with - for each of the children - the checksum, size, and number of children. The hash of the directory is then equal to the hash of the serialized JSON.</p> <p>The Polaris implementation is heavily based on the <code>zarr-checksum</code> package. This method is the biggest deviation of the original code.</p>"},{"location":"api/dataset.html#polaris.dataset.zarr.generate_zarr_manifest","title":"generate_zarr_manifest","text":"<pre><code>generate_zarr_manifest(zarr_root_path: str, output_dir: str) -&gt; str\n</code></pre> <p>Entry point function which triggers the creation of a Zarr manifest for a V2 dataset.</p> <p>Parameters:</p> Name Type Description Default <code>zarr_root_path</code> <code>str</code> <p>The path to the root of a Zarr archive</p> required <code>output_dir</code> <code>str</code> <p>The path to the directory which will hold the generated manifest</p> required"},{"location":"api/evaluation.html","title":"Evaluation","text":""},{"location":"api/evaluation.html#polaris.evaluate.BenchmarkPredictions","title":"polaris.evaluate.BenchmarkPredictions","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base model to represent predictions in the Polaris code base.</p> <p>Guided by Postel's Law, this class normalizes different formats to a single, internal representation.</p> <p>Attributes:</p> Name Type Description <code>predictions</code> <code>PredictionsType</code> <p>The predictions for the benchmark.</p> <code>target_labels</code> <code>list[str]</code> <p>The target columns for the associated benchmark.</p> <code>test_set_labels</code> <code>list[str]</code> <p>The names of the test sets for the associated benchmark.</p> <code>test_set_sizes</code> <code>dict[str, int]</code> <p>The number of rows in each test set for the associated benchmark.</p>"},{"location":"api/evaluation.html#polaris.evaluate.BenchmarkPredictions.check_test_set_size","title":"check_test_set_size","text":"<pre><code>check_test_set_size() -&gt; Self\n</code></pre> <p>Verify that the size of all predictions</p>"},{"location":"api/evaluation.html#polaris.evaluate.BenchmarkPredictions.get_subset","title":"get_subset","text":"<pre><code>get_subset(\n    test_set_subset: list[str] | None = None,\n    target_subset: list[str] | None = None,\n) -&gt; BenchmarkPredictions\n</code></pre> <p>Return a subset of the original predictions</p>"},{"location":"api/evaluation.html#polaris.evaluate.BenchmarkPredictions.get_size","title":"get_size","text":"<pre><code>get_size(\n    test_set_subset: list[str] | None = None,\n    target_subset: list[str] | None = None,\n) -&gt; int\n</code></pre> <p>Return the total number of predictions, allowing for filtering by test set and target</p>"},{"location":"api/evaluation.html#polaris.evaluate.BenchmarkPredictions.flatten","title":"flatten","text":"<pre><code>flatten() -&gt; np.ndarray\n</code></pre> <p>Return the predictions as a single, flat numpy array</p>"},{"location":"api/evaluation.html#polaris.evaluate.BenchmarkPredictions.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Return the total number of predictions</p>"},{"location":"api/evaluation.html#polaris.evaluate.ResultsMetadata","title":"polaris.evaluate.ResultsMetadata","text":"<p>               Bases: <code>BaseArtifactModel</code></p> <p>V1 implementation of evaluation results without model field support</p> <p>Attributes:</p> Name Type Description <code>github_url</code> <code>HttpUrlString | None</code> <p>The URL to the code repository that was used to generate these results.</p> <code>paper_url</code> <code>HttpUrlString | None</code> <p>The URL to the paper describing the methodology used to generate these results.</p> <code>contributors</code> <code>list[HubUser]</code> <p>The users that are credited for these results.</p> <p>For additional metadata attributes, see the base classes.</p>"},{"location":"api/evaluation.html#polaris.evaluate.EvaluationResult","title":"polaris.evaluate.EvaluationResult","text":"<p>               Bases: <code>ResultsMetadataV1</code>, <code>BaseEvaluationResult</code></p> <p>V1 implementation of evaluation results without model field support</p>"},{"location":"api/evaluation.html#polaris.evaluate.BenchmarkResults","title":"polaris.evaluate.BenchmarkResults","text":"<p>               Bases: <code>EvaluationResultV1</code>, <code>BaseBenchmarkResults</code></p> <p>V1 implementation of benchmark results without model field support</p>"},{"location":"api/evaluation.html#polaris.evaluate.MetricInfo","title":"polaris.evaluate.MetricInfo","text":"<p>               Bases: <code>BaseModel</code></p> <p>Metric metadata</p> <p>Attributes:</p> Name Type Description <code>fn</code> <code>Callable</code> <p>The callable that actually computes the metric.</p> <code>is_multitask</code> <code>bool</code> <p>Whether the metric expects a single set of predictions or a dict of predictions.</p> <code>kwargs</code> <code>dict</code> <p>Additional parameters required for the metric.</p> <code>direction</code> <code>DirectionType</code> <p>The direction for ranking of the metric,  \"max\" for maximization and \"min\" for minimization.</p> <code>y_type</code> <code>PredictionKwargs</code> <p>The type of predictions expected by the metric interface.</p>"},{"location":"api/evaluation.html#polaris.evaluate.Metric","title":"polaris.evaluate.Metric","text":"<p>               Bases: <code>BaseModel</code></p> <p>A Metric in Polaris.</p> <p>A metric consists of a default metric, which is a callable labeled with additional metadata, as well as a config. The config can change how the metric is computed, for example by grouping the data before computing the metric.</p> <p>Attributes:</p> Name Type Description <code>label</code> <code>MetricLabel</code> <p>The actual callable that is at the core of the metric implementation.</p> <code>custom_name</code> <code>str | None</code> <p>A optional, custom name of the metric. Names should be unique within the context of a benchmark.</p> <code>config</code> <code>GroupedMetricConfig | None</code> <p>For more complex metrics, this object should hold all parameters for the metric.</p> <code>fn</code> <code>Callable</code> <p>The callable that actually computes the metric, automatically set based on the label.</p> <code>is_multitask</code> <code>bool</code> <p>Whether the metric expects a single set of predictions or a dict of predictions, automatically set based on the label.</p> <code>kwargs</code> <code>dict</code> <p>Additional parameters required for the metric, automatically set based on the label.</p> <code>direction</code> <code>DirectionType</code> <p>The direction for ranking of the metric,  \"max\" for maximization and \"min\" for minimization, automatically set based on the label.</p> <code>y_type</code> <code>PredictionKwargs</code> <p>The type of predictions expected by the metric interface, automatically set based on the label.</p>"},{"location":"api/evaluation.html#polaris.evaluate.Metric.score","title":"score","text":"<pre><code>score(\n    y_true: GroundTruth,\n    y_pred: BenchmarkPredictions | None = None,\n    y_prob: BenchmarkPredictions | None = None,\n) -&gt; float\n</code></pre> <p>Compute the metric.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>GroundTruth</code> <p>The true target values.</p> required <code>y_pred</code> <code>BenchmarkPredictions | None</code> <p>The predicted target values, if any.</p> <code>None</code> <code>y_prob</code> <code>BenchmarkPredictions | None</code> <p>The predicted target probabilities, if any.</p> <code>None</code>"},{"location":"api/evaluation.html#polaris.evaluate.metrics.generic_metrics","title":"polaris.evaluate.metrics.generic_metrics","text":""},{"location":"api/evaluation.html#polaris.evaluate.metrics.generic_metrics.pearsonr","title":"pearsonr","text":"<pre><code>pearsonr(y_true: np.ndarray, y_pred: np.ndarray)\n</code></pre> <p>Calculate a pearson r correlation</p>"},{"location":"api/evaluation.html#polaris.evaluate.metrics.generic_metrics.spearman","title":"spearman","text":"<pre><code>spearman(y_true: np.ndarray, y_pred: np.ndarray)\n</code></pre> <p>Calculate a Spearman correlation</p>"},{"location":"api/evaluation.html#polaris.evaluate.metrics.generic_metrics.absolute_average_fold_error","title":"absolute_average_fold_error","text":"<pre><code>absolute_average_fold_error(y_true: np.ndarray, y_pred: np.ndarray) -&gt; float\n</code></pre> <p>Calculate the Absolute Average Fold Error (AAFE) metric. It measures the fold change between predicted values and observed values. The implementation is based on this paper.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The true target values of shape (n_samples,)</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted target values of shape (n_samples,).</p> required <p>Returns:</p> Name Type Description <code>aafe</code> <code>float</code> <p>The Absolute Average Fold Error.</p>"},{"location":"api/evaluation.html#polaris.evaluate.metrics.generic_metrics.cohen_kappa_score","title":"cohen_kappa_score","text":"<pre><code>cohen_kappa_score(y_true, y_pred, **kwargs)\n</code></pre> <p>Scikit learn cohen_kappa_score wraper with renamed arguments</p>"},{"location":"api/evaluation.html#polaris.evaluate.metrics.generic_metrics.average_precision_score","title":"average_precision_score","text":"<pre><code>average_precision_score(y_true, y_score, **kwargs)\n</code></pre> <p>Scikit learn average_precision_score wrapper that throws an error if y_true has no positive class</p>"},{"location":"api/evaluation.html#polaris.evaluate.metrics.docking_metrics","title":"polaris.evaluate.metrics.docking_metrics","text":""},{"location":"api/evaluation.html#polaris.evaluate.metrics.docking_metrics.rmsd_coverage","title":"rmsd_coverage","text":"<pre><code>rmsd_coverage(\n    y_pred: Union[str, List[dm.Mol]],\n    y_true: Union[str, list[dm.Mol]],\n    max_rsmd: float = 2,\n)\n</code></pre> <p>Calculate the coverage of molecules with an RMSD less than a threshold (2 \u00c5 by default) compared to the reference molecule conformer.</p> <p>It is assumed that the predicted binding conformers are extracted from the docking output, where the receptor (protein) coordinates have been aligned with the original crystal structure.</p> <p>Attributes:</p> Name Type Description <code>y_pred</code> <p>List of predicted binding conformers.</p> <code>y_true</code> <p>List of ground truth binding confoermers.</p> <code>max_rsmd</code> <p>The threshold for determining acceptable rsmd.</p>"},{"location":"api/factory.html","title":"Factory","text":""},{"location":"api/factory.html#polaris.dataset.DatasetFactory","title":"polaris.dataset.DatasetFactory","text":"<p>The <code>DatasetFactory</code> makes it easier to create complex datasets.</p> <p>It is based on the factory design pattern and allows a user to specify specific handlers (i.e. <code>Converter</code> objects) for different file types. These converters are used to convert commonly used file types in drug discovery to something that can be used within Polaris while losing as little information as possible.</p> <p>In addition, it contains utility method to incrementally build out a dataset from different sources.</p> Try quickly converting one of your datasets <p>The <code>DatasetFactory</code> is designed to give you full control. If your dataset is saved in a single file and you don't need anything fancy, you can try use <code>create_dataset_from_file</code> instead.</p> <pre><code>from polaris.dataset import create_dataset_from_file\ndataset = create_dataset_from_file(\"path/to/my_dataset.sdf\")\n</code></pre> How to make adding metadata easier? <p>The <code>DatasetFactory</code> is designed to more easily pull together data from different sources. However, adding metadata remains a laborious process. How could we make this simpler through the Python API?</p>"},{"location":"api/factory.html#polaris.dataset.DatasetFactory.zarr_root_path","title":"zarr_root_path  <code>property</code>","text":"<pre><code>zarr_root_path: Group\n</code></pre> <p>The root of the zarr archive for the Dataset that is being built. All data for a single dataset is expected to be stored in the same Zarr archive.</p>"},{"location":"api/factory.html#polaris.dataset.DatasetFactory.zarr_root","title":"zarr_root  <code>property</code>","text":"<pre><code>zarr_root: Group\n</code></pre> <p>The root of the zarr archive for the Dataset that is being built. All data for a single dataset is expected to be stored in the same Zarr archive.</p>"},{"location":"api/factory.html#polaris.dataset.DatasetFactory.register_converter","title":"register_converter","text":"<pre><code>register_converter(ext: str, converter: Converter)\n</code></pre> <p>Registers a new converter for a specific file type.</p> <p>Parameters:</p> Name Type Description Default <code>ext</code> <code>str</code> <p>The file extension for which the converter should be used. There can only be a single converter per file extension.</p> required <code>converter</code> <code>Converter</code> <p>The handler for the file type. This should convert the file to a Polaris-compatible format.</p> required"},{"location":"api/factory.html#polaris.dataset.DatasetFactory.add_column","title":"add_column","text":"<pre><code>add_column(\n    column: pd.Series,\n    annotation: ColumnAnnotation | None = None,\n    adapters: Adapter | None = None,\n)\n</code></pre> <p>Add a single column to the DataFrame</p> <p>We require:</p> <ol> <li>The name attribute of the column to be set.</li> <li>The name attribute of the column to be unique.</li> <li>If the column is a pointer column, the <code>zarr_root_path</code> needs to be set.</li> <li>The length of the column to match the length of the already constructed table.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>column</code> <code>Series</code> <p>The column to add to the dataset.</p> required <code>annotation</code> <code>ColumnAnnotation | None</code> <p>The annotation for the column. If None, a default annotation will be used.</p> <code>None</code>"},{"location":"api/factory.html#polaris.dataset.DatasetFactory.add_columns","title":"add_columns","text":"<pre><code>add_columns(\n    df: pd.DataFrame,\n    annotations: dict[str, ColumnAnnotation] | None = None,\n    adapters: dict[str, Adapter] | None = None,\n    merge_on: str | None = None,\n)\n</code></pre> <p>Add multiple columns to the dataset based on another dataframe.</p> <p>To have more control over how the two dataframes are combined, you can specify a column to merge on. This will always do an outer join.</p> <p>If not specifying a key to merge on, the columns will simply be added to the dataset that has been built so far without any reordering. They are therefore expected to meet all the same expectations as for <code>add_column()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Pandas DataFrame with the columns that we want to add to the dataset.</p> required <code>annotations</code> <code>dict[str, ColumnAnnotation] | None</code> <p>The annotations for the columns. If None, default annotations will be used.</p> <code>None</code> <code>merge_on</code> <code>str | None</code> <p>The column to merge on, if any.</p> <code>None</code>"},{"location":"api/factory.html#polaris.dataset.DatasetFactory.add_from_file","title":"add_from_file","text":"<pre><code>add_from_file(path: str)\n</code></pre> <p>Uses the registered converters to parse the data from a specific file and add it to the dataset. If no converter is found for the file extension, it raises an error.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the file that should be parsed.</p> required"},{"location":"api/factory.html#polaris.dataset.DatasetFactory.add_from_files","title":"add_from_files","text":"<pre><code>add_from_files(paths: list[str], axis: Literal[0, 1, 'index', 'columns'])\n</code></pre> <p>Uses the registered converters to parse the data from a specific files and add them to the dataset. If no converter is found for the file extension, it raises an error.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>list[str]</code> <p>The list of paths that should be parsed.</p> required <code>axis</code> <code>Literal[0, 1, 'index', 'columns']</code> <p>Axis along which the files should be added. - 0 or 'index': append the rows with files. Files must be of the same type. - 1 or 'columns': append the columns with files. Files can be of the different types.</p> required"},{"location":"api/factory.html#polaris.dataset.DatasetFactory.build","title":"build","text":"<pre><code>build() -&gt; DatasetV1\n</code></pre> <p>Returns a Dataset based on the current state of the factory.</p>"},{"location":"api/factory.html#polaris.dataset.DatasetFactory.reset","title":"reset","text":"<pre><code>reset(zarr_root_path: str | None = None)\n</code></pre> <p>Resets the factory to its initial state to start building the next dataset from scratch. Note that this will not reset the registered converters.</p> <p>Parameters:</p> Name Type Description Default <code>zarr_root_path</code> <code>str | None</code> <p>The root path of the zarr hierarchy. If you want to use pointer columns for your next dataset, this arguments needs to be passed.</p> <code>None</code>"},{"location":"api/factory.html#polaris.dataset.create_dataset_from_file","title":"polaris.dataset.create_dataset_from_file","text":"<pre><code>create_dataset_from_file(\n    path: str, zarr_root_path: str | None = None\n) -&gt; DatasetV1\n</code></pre> <p>This function is a convenience function to create a dataset from a file.</p> <p>It sets up the dataset factory with sensible defaults for the converters. For creating more complicated datasets, please use the <code>DatasetFactory</code> directly.</p>"},{"location":"api/factory.html#polaris.dataset.create_dataset_from_files","title":"polaris.dataset.create_dataset_from_files","text":"<pre><code>create_dataset_from_files(\n    paths: list[str],\n    zarr_root_path: str | None = None,\n    axis: Literal[0, 1, \"index\", \"columns\"] = 0,\n) -&gt; DatasetV1\n</code></pre> <p>This function is a convenience function to create a dataset from multiple files.</p> <p>It sets up the dataset factory with sensible defaults for the converters. For creating more complicated datasets, please use the <code>DatasetFactory</code> directly.</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>Literal[0, 1, 'index', 'columns']</code> <p>Axis along which the files should be added. - 0 or 'index': append the rows with files. Files must be of the same type. - 1 or 'columns': append the columns with files. Files can be of the different types.</p> <code>0</code>"},{"location":"api/hub.client.html","title":"Client","text":""},{"location":"api/hub.client.html#polaris.hub.settings.PolarisHubSettings","title":"polaris.hub.settings.PolarisHubSettings","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Settings for the OAuth2 Polaris Hub API Client.</p> Secrecy of these settings <p>Since the Polaris Hub uses PCKE (Proof Key for Code Exchange) for OAuth2, these values thus do not have to be kept secret. See RFC 7636 for more info.</p> <p>Attributes:</p> Name Type Description <code>hub_url</code> <code>HttpUrlString</code> <p>The URL to the main page of the Polaris Hub.</p> <code>api_url</code> <code>HttpUrlString | None</code> <p>The URL to the main entrypoint of the Polaris API.</p> <code>authorize_url</code> <code>HttpUrlString</code> <p>The URL of the OAuth2 authorization endpoint.</p> <code>callback_url</code> <code>HttpUrlString | None</code> <p>The URL to which the user is redirected after authorization.</p> <code>token_fetch_url</code> <code>HttpUrlString</code> <p>The URL of the OAuth2 token endpoint.</p> <code>user_info_url</code> <code>HttpUrlString</code> <p>The URL of the OAuth2 user info endpoint.</p> <code>scopes</code> <code>str</code> <p>The OAuth2 scopes that are requested.</p> <code>client_id</code> <code>str</code> <p>The OAuth2 client ID.</p> <code>ca_bundle</code> <code>Union[str, bool, None]</code> <p>The path to a CA bundle file for requests. Allows for custom SSL certificates to be used.</p> <code>default_timeout</code> <code>TimeoutTypes</code> <p>The default timeout for requests.</p> <code>hub_token_url</code> <code>HttpUrlString | None</code> <p>The URL of the Polaris Hub token endpoint. A default value is generated based on the Hub URL, and this should not need to be overridden.</p> <code>username</code> <code>str | None</code> <p>The username for the Polaris Hub, for the optional password-based authentication.</p> <code>password</code> <code>str | None</code> <p>The password for the specified username.</p>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient","title":"polaris.hub.client.PolarisHubClient","text":"<pre><code>PolarisHubClient(\n    settings: PolarisHubSettings | None = None,\n    cache_auth_token: bool = True,\n    **kwargs: dict,\n)\n</code></pre> <p>               Bases: <code>OAuth2Client</code></p> <p>A client for the Polaris Hub API. The Polaris Hub is a central repository of datasets, benchmarks and results. Visit it here: https://polarishub.io/.</p> <p>Bases the <code>authlib</code> client, which in turns bases the <code>httpx</code> client. See the relevant docs to learn more about how to use these clients outside of the integration with the Polaris Hub.</p> Closing the client <p>The client should be closed after all requests have been made. For convenience, you can also use the client as a context manager to automatically close the client when the context is exited. Note that once the client has been closed, it cannot be used anymore.</p> <pre><code># Make sure to close the client once finished\nclient = PolarisHubClient()\nclient.get(...)\nclient.close()\n\n# Or use the client as a context manager\nwith PolarisHubClient() as client:\n    client.get(...)\n</code></pre> Interacting with artifacts owned by an organization <p>Soon after being added to a new organization on Polaris, there may be a delay spanning some minutes where you cannot upload/download artifacts where the aforementioned organization is the owner. If this occurs, please re-login via <code>polaris login --overwrite</code> and try again.</p> Async Client <p><code>authlib</code> also supports an async client. Since we don't expect to make multiple requests to the Hub in parallel and due to the added complexity stemming from using the Python asyncio API, we are sticking to the sync client - at least for now.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>PolarisHubSettings | None</code> <p>A <code>PolarisHubSettings</code> instance.</p> <code>None</code> <code>cache_auth_token</code> <code>bool</code> <p>Whether to cache the auth token to a file.</p> <code>True</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to the authlib <code>OAuth2Client</code> constructor.</p> <code>{}</code>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.get_metadata_from_response","title":"get_metadata_from_response","text":"<pre><code>get_metadata_from_response(response: Response, key: str) -&gt; str | None\n</code></pre> <p>Get custom metadata saved to the R2 object from the headers.</p>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.login","title":"login","text":"<pre><code>login(overwrite: bool = False, auto_open_browser: bool = True)\n</code></pre> <p>Login to the Polaris Hub using the OAuth2 protocol.</p> Headless authentication <p>It is currently not possible to login to the Polaris Hub without a browser. See this Github issue for more info.</p> <p>Parameters:</p> Name Type Description Default <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the current token if the user is already logged in.</p> <code>False</code> <code>auto_open_browser</code> <code>bool</code> <p>Whether to automatically open the browser to visit the authorization URL.</p> <code>True</code>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.list_datasets","title":"list_datasets","text":"<pre><code>list_datasets(limit: int = 100, offset: int = 0) -&gt; list[str]\n</code></pre> <p>List all available datasets (v1 and v2) on the Polaris Hub. We prioritize v2 datasets over v1 datasets.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>The maximum number of datasets to return.</p> <code>100</code> <code>offset</code> <code>int</code> <p>The offset from which to start returning datasets.</p> <code>0</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of dataset names in the format <code>owner/dataset_name</code>.</p>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(\n    owner: str | HubOwner,\n    name: str,\n    verify_checksum: ChecksumStrategy = \"verify_unless_zarr\",\n) -&gt; DatasetV1 | DatasetV2\n</code></pre> <p>Load a standard dataset from the Polaris Hub.</p> <p>Parameters:</p> Name Type Description Default <code>owner</code> <code>str | HubOwner</code> <p>The owner of the dataset. Can be either a user or organization from the Polaris Hub.</p> required <code>name</code> <code>str</code> <p>The name of the dataset.</p> required <code>verify_checksum</code> <code>ChecksumStrategy</code> <p>Whether to use the checksum to verify the integrity of the dataset. If None, will infer a practical default based on the dataset's storage location.</p> <code>'verify_unless_zarr'</code> <p>Returns:</p> Type Description <code>DatasetV1 | DatasetV2</code> <p>A <code>Dataset</code> instance, if it exists.</p>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.list_benchmarks","title":"list_benchmarks","text":"<pre><code>list_benchmarks(limit: int = 100, offset: int = 0) -&gt; list[str]\n</code></pre> <p>List all available benchmarks on the Polaris Hub.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>The maximum number of benchmarks to return.</p> <code>100</code> <code>offset</code> <code>int</code> <p>The offset from which to start returning benchmarks.</p> <code>0</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of benchmark names in the format <code>owner/benchmark_name</code>.</p>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.get_benchmark","title":"get_benchmark","text":"<pre><code>get_benchmark(\n    owner: str | HubOwner,\n    name: str,\n    verify_checksum: ChecksumStrategy = \"verify_unless_zarr\",\n) -&gt; BenchmarkV1Specification | BenchmarkV2Specification\n</code></pre> <p>Load a benchmark from the Polaris Hub.</p> <p>Parameters:</p> Name Type Description Default <code>owner</code> <code>str | HubOwner</code> <p>The owner of the benchmark. Can be either a user or organization from the Polaris Hub.</p> required <code>name</code> <code>str</code> <p>The name of the benchmark.</p> required <code>verify_checksum</code> <code>ChecksumStrategy</code> <p>Whether to use the checksum to verify the integrity of the benchmark.</p> <code>'verify_unless_zarr'</code> <p>Returns:</p> Type Description <code>BenchmarkV1Specification | BenchmarkV2Specification</code> <p>A <code>BenchmarkSpecification</code> instance, if it exists.</p>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.upload_results","title":"upload_results","text":"<pre><code>upload_results(\n    results: BenchmarkResults,\n    access: AccessType = \"private\",\n    owner: HubOwner | str | None = None,\n)\n</code></pre> <p>Upload the results to the Polaris Hub.</p> Owner <p>The owner of the results will automatically be inferred by the Hub from the user making the request. Contrary to other artifact types, an organization cannot own a set of results. However, you can specify the <code>BenchmarkResults.contributors</code> field to share credit with other hub users.</p> Required metadata <p>The Polaris client and Hub maintain different requirements as to which metadata is required. The requirements by the Hub are stricter, so when uploading to the Hub you might get some errors on missing metadata. Make sure to fill-in as much of the metadata as possible before uploading.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>BenchmarkResults</code> <p>The results to upload.</p> required <code>access</code> <code>AccessType</code> <p>Grant public or private access to result</p> <code>'private'</code> <code>owner</code> <code>HubOwner | str | None</code> <p>Which Hub user or organization owns the artifact. Takes precedence over <code>results.owner</code>.</p> <code>None</code>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.upload_dataset","title":"upload_dataset","text":"<pre><code>upload_dataset(\n    dataset: DatasetV1 | DatasetV2,\n    access: AccessType = \"private\",\n    timeout: TimeoutTypes = (10, 200),\n    owner: HubOwner | str | None = None,\n    if_exists: ZarrConflictResolution = \"replace\",\n)\n</code></pre> <p>Upload a dataset to the Polaris Hub.</p> Owner <p>You have to manually specify the owner in the dataset data model. Because the owner could be a user or an organization, we cannot automatically infer this from just the logged-in user.</p> Required metadata <p>The Polaris client and Hub maintain different requirements as to which metadata is required. The requirements by the Hub are stricter, so when uploading to the Hub you might get some errors on missing metadata. Make sure to fill-in as much of the metadata as possible before uploading.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DatasetV1 | DatasetV2</code> <p>The dataset to upload.</p> required <code>access</code> <code>AccessType</code> <p>Grant public or private access to result</p> <code>'private'</code> <code>timeout</code> <code>TimeoutTypes</code> <p>Request timeout values. User can modify the value when uploading large dataset as needed. This can be a single value with the timeout in seconds for all IO operations, or a more granular tuple with (connect_timeout, write_timeout). The type of the the timout parameter comes from <code>httpx</code>. Since datasets can get large, it might be needed to increase the write timeout for larger datasets. See also: https://www.python-httpx.org/advanced/#timeout-configuration</p> <code>(10, 200)</code> <code>owner</code> <code>HubOwner | str | None</code> <p>Which Hub user or organization owns the artifact. Takes precedence over <code>dataset.owner</code>.</p> <code>None</code> <code>if_exists</code> <code>ZarrConflictResolution</code> <p>Action for handling existing files in the Zarr archive. Options are 'raise' to throw an error, 'replace' to overwrite, or 'skip' to proceed without altering the existing files.</p> <code>'replace'</code>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.upload_benchmark","title":"upload_benchmark","text":"<pre><code>upload_benchmark(\n    benchmark: BenchmarkV1Specification | BenchmarkV2Specification,\n    access: AccessType = \"private\",\n    owner: HubOwner | str | None = None,\n)\n</code></pre> <p>Upload the benchmark to the Polaris Hub.</p> Owner <p>You have to manually specify the owner in the benchmark data model. Because the owner could be a user or an organization, we cannot automatically infer this from the logged-in user.</p> Required metadata <p>The Polaris client and Hub maintain different requirements as to which metadata is required. The requirements by the Hub are stricter, so when uploading to the Hub you might get some errors on missing metadata. Make sure to fill-in as much of the metadata as possible before uploading.</p> Non-existent datasets <p>The client will not upload the associated dataset to the Hub if it does not yet exist. Make sure to specify an existing dataset or upload the dataset first.</p> <p>Parameters:</p> Name Type Description Default <code>benchmark</code> <code>BenchmarkV1Specification | BenchmarkV2Specification</code> <p>The benchmark to upload.</p> required <code>access</code> <code>AccessType</code> <p>Grant public or private access to result</p> <code>'private'</code> <code>owner</code> <code>HubOwner | str | None</code> <p>Which Hub user or organization owns the artifact. Takes precedence over <code>benchmark.owner</code>.</p> <code>None</code>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.get_competition","title":"get_competition","text":"<pre><code>get_competition(artifact_id: str) -&gt; CompetitionSpecification\n</code></pre> <p>Load a competition from the Polaris Hub.</p> <p>Parameters:</p> Name Type Description Default <code>artifact_id</code> <code>str</code> <p>The artifact identifier for the competition</p> required <p>Returns:</p> Type Description <code>CompetitionSpecification</code> <p>A <code>CompetitionSpecification</code> instance, if it exists.</p>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.submit_competition_predictions","title":"submit_competition_predictions","text":"<pre><code>submit_competition_predictions(\n    competition: CompetitionSpecification,\n    competition_predictions: CompetitionPredictions,\n)\n</code></pre> <p>Submit predictions for a competition to the Polaris Hub. The Hub will evaluate them against the secure test set and store the result.</p> <p>Parameters:</p> Name Type Description Default <code>competition</code> <code>CompetitionSpecification</code> <p>The competition to evaluate the predictions for.</p> required <code>competition_predictions</code> <code>CompetitionPredictions</code> <p>The predictions and associated metadata to be submitted to the Hub.</p> required"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.upload_model","title":"upload_model","text":"<pre><code>upload_model(\n    model: Model,\n    access: AccessType = \"private\",\n    owner: HubOwner | str | None = None,\n)\n</code></pre> <p>Upload a model to the Polaris Hub.</p> Owner <p>You have to manually specify the owner in the model data model. Because the owner could be a user or an organization, we cannot automatically infer this from just the logged-in user.</p> Required metadata <p>The Polaris client and Hub maintain different requirements as to which metadata is required. The requirements by the Hub are stricter, so when uploading to the Hub you might get some errors on missing metadata. Make sure to fill-in as much of the metadata as possible before uploading.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model to upload.</p> required <code>access</code> <code>AccessType</code> <p>Grant public or private access to result</p> <code>'private'</code> <code>owner</code> <code>HubOwner | str | None</code> <p>Which Hub user or organization owns the artifact. Takes precedence over <code>model.owner</code>.</p> <code>None</code>"},{"location":"api/hub.external_client.html","title":"External Auth Client","text":""},{"location":"api/hub.external_client.html#polaris.hub.external_client.ExternalAuthClient","title":"polaris.hub.external_client.ExternalAuthClient","text":"<pre><code>ExternalAuthClient(\n    settings: PolarisHubSettings, cache_auth_token: bool = True, **kwargs: dict\n)\n</code></pre> <p>               Bases: <code>OAuth2Client</code></p> <p>This authentication client is used to obtain OAuth 2 tokens from Polaris's external OAuth2 server. These can in turn be used to obtain Polaris Hub tokens.</p> Internal use <p>This class is intended for internal use by the <code>PolarisHubClient</code> class, and you should not have to interact with it directly.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>PolarisHubSettings</code> <p>A <code>PolarisHubSettings</code> instance.</p> required <code>cache_auth_token</code> <code>bool</code> <p>Whether to cache the auth token to a file.</p> <code>True</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to the authlib <code>OAuth2Client</code> constructor.</p> <code>{}</code>"},{"location":"api/hub.external_client.html#polaris.hub.external_client.ExternalAuthClient.user_info","title":"user_info  <code>property</code>","text":"<pre><code>user_info: dict\n</code></pre> <p>Get information about the currently logged-in user through the OAuth2 User Info flow.</p>"},{"location":"api/hub.external_client.html#polaris.hub.external_client.ExternalAuthClient.interactive_login","title":"interactive_login","text":"<pre><code>interactive_login(overwrite: bool = False, auto_open_browser: bool = True)\n</code></pre> <p>Login to the Polaris Hub using an interactive flow, through a Web browser.</p> Headless authentication <p>It is currently not possible to log in to the Polaris Hub without a browser. See this GitHub issue for more info.</p> <p>Parameters:</p> Name Type Description Default <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the current token if the user is already logged in.</p> <code>False</code> <code>auto_open_browser</code> <code>bool</code> <p>Whether to automatically open the browser to visit the authorization URL.</p> <code>True</code>"},{"location":"api/hub.storage.html","title":"Hub.storage","text":""},{"location":"api/hub.storage.html#polaris.hub.storage.StorageSession","title":"polaris.hub.storage.StorageSession","text":"<pre><code>StorageSession(hub_client, scope: Scope, resource: ArtifactUrn)\n</code></pre> <p>               Bases: <code>OAuth2Client</code></p> <p>A context manager for managing a storage session, with token exchange and token refresh capabilities. Each session is associated with a specific scope and resource.</p>"},{"location":"api/hub.storage.html#polaris.hub.storage.StorageSession.fetch_token","title":"fetch_token","text":"<pre><code>fetch_token(**kwargs) -&gt; dict[str, Any]\n</code></pre> <p>Error handling for token fetching.</p>"},{"location":"api/hub.storage.html#polaris.hub.storage.StorageSession.ensure_active_token","title":"ensure_active_token","text":"<pre><code>ensure_active_token(token: OAuth2Token | None = None) -&gt; bool\n</code></pre> <p>Override the active check to trigger a re-fetch of the token if it is not active.</p>"},{"location":"api/hub.storage.html#polaris.hub.storage.StorageSession.set_file","title":"set_file","text":"<pre><code>set_file(path: str, value: bytes | bytearray)\n</code></pre> <p>Set a value at the given path.</p>"},{"location":"api/hub.storage.html#polaris.hub.storage.StorageSession.get_file","title":"get_file","text":"<pre><code>get_file(path: str) -&gt; bytes | bytearray\n</code></pre> <p>Get the value at the given path.</p>"},{"location":"api/hub.storage.html#polaris.hub.storage.S3Store","title":"polaris.hub.storage.S3Store","text":"<pre><code>S3Store(\n    path: str | PurePath,\n    access_key: str,\n    secret_key: str,\n    token: str,\n    endpoint_url: str,\n    part_size: int = 10 * 1024 * 1024,\n    content_type: str = \"application/octet-stream\",\n)\n</code></pre> <p>               Bases: <code>Store</code></p> <p>A Zarr store implementation using a S3 bucket as the backend storage.</p> <p>It supports multipart uploads for large objects and handles S3-specific exceptions.</p>"},{"location":"api/hub.storage.html#polaris.hub.storage.S3Store.copy_to_destination","title":"copy_to_destination","text":"<pre><code>copy_to_destination(\n    destination: Store,\n    if_exists: ZarrConflictResolution = \"replace\",\n    log: Callable = lambda: None,\n) -&gt; tuple[int, int, int]\n</code></pre> <p>Copy the content of this store to the destination store.</p> <p>We leverage the internal knowledge of this store to make the operation more efficient than <code>zarr.copy_store</code>:     - Parallel, concurrent <code>getitems</code> operations using a thread pool</p> <p>Zarr V3 supports partial writes, that would allow us to stream the response back into the store. Unfortunately, it's not supported right now by the library version we use, so we'll have to read the whole response into memory and write it back to the destination.</p> <p>Parameters:</p> Name Type Description Default <code>destination</code> <code>Store</code> <p>destination store</p> required <code>if_exists</code> <code>ZarrConflictResolution</code> <p>behavior if the destination key already exists</p> <code>'replace'</code> <code>log</code> <code>Callable</code> <p>optional logging function</p> <code>lambda: None</code>"},{"location":"api/hub.storage.html#polaris.hub.storage.S3Store.copy_from_source","title":"copy_from_source","text":"<pre><code>copy_from_source(\n    source: Store,\n    if_exists: ZarrConflictResolution = \"replace\",\n    log: Callable = lambda: None,\n) -&gt; tuple[int, int, int]\n</code></pre> <p>Copy the content of the source store to this store.</p> <p>We leverage the internal knowledge of this store to make the operation more efficient than <code>zarr.copy_store</code>:     - Conditional <code>put_object</code> operation for the \"skip\" conflict resolution     - Parallel, concurrent <code>put_object</code> operations using a thread pool</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Store</code> <p>source store</p> required <code>if_exists</code> <code>ZarrConflictResolution</code> <p>behavior if the destination key already exists</p> <code>'replace'</code> <code>log</code> <code>Callable</code> <p>optional logging function</p> <code>lambda: None</code>"},{"location":"api/hub.storage.html#polaris.hub.storage.S3Store.listdir","title":"listdir","text":"<pre><code>listdir(path: str = '') -&gt; Generator[str, None, None]\n</code></pre> <p>For a given path, list all the \"subdirectories\" and \"files\" for that path. The returned paths are relative to the input path.</p> <p>Uses pagination and return a generator to handle very large number of keys. Note: This might not help with some Zarr operations that materialize the whole sequence.</p>"},{"location":"api/hub.storage.html#polaris.hub.storage.S3Store.getitems","title":"getitems","text":"<pre><code>getitems(\n    keys: Sequence[str], *, contexts: Mapping[str, Context]\n) -&gt; dict[str, Any]\n</code></pre> <p>More efficient implementation of getitems using concurrent fetching through multiple threads.</p> <p>The default implementation uses contains to check existence before fetching the value, which doubles the number of requests.</p>"},{"location":"api/hub.storage.html#polaris.hub.storage.S3Store.getsize","title":"getsize","text":"<pre><code>getsize(key: str) -&gt; int\n</code></pre> <p>Return the size (in bytes) of the object at the given key.</p>"},{"location":"api/hub.storage.html#polaris.hub.storage.S3Store.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key: str) -&gt; bytes\n</code></pre> <p>Retrieves the value for the given key from the store.</p> <p>Makes no provision to handle overly large values returned.</p>"},{"location":"api/hub.storage.html#polaris.hub.storage.S3Store.__setitem__","title":"__setitem__","text":"<pre><code>__setitem__(key: str, value: bytes | bytearray | memoryview) -&gt; None\n</code></pre> <p>Persists the given value in the store.</p> <p>Based on value size, will use multipart upload for large files, or a single put_object call.</p>"},{"location":"api/hub.storage.html#polaris.hub.storage.S3Store.__delitem__","title":"__delitem__","text":"<pre><code>__delitem__(key: str) -&gt; None\n</code></pre> <p>Removing a key from the store is not supported.</p>"},{"location":"api/hub.storage.html#polaris.hub.storage.S3Store.__contains__","title":"__contains__","text":"<pre><code>__contains__(key: str) -&gt; bool\n</code></pre> <p>Checks the existence of a key in the store.</p> <p>If the intent is to download the value after this check, it is more efficient to attempt tp retrieve it and handle the KeyError from a non-existent key.</p>"},{"location":"api/hub.storage.html#polaris.hub.storage.S3Store.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; Generator[str, None, None]\n</code></pre> <p>Iterate through all the keys in the store.</p>"},{"location":"api/hub.storage.html#polaris.hub.storage.S3Store.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Number of keys in the store.</p>"},{"location":"api/hub.storage.html#polaris.hub.storage.S3Store.__hash__","title":"__hash__","text":"<pre><code>__hash__()\n</code></pre> <p>Custom hash function, to enable lru_cache decorator on methods</p>"},{"location":"api/load.html","title":"Load","text":""},{"location":"api/load.html#polaris.load_dataset","title":"polaris.load_dataset","text":"<pre><code>load_dataset(\n    path: str, verify_checksum: ChecksumStrategy = \"verify_unless_zarr\"\n) -&gt; DatasetV1\n</code></pre> <p>Loads a Polaris dataset.</p> <p>In Polaris, a dataset is a tabular data structure that stores data-points in a row-wise manner. A dataset can have multiple modalities or targets, can be sparse and can be part of one or multiple benchmarks.</p> <p>The Polaris dataset can be loaded from the Hub or from a local or remote directory.</p> <ul> <li>Hub (recommended): When loading the dataset from the Hub, you can simply     provide the <code>owner/name</code> slug. This can be easily copied from the relevant dataset     page on the Hub.</li> <li>Directory: When loading the dataset from a directory, you should provide the path     as returned by <code>dataset.to_json()</code>. The path can be local or remote.</li> </ul>"},{"location":"api/load.html#polaris.load_benchmark","title":"polaris.load_benchmark","text":"<pre><code>load_benchmark(\n    path: str, verify_checksum: ChecksumStrategy = \"verify_unless_zarr\"\n)\n</code></pre> <p>Loads a Polaris benchmark.</p> <p>In Polaris, a benchmark wraps a dataset with additional metadata to specify the evaluation logic.</p> <p>The Polaris benchmark can be loaded from the Hub or from a local or remote directory.</p> Dataset is automatically loaded <p>The dataset underlying the benchmark is automatically loaded when loading the benchmark.</p> <ul> <li>Hub (recommended): When loading the benchmark from the Hub, you can simply     provide the <code>owner/name</code> slug. This can be easily copied from the relevant benchmark     page on the Hub.</li> <li>Directory: When loading the benchmark from a directory, you should provide the path     as returned by <code>benchmmark.to_json()</code>. The path can be local or remote.</li> </ul>"},{"location":"api/model.html","title":"Model","text":""},{"location":"api/model.html#polaris.model.Model","title":"polaris.model.Model","text":"<p>               Bases: <code>BaseArtifactModel</code></p> <p>Represents a Model artifact in the Polaris ecosystem.</p> <p>A Model artifact serves as a centralized representation of a method or model, encapsulating its metadata. It can be associated with multiple result artifacts but is immutable after upload, except for the README field.</p> <p>Examples:</p> <p>Basic API usage: <pre><code>from polaris.model import Model\n\n# Create a new Model Card\nmodel = Model(\n    name=\"MolGPS\",\n    description=\"Graph transformer foundation model for molecular modeling\",\n    code_url=\"https://github.com/datamol-io/graphium\"\n)\n\n# Upload the model card to the Hub\nmodel.upload_to_hub(owner=\"recursion\")\n</code></pre></p> <p>Attributes:</p> Name Type Description <code>readme</code> <code>str</code> <p>A detailed README describing the model.</p> <code>code_url</code> <code>HttpUrlString | None</code> <p>Optional URL pointing to the model's code repository.</p> <code>report_url</code> <code>HttpUrlString | None</code> <p>Optional URL linking to a report or publication related to the model.</p> <p>Methods:</p> Name Description <code>upload_to_hub</code> <p>AccessType = \"private\", owner: HubOwner | str | None = None): Uploads the model artifact to the Polaris Hub, associating it with a specified owner and access level.</p> <p>For additional metadata attributes, see the base class.</p>"},{"location":"api/model.html#polaris.model.Model.upload_to_hub","title":"upload_to_hub","text":"<pre><code>upload_to_hub(\n    access: AccessType = \"private\", owner: HubOwner | str | None = None\n)\n</code></pre> <p>Uploads the model to the Polaris Hub.</p>"},{"location":"api/subset.html","title":"Subset","text":""},{"location":"api/subset.html#polaris.dataset.Subset","title":"polaris.dataset.Subset","text":"<p>The <code>Subset</code> class provides easy access to a single partition of a split dataset.</p> No need to create this class manually <p>You should not have to create this class manually. In most use-cases, you can create a <code>Subset</code> through the <code>get_train_test_split</code> method of a <code>BenchmarkSpecification</code> object.</p> Featurize your inputs <p>Not all datasets are already featurized. For example, a small-molecule task might simply provide the SMILES string. To easily featurize the inputs, you can pass or set a transformation function. For example:</p> <pre><code>import datamol as dm\n\nbenchmark.get_train_test_split(..., featurization_fn=dm.to_fp)\n</code></pre> <p>This should be the starting point for any framework-specific (e.g. PyTorch, Tensorflow) data-loader implementation. How the data is loaded in Polaris can be non-trivial, so this class is provided to abstract away the details. To easily build framework-specific data-loaders, a <code>Subset</code> supports various styles of accessing the data:</p> <ol> <li>In memory: Loads the entire dataset in memory and returns a single array with all datapoints,     this style is accessible through the <code>subset.targets</code> and <code>subset.inputs</code> properties.</li> <li>List: Index the subset like a list, this style is accessible through the <code>subset[idx]</code> syntax.</li> <li>Iterator: Iterate over the subset, this style is accessible through the <code>iter(subset)</code> syntax.</li> </ol> <p>Examples:</p> <p>The different styles of accessing the data:</p> <pre><code>import polaris as po\n\nbenchmark = po.load_benchmark(...)\ntrain, test = benchmark.get_train_test_split()\n\n# Load the entire dataset in memory, useful for e.g. scikit-learn.\nX = train.inputs\ny = train.targets\n\n# Access a single datapoint as with a list, useful for e.g. PyTorch.\nx, y = train[0]\n\n# Iterate over the dataset, useful for very large datasets.\nfor x, y in train:\n    ...\n</code></pre> <p>Raises:</p> Type Description <code>TestAccessError</code> <p>When trying to access the targets of the test set (specified by the <code>hide_targets</code> attribute).</p>"},{"location":"api/utils.types.html","title":"Types","text":""},{"location":"api/utils.types.html#polaris.utils.types.SplitIndicesType","title":"SplitIndicesType  <code>module-attribute</code>","text":"<pre><code>SplitIndicesType: TypeAlias = list[int]\n</code></pre> <p>A split is defined by a sequence of integers.</p>"},{"location":"api/utils.types.html#polaris.utils.types.SplitType","title":"SplitType  <code>module-attribute</code>","text":"<pre><code>SplitType: TypeAlias = tuple[\n    SplitIndicesType, Union[SplitIndicesType, dict[str, SplitIndicesType]]\n]\n</code></pre> <p>A split is a pair of which the first item is always assumed to be the train set. The second item can either be a single test set or a dictionary with multiple, named test sets.</p>"},{"location":"api/utils.types.html#polaris.utils.types.ListOrArrayType","title":"ListOrArrayType  <code>module-attribute</code>","text":"<pre><code>ListOrArrayType: TypeAlias = list | ndarray\n</code></pre> <p>A list of numbers or a numpy array. Predictions can be provided as either a list or a numpy array.</p>"},{"location":"api/utils.types.html#polaris.utils.types.IncomingPredictionsType","title":"IncomingPredictionsType  <code>module-attribute</code>","text":"<pre><code>IncomingPredictionsType: TypeAlias = (\n    ListOrArrayType | dict[str, ListOrArrayType | dict[str, ListOrArrayType]]\n)\n</code></pre> <p>The type of the predictions that are ingested into the Polaris BenchmarkPredictions object. Can be one of the following:</p> <ul> <li>A single array (single-task, single test set)</li> <li>A dictionary of arrays (single-task, multiple test sets)</li> <li>A dictionary of dictionaries of arrays (multi-task, multiple test sets)</li> </ul>"},{"location":"api/utils.types.html#polaris.utils.types.PredictionsType","title":"PredictionsType  <code>module-attribute</code>","text":"<pre><code>PredictionsType: TypeAlias = dict[str, dict[str, ndarray]]\n</code></pre> <p>The normalized format for predictions for internal use. Predictions are accepted in a generous variety of representations and normalized into this standard format, a dictionary of dictionaries that looks like {\"test_set_name\": {\"target_name\": np.ndarray}}.</p>"},{"location":"api/utils.types.html#polaris.utils.types.DatapointType","title":"DatapointType  <code>module-attribute</code>","text":"<pre><code>DatapointType: TypeAlias = tuple[DatapointPartType, DatapointPartType]\n</code></pre> <p>A datapoint has:</p> <ul> <li>A single input or multiple inputs (either as dict or tuple)</li> <li>No target, a single target or a multiple targets (either as dict or tuple)</li> </ul>"},{"location":"api/utils.types.html#polaris.utils.types.SlugStringType","title":"SlugStringType  <code>module-attribute</code>","text":"<pre><code>SlugStringType: TypeAlias = Annotated[\n    str, StringConstraints(pattern=\"^[a-z0-9-]+$\", min_length=4, max_length=64)\n]\n</code></pre> <p>A URL-compatible string that can serve as slug on the Hub.</p>"},{"location":"api/utils.types.html#polaris.utils.types.SlugCompatibleStringType","title":"SlugCompatibleStringType  <code>module-attribute</code>","text":"<pre><code>SlugCompatibleStringType: TypeAlias = Annotated[\n    str,\n    StringConstraints(pattern=\"^[A-Za-z0-9_-]+$\", min_length=4, max_length=64),\n]\n</code></pre> <p>A URL-compatible string that can be turned into a slug by the Hub.</p> <p>Can only use alpha-numeric characters, underscores and dashes. The string must be at least 4 and at most 64 characters long.</p>"},{"location":"api/utils.types.html#polaris.utils.types.Md5StringType","title":"Md5StringType  <code>module-attribute</code>","text":"<pre><code>Md5StringType: TypeAlias = Annotated[\n    str, StringConstraints(pattern=\"^[a-f0-9]{32}$\")\n]\n</code></pre> <p>A string that represents an MD5 hash.</p>"},{"location":"api/utils.types.html#polaris.utils.types.HubUser","title":"HubUser  <code>module-attribute</code>","text":"<pre><code>HubUser: TypeAlias = SlugCompatibleStringType\n</code></pre> <p>A user on the Polaris Hub is identified by a username, which is a <code>SlugCompatibleStringType</code>.</p>"},{"location":"api/utils.types.html#polaris.utils.types.HttpUrlString","title":"HttpUrlString  <code>module-attribute</code>","text":"<pre><code>HttpUrlString: TypeAlias = Annotated[\n    str, BeforeValidator(lambda v: validate_python(v) and v)\n]\n</code></pre> <p>A validated HTTP URL that will be turned into a string. This is useful for interactions with httpx and authlib, who have their own URL types.</p>"},{"location":"api/utils.types.html#polaris.utils.types.AnyUrlString","title":"AnyUrlString  <code>module-attribute</code>","text":"<pre><code>AnyUrlString: TypeAlias = Annotated[\n    str, BeforeValidator(lambda v: validate_python(v) and v)\n]\n</code></pre> <p>A validated generic URL that will be turned into a string. This is useful for interactions with other libraries that expect a string.</p>"},{"location":"api/utils.types.html#polaris.utils.types.DirectionType","title":"DirectionType  <code>module-attribute</code>","text":"<pre><code>DirectionType: TypeAlias = float | Literal['min', 'max']\n</code></pre> <p>The direction of any variable to be sorted. This can be used to sort the metric score, indicate the optmization direction of endpoint.</p>"},{"location":"api/utils.types.html#polaris.utils.types.AccessType","title":"AccessType  <code>module-attribute</code>","text":"<pre><code>AccessType: TypeAlias = Literal['public', 'private']\n</code></pre> <p>Type to specify access to a dataset, benchmark or result in the Hub.</p>"},{"location":"api/utils.types.html#polaris.utils.types.TimeoutTypes","title":"TimeoutTypes  <code>module-attribute</code>","text":"<pre><code>TimeoutTypes = Union[Tuple[int, int], Literal['timeout', 'never']]\n</code></pre> <p>Timeout types for specifying maximum wait times.</p>"},{"location":"api/utils.types.html#polaris.utils.types.IOMode","title":"IOMode  <code>module-attribute</code>","text":"<pre><code>IOMode: TypeAlias = Literal['r', 'r+', 'a', 'w', 'w-']\n</code></pre> <p>Type to specify the mode for input/output operations (I/O) when interacting with a file or resource.</p>"},{"location":"api/utils.types.html#polaris.utils.types.SupportedLicenseType","title":"SupportedLicenseType  <code>module-attribute</code>","text":"<pre><code>SupportedLicenseType: TypeAlias = Literal[\n    \"CC-BY-4.0\",\n    \"CC-BY-SA-4.0\",\n    \"CC-BY-NC-4.0\",\n    \"CC-BY-NC-SA-4.0\",\n    \"CC0-1.0\",\n    \"MIT\",\n]\n</code></pre> <p>Supported license types for dataset uploads to Polaris Hub</p>"},{"location":"api/utils.types.html#polaris.utils.types.ZarrConflictResolution","title":"ZarrConflictResolution  <code>module-attribute</code>","text":"<pre><code>ZarrConflictResolution: TypeAlias = Literal['raise', 'replace', 'skip']\n</code></pre> <p>Type to specify which action to take when encountering existing files within a Zarr archive.</p>"},{"location":"api/utils.types.html#polaris.utils.types.ChecksumStrategy","title":"ChecksumStrategy  <code>module-attribute</code>","text":"<pre><code>ChecksumStrategy: TypeAlias = Literal['verify', 'verify_unless_zarr', 'ignore']\n</code></pre> <p>Type to specify which action to take to verify the data integrity of an artifact through a checksum.</p>"},{"location":"api/utils.types.html#polaris.utils.types.ArtifactUrn","title":"ArtifactUrn  <code>module-attribute</code>","text":"<pre><code>ArtifactUrn: TypeAlias = Annotated[\n    str, StringConstraints(pattern=\"^urn:polaris:\\\\w+:\\\\w+:\\\\w+$\")\n]\n</code></pre> <p>A Uniform Resource Name (URN) for an artifact on the Polaris Hub.</p>"},{"location":"api/utils.types.html#polaris.utils.types.DatasetIndex","title":"DatasetIndex  <code>module-attribute</code>","text":"<pre><code>DatasetIndex: TypeAlias = RowIndex | tuple[RowIndex, ColumnIndex]\n</code></pre> <p>To index a dataset using square brackets, we have a few options:</p> <ul> <li>A single row, e.g. dataset[0]</li> <li>Specify a specific value, e.g. dataset[0, \"col1\"]</li> </ul> <p>There are more exciting options we could implement, such as slicing,  but this gets complex.</p>"},{"location":"api/utils.types.html#polaris.utils.types.PredictionKwargs","title":"PredictionKwargs  <code>module-attribute</code>","text":"<pre><code>PredictionKwargs: TypeAlias = Literal['y_pred', 'y_prob', 'y_score']\n</code></pre> <p>The type of predictions expected by the metric interface.</p>"},{"location":"api/utils.types.html#polaris.utils.types.ColumnName","title":"ColumnName  <code>module-attribute</code>","text":"<pre><code>ColumnName: TypeAlias = str\n</code></pre> <p>A column name in a dataset.</p>"},{"location":"api/utils.types.html#polaris.utils.types.HubOwner","title":"HubOwner","text":"<p>               Bases: <code>BaseModel</code></p> <p>An owner of an artifact on the Polaris Hub</p> <p>The slug is most important as it is the user-facing part of this data model. The externalId and type are added to be consistent with the model returned by the Polaris Hub .</p>"},{"location":"api/utils.types.html#polaris.utils.types.HubOwner.normalize","title":"normalize  <code>staticmethod</code>","text":"<pre><code>normalize(owner: str | Self) -&gt; Self\n</code></pre> <p>Normalize a string or <code>HubOwner</code> instance to a <code>HubOwner</code> instance.</p>"},{"location":"api/utils.types.html#polaris.utils.types.TargetType","title":"TargetType","text":"<p>               Bases: <code>Enum</code></p> <p>The high-level classification of different targets.</p>"},{"location":"api/utils.types.html#polaris.utils.types.TaskType","title":"TaskType","text":"<p>               Bases: <code>Enum</code></p> <p>The high-level classification of different tasks.</p>"},{"location":"tutorials/create_a_benchmark.html","title":"Create a Benchmark","text":"<p>Polaris explicitly distinguished datasets from benchmarks. A benchmark defines the ML task and evaluation logic (e.g. split and metrics) for a dataset. Because of this, a single dataset can be the basis of multiple benchmarks.</p> In\u00a0[\u00a0]: Copied! <pre>input_columns = [\"SMILES\"]\ntarget_columns = [\"LOG_SOLUBILITY\"]\n</pre> input_columns = [\"SMILES\"] target_columns = [\"LOG_SOLUBILITY\"] <p>In this case, we specified just a single input and target column, but a benchmark can have multiple (e.g. a multi-task benchmark).</p> In\u00a0[\u00a0]: Copied! <pre>from polaris.benchmark._split_v2 import IndexSet\n\n# To specify a set of integers, you can directly pass in a list of integers\n# This will automatically convert the indices to a BitMap\ntraining = IndexSet(indices=[0, 1])\ntest = IndexSet(indices=[2])\n</pre> from polaris.benchmark._split_v2 import IndexSet  # To specify a set of integers, you can directly pass in a list of integers # This will automatically convert the indices to a BitMap training = IndexSet(indices=[0, 1]) test = IndexSet(indices=[2]) In\u00a0[\u00a0]: Copied! <pre>from pyroaring import BitMap\n\n# Or you can create the BitMap manually and iteratively\nindices = BitMap()\nindices.add(0)\nindices.add(1)\n\ntraining = IndexSet(indices=indices)\n</pre> from pyroaring import BitMap  # Or you can create the BitMap manually and iteratively indices = BitMap() indices.add(0) indices.add(1)  training = IndexSet(indices=indices) In\u00a0[\u00a0]: Copied! <pre>from polaris.benchmark._split_v2 import SplitV2\n\n# Finally, we create the actual split object\nsplit = SplitV2(training=training, test=test)\n</pre> from polaris.benchmark._split_v2 import SplitV2  # Finally, we create the actual split object split = SplitV2(training=training, test=test) In\u00a0[\u00a0]: Copied! <pre>metrics = [\"mean_absolute_error\", \"mean_squared_error\"]\n</pre> metrics = [\"mean_absolute_error\", \"mean_squared_error\"] <p>You can also specify a main metric, which will be the metric used to rank the leaderboard.</p> In\u00a0[\u00a0]: Copied! <pre>main_metric = \"mean_absolute_error\"\n</pre> main_metric = \"mean_absolute_error\" <p>To get a list of all support metrics, you can use:</p> In\u00a0[\u00a0]: Copied! <pre>from polaris.evaluate._metric import DEFAULT_METRICS\n\nDEFAULT_METRICS.keys()\n</pre> from polaris.evaluate._metric import DEFAULT_METRICS  DEFAULT_METRICS.keys() <p>You can also create more complex metrics that wrap these base metrics.</p> In\u00a0[\u00a0]: Copied! <pre>from polaris.evaluate import Metric\n\nmae_agg = Metric(label=\"mean_absolute_error\", config={\"group_by\": \"UNIQUE_ID\", \"on_error\": \"ignore\", \"aggregation\": \"mean\"})\nmetrics.append(mae_agg)\n</pre> from polaris.evaluate import Metric  mae_agg = Metric(label=\"mean_absolute_error\", config={\"group_by\": \"UNIQUE_ID\", \"on_error\": \"ignore\", \"aggregation\": \"mean\"}) metrics.append(mae_agg) <p>What if my metric isn't supported yet?</p> <p>Using a metric that's not supported yet, currently requires adding it to the Polaris codebase. We're always looking to improve support. Reach out to us over Github and we're happy to help!</p> In\u00a0[\u00a0]: Copied! <pre>type(dataset)\n</pre> type(dataset) In\u00a0[\u00a0]: Copied! <pre>from polaris.benchmark._benchmark_v2 import BenchmarkV2Specification\n\nbenchmark = BenchmarkV2Specification(\n    # 1. The dataset\n    dataset=dataset,\n    # 2. The task\n    input_cols=input_columns,\n    target_cols=target_columns,\n    # 3. The split\n    split=split,\n    # 4. The metrics\n    metrics=metrics,\n    main_metric=main_metric,\n    # 5. The metadata\n    name=\"my-first-benchmark\",\n    owner=\"your-username\", \n    description=\"Created using the Polaris tutorial\",\n    tags=[\"tutorial\"], \n    user_attributes={\"Key\": \"Value\"}\n)\n</pre> from polaris.benchmark._benchmark_v2 import BenchmarkV2Specification  benchmark = BenchmarkV2Specification(     # 1. The dataset     dataset=dataset,     # 2. The task     input_cols=input_columns,     target_cols=target_columns,     # 3. The split     split=split,     # 4. The metrics     metrics=metrics,     main_metric=main_metric,     # 5. The metadata     name=\"my-first-benchmark\",     owner=\"your-username\",      description=\"Created using the Polaris tutorial\",     tags=[\"tutorial\"],      user_attributes={\"Key\": \"Value\"} ) In\u00a0[\u00a0]: Copied! <pre>benchmark.upload_to_hub(owner=\"your-username\")\n</pre> benchmark.upload_to_hub(owner=\"your-username\") <p>The End.</p>"},{"location":"tutorials/create_a_benchmark.html#create-a-benchmark","title":"Create a Benchmark\u00b6","text":"<p>To create a benchmark, you need to instantiate the <code>BenchmarkV2Specification</code> class. This requires you to specify:</p> <ol> <li>The dataset, which can be stored either locally or on the Hub.</li> <li>The task, where a task is defined by input and target columns.</li> <li>The split, where a split is defined by a bunch of indices.</li> <li>The metric, where a metric needs to be officially supported by Polaris.</li> <li>The metadata to contextualize your benchmark.</li> </ol>"},{"location":"tutorials/create_a_benchmark.html#define-the-dataset","title":"Define the dataset\u00b6","text":"<p>To learn how to create a dataset, see this tutorial.</p> <p>Alternatively, we can also load an existing dataset from the Hub.</p> <p>Not all Hub datasets are supported</p> <p>You can only create benchmarks for DatasetV2 instances, not for DatasetV1 instances. Some of the datasets stored on the Hub are still V1 datasets.</p>"},{"location":"tutorials/create_a_benchmark.html#define-the-task","title":"Define the task\u00b6","text":"<p>Currently, Polaris only supports predictive tasks. Specifying a predictive task is simply done by specifying the input and target columns.</p>"},{"location":"tutorials/create_a_benchmark.html#define-the-split","title":"Define the split\u00b6","text":"<p>To ensure reproducible results, Polaris represents a split through a bunch of sets of indices.</p> <p>But there is a catch: We want Polaris to scale to extra large datasets. If we are to naively store millions of indices as lists of integers, this would impose a significant memory footprint. We therefore use bitmaps, more specifically roaring bitmaps to store the splits in a memory efficient way.</p>"},{"location":"tutorials/create_a_benchmark.html#define-the-metrics","title":"Define the metrics\u00b6","text":"<p>Even something as widely used as Mean Absolute Error (MAE) can be implemented in subtly different ways. Some people apply a log transform first, others might clip outliers, and sometimes an off-by-one or a bug creeps in. Over time, these variations add up. We decided to codify each metric for a Polaris benchmark in a single, transparent implementation. Our priority here is eliminating \u201cmystery differences\u201d that have nothing to do with actual model performance. Learn more here.</p> <p>Specifying a metric is easy. You can simply specify its label.</p>"},{"location":"tutorials/create_a_benchmark.html#bringing-it-all-together","title":"Bringing it all together\u00b6","text":"<p>Now we can create the <code>BenchmarkV2Specification</code> instance.</p>"},{"location":"tutorials/create_a_benchmark.html#share-your-benchmark","title":"Share your benchmark\u00b6","text":"<p>Want to share your benchmark with the community? Upload it to the Polaris Hub!</p>"},{"location":"tutorials/create_a_dataset.html","title":"Create a Dataset","text":"<p>On the surface, a dataset in Polaris is simply a tabular collection of data, storing datapoints in a row-wise manner. However, as you try create your own, you'll realize that there is some additional complexity under the hood.</p> In\u00a0[\u00a0]: Copied! <pre>from polaris.dataset import DatasetV2, ColumnAnnotation\n\ndataset = DatasetV2(\n    \n    # Specify metadata on the dataset level\n    name=\"tutorial-example\",\n    owner=\"your-username\",\n    tags=[\"small-molecules\", \"predictive\", \"admet\"],\n    source=\"https://example.com\",\n    license=\"CC-BY-4.0\",\n    \n    # Specify metadata on the column level\n    annotations = {\n        \"Ligand Pose\": ColumnAnnotation(\n            description=\"The 3D pose of the ligand\", \n            user_attributes={\"Object Type\": \"rdkit.Chem.Mol\"}, \n            modality=\"MOLECULE_3D\"\n        ),\n        \"Ligand SMILES\": ColumnAnnotation(\n            description=\"The 2D graph structure of the ligand, as SMILES\", \n            user_attributes={\"Object Type\": \"str\"}, \n            modality=\"MOLECULE\"\n        ),\n        \"Permeability\": ColumnAnnotation(\n            description=\"MDR1-MDCK efflux ratio (B-A/A-B)\", \n            user_attributes={\"Unit\": \"\tmL/min/kg\"}\n        )\n    },\n    \n    # Specify the actual data\n    zarr_root_path=\"path/to/root.zarr\",\n)\n</pre> from polaris.dataset import DatasetV2, ColumnAnnotation  dataset = DatasetV2(          # Specify metadata on the dataset level     name=\"tutorial-example\",     owner=\"your-username\",     tags=[\"small-molecules\", \"predictive\", \"admet\"],     source=\"https://example.com\",     license=\"CC-BY-4.0\",          # Specify metadata on the column level     annotations = {         \"Ligand Pose\": ColumnAnnotation(             description=\"The 3D pose of the ligand\",              user_attributes={\"Object Type\": \"rdkit.Chem.Mol\"},              modality=\"MOLECULE_3D\"         ),         \"Ligand SMILES\": ColumnAnnotation(             description=\"The 2D graph structure of the ligand, as SMILES\",              user_attributes={\"Object Type\": \"str\"},              modality=\"MOLECULE\"         ),         \"Permeability\": ColumnAnnotation(             description=\"MDR1-MDCK efflux ratio (B-A/A-B)\",              user_attributes={\"Unit\": \"\tmL/min/kg\"}         )     },          # Specify the actual data     zarr_root_path=\"path/to/root.zarr\", ) <p>For the rest of this tutorial, we will take a deeper look at the <code>zarr_root_path</code> parameter.</p> <p>First, some context.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\ndata = np.random.random(2048)\n</pre> import numpy as np  data = np.random.random(2048) In\u00a0[\u00a0]: Copied! <pre>import zarr\n\n# Create an empty Zarr group\nroot = zarr.open(path, \"w\")\n\n# Populate it with the array\nroot.array(\"column_name\", data)\n</pre> import zarr  # Create an empty Zarr group root = zarr.open(path, \"w\")  # Populate it with the array root.array(\"column_name\", data) In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n\ndf = pd.DataFrame({\n    \"A\": np.random.random(2048),\n    \"B\": np.random.random(2048)\n})\n</pre> import pandas as pd  df = pd.DataFrame({     \"A\": np.random.random(2048),     \"B\": np.random.random(2048) }) <p>Converting it to Zarr is as simple as creating equally named Zarr Arrays.</p> In\u00a0[\u00a0]: Copied! <pre>import zarr\n\n# Create an empty Zarr group\nroot = zarr.open(zarr_root_path, \"w\")\n\n# Populate it with the arrays\nfor col in set(df.columns):\n    root.array(col, data=df[col].values)\n</pre> import zarr  # Create an empty Zarr group root = zarr.open(zarr_root_path, \"w\")  # Populate it with the arrays for col in set(df.columns):     root.array(col, data=df[col].values) <p>Things get a little more tricky if you have columns with the <code>object</code> dtype, for example text.</p> In\u00a0[\u00a0]: Copied! <pre>df[\"C\"] = [\"test\"] * 2048\n</pre> df[\"C\"] = [\"test\"] * 2048 <p>In that case you need to tell Zarr how to encode the Python object.</p> In\u00a0[\u00a0]: Copied! <pre>import numcodecs\n\nroot.array(\"C\", data=df[\"C\"].values, dtype=object, object_codec=numcodecs.VLenUTF8())\n</pre> import numcodecs  root.array(\"C\", data=df[\"C\"].values, dtype=object, object_codec=numcodecs.VLenUTF8()) In\u00a0[\u00a0]: Copied! <pre># Create an exemplary molecule\nmol = Chem.MolFromSmiles('Cc1ccccc1')\nmol\n</pre> # Create an exemplary molecule mol = Chem.MolFromSmiles('Cc1ccccc1') mol In\u00a0[\u00a0]: Copied! <pre>from polaris.dataset.zarr.codecs import RDKitMolCodec\n\n# Write it to a Zarr array\nroot = zarr.open(zarr_root_path, \"w\")\nroot.array(\"molecules\", data=[mol] * 100, dtype=object, object_codec=RDKitMolCodec())\n</pre> from polaris.dataset.zarr.codecs import RDKitMolCodec  # Write it to a Zarr array root = zarr.open(zarr_root_path, \"w\") root.array(\"molecules\", data=[mol] * 100, dtype=object, object_codec=RDKitMolCodec()) <p>A common use case of this is to convert a number of SDF files to a Zarr array.</p> <ol> <li>Load the SDF files using RDKit to <code>Chem.Mol</code> objects.</li> <li>Create a Zarr array with the <code>RDKitMolCodec</code>.</li> <li>Store all RDKit objects in the Zarr array.</li> </ol> In\u00a0[\u00a0]: Copied! <pre>from tempfile import TemporaryDirectory\n\nimport biotite.database.rcsb as rcsb\nfrom biotite.structure.io import load_structure\n\n# Load an exemplary structure\nwith TemporaryDirectory() as tmpdir: \n    path = rcsb.fetch(\"1l2y\", \"pdb\", tmpdir)\n    struct = load_structure(path, model=1)\n</pre> from tempfile import TemporaryDirectory  import biotite.database.rcsb as rcsb from biotite.structure.io import load_structure  # Load an exemplary structure with TemporaryDirectory() as tmpdir:      path = rcsb.fetch(\"1l2y\", \"pdb\", tmpdir)     struct = load_structure(path, model=1) In\u00a0[\u00a0]: Copied! <pre>from polaris.dataset.zarr.codecs import AtomArrayCodec\n\n# Write it to a Zarr array\nroot = zarr.open(zarr_root_path, \"w\")\nroot.array(\"molecules\", data=[struct] * 100, dtype=object, object_codec=AtomArrayCodec())\n</pre> from polaris.dataset.zarr.codecs import AtomArrayCodec  # Write it to a Zarr array root = zarr.open(zarr_root_path, \"w\") root.array(\"molecules\", data=[struct] * 100, dtype=object, object_codec=AtomArrayCodec()) In\u00a0[\u00a0]: Copied! <pre>from imagecodecs.numcodecs import Jpeg2k\n\n# You need to explicitly register the codec\nnumcodecs.register_codec(Jpeg2k)\n</pre> from imagecodecs.numcodecs import Jpeg2k  # You need to explicitly register the codec numcodecs.register_codec(Jpeg2k) In\u00a0[\u00a0]: Copied! <pre>root = zarr.open(zarr_root_path, \"w\")\n\n# Array with a single 3 channel image\narr = root.zeros(\n    \"image\",\n    shape=(1, 512, 512, 3),\n    chunks=(1, 512, 512, 3),\n    dtype='u1',\n    compressor=Jpeg2k(level=52, reversible=True),\n)\n\narr[0] = img\n</pre> root = zarr.open(zarr_root_path, \"w\")  # Array with a single 3 channel image arr = root.zeros(     \"image\",     shape=(1, 512, 512, 3),     chunks=(1, 512, 512, 3),     dtype='u1',     compressor=Jpeg2k(level=52, reversible=True), )  arr[0] = img In\u00a0[\u00a0]: Copied! <pre>dataset.upload_to_hub(owner=\"your-username\")\n</pre> dataset.upload_to_hub(owner=\"your-username\")"},{"location":"tutorials/create_a_dataset.html#create-a-dataset","title":"Create a Dataset\u00b6","text":"<p>To create a dataset, you need to instantiate the <code>DatasetV2</code> class.</p>"},{"location":"tutorials/create_a_dataset.html#universal-and-ml-ready","title":"Universal and ML-ready\u00b6","text":"<p> An illustration of Zarr, which is core to Polaris its datamodel</p> <p>With the Polaris Hub we set out to design a universal data format for ML scientists in drug discovery. Whether you\u2019re working with phenomics, small molecules, or protein structures, you shouldn\u2019t have to spend time learning about domain-specific file formats, APIs, and software tools to be able to run some ML experiments. Beyond modalities, drug discovery datasets also come in different sizes, from kilobytes to terabytes. </p> <p>We found such a universal data format in Zarr. Zarr is a powerful library for storage of n-dimensional arrays, supporting chunking, compression, and various backends, making it a versatile choice for scientific and large-scale data. It's similar to HDF5, if you're familiar with that.</p> <p>Want to learn more?</p> <ul> <li>Learn about the motivation of our dataset implementation here.</li> <li>Learn what we mean by ML-ready here.</li> </ul>"},{"location":"tutorials/create_a_dataset.html#zarr-basics","title":"Zarr basics\u00b6","text":"<p>Zarr is well documented and before continuing this tutorial, we recommend you to at least read through the Quickstart.</p>"},{"location":"tutorials/create_a_dataset.html#converting-to-zarr","title":"Converting to Zarr\u00b6","text":"<p>In its most basic form, a Polaris compatible Zarr archive is a single Zarr group (the root) with equal length Zarr arrays for each of the columns in the dataset.</p> <p>Chances are that your dataset is currently not stored in a Zarr archive. We will show you how to convert a few common formats to a Polaris compatible Zarr archive.</p>"},{"location":"tutorials/create_a_dataset.html#from-a-numpy-array","title":"From a Numpy Array\u00b6","text":"<p>The most simple case is if you have your data in a NumPy array.</p>"},{"location":"tutorials/create_a_dataset.html#from-a-dataframe","title":"From a DataFrame\u00b6","text":"<p>Since Pandas DataFrames can be thought of as labeled NumPy arrays, converting a DataFrame is straight-forward too.</p>"},{"location":"tutorials/create_a_dataset.html#from-rdkit-eg-sdf","title":"From RDKit (e.g. SDF)\u00b6","text":"<p>The ability to encode custom Python objects is powerful.</p> <p>Using custom object codecs that Polaris provides, we can for example also store RDKit <code>Chem.Mol</code> objects in a Zarr array.</p>"},{"location":"tutorials/create_a_dataset.html#from-biotite-eg-mmcif","title":"From Biotite (e.g. mmCIF)\u00b6","text":"<p>Similarly, we can also store entire protein structures, as represented by the Biotite <code>AtomArray</code> class.</p>"},{"location":"tutorials/create_a_dataset.html#from-images-eg-png","title":"From Images (e.g. PNG)\u00b6","text":"<p>For more convential formats, such as images, codecs likely exist already.</p> <p>For images for example, these codecs are bundled in <code>imagecodecs</code>, which is an optional dependency of Polaris.</p> <p>An image is commonly represented as a 3D array (i.e. width x height x channels). It's therefore not needed to use object_codecs here. Instead, we specify the compressor Zarr should use to compress its chunks.</p>"},{"location":"tutorials/create_a_dataset.html#share-your-dataset","title":"Share your dataset\u00b6","text":"<p>Want to share your dataset with the community? Upload it to the Polaris Hub!</p>"},{"location":"tutorials/create_a_dataset.html#advanced-optimization","title":"Advanced: Optimization\u00b6","text":"<p>In this tutorial, we only briefly touched on the high-level concepts that need to be understood to create a Polaris compatible dataset using Zarr. However, Zarr has a lot more to offer and tweaking the settings can drastically improve storage or data access efficiency.</p> <p>If you would like to learn more, please see the Zarr documentation.</p> <p>The End.</p>"},{"location":"tutorials/submit_to_benchmark.html","title":"Submit to a Benchmark","text":"<p>This tutorial is an extended version of the Quickstart Guide</p> In\u00a0[\u00a0]: Copied! <pre>import polaris as po\n</pre> import polaris as po In\u00a0[\u00a0]: Copied! <pre>from polaris.hub.client import PolarisHubClient\n\nwith PolarisHubClient() as client:\n    client.login()\n</pre> from polaris.hub.client import PolarisHubClient  with PolarisHubClient() as client:     client.login() In\u00a0[\u00a0]: Copied! <pre>benchmark = po.load_benchmark(\"polaris/hello-world-benchmark\")\n</pre> benchmark = po.load_benchmark(\"polaris/hello-world-benchmark\") <p>Loading a benchmark will automatically load the underlying dataset.</p> <p>You can also load the dataset directly.</p> In\u00a0[\u00a0]: Copied! <pre>dataset = po.load_dataset(\"polaris/hello-world\")\n</pre> dataset = po.load_dataset(\"polaris/hello-world\") In\u00a0[\u00a0]: Copied! <pre>train, test = benchmark.get_train_test_split()\n</pre> train, test = benchmark.get_train_test_split() <p>The created objects support various flavours to access the data.</p> <ul> <li>The objects are iterable;</li> <li>The objects can be indexed;</li> <li>The objects have properties to access all data at once.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>for x, y in train:\n    pass\n</pre> for x, y in train:     pass In\u00a0[\u00a0]: Copied! <pre>for i in range(len(train)):\n    x, y = train[i]\n</pre> for i in range(len(train)):     x, y = train[i] In\u00a0[\u00a0]: Copied! <pre>x = train.inputs\ny = train.targets\n</pre> x = train.inputs y = train.targets <p>To avoid accidental access to the test targets, the test object does not expose the labels and will throw an error if you try access them explicitly.</p> In\u00a0[\u00a0]: Copied! <pre>for x in test:\n    pass\n</pre> for x in test:     pass In\u00a0[\u00a0]: Copied! <pre>for i in range(len(test)):\n    x = test[i]\n</pre> for i in range(len(test)):     x = test[i] In\u00a0[\u00a0]: Copied! <pre>x = test.inputs\n\n# NOTE: The below will throw an error!\n# y = test.targets\n</pre> x = test.inputs  # NOTE: The below will throw an error! # y = test.targets <p>We also support conversion to other typical formats.</p> In\u00a0[\u00a0]: Copied! <pre>df_train = train.as_dataframe()\n</pre> df_train = train.as_dataframe() In\u00a0[\u00a0]: Copied! <pre>import datamol as dm\nfrom sklearn.ensemble import RandomForestRegressor\n\n# We will recreate the split to pass a featurization function.\ntrain, test = benchmark.get_train_test_split(featurization_fn=dm.to_fp)\n\n# Define a model and train\nmodel = RandomForestRegressor(max_depth=2, random_state=0)\nmodel.fit(train.X, train.y)\n</pre> import datamol as dm from sklearn.ensemble import RandomForestRegressor  # We will recreate the split to pass a featurization function. train, test = benchmark.get_train_test_split(featurization_fn=dm.to_fp)  # Define a model and train model = RandomForestRegressor(max_depth=2, random_state=0) model.fit(train.X, train.y) In\u00a0[\u00a0]: Copied! <pre>predictions = model.predict(test.X)\n</pre> predictions = model.predict(test.X) <p>As said before, evaluating the submissions should be done through the <code>evaluate()</code> endpoint.</p> In\u00a0[\u00a0]: Copied! <pre>results = benchmark.evaluate(predictions)\nresults\n</pre> results = benchmark.evaluate(predictions) results <p>Before uploading the results to the Hub, you can provide some additional information about the results that will be displayed on the Polaris Hub.</p> In\u00a0[\u00a0]: Copied! <pre># For a complete list of metadata, check out the BenchmarkResults object\nresults.name = \"hello-world-result\"\nresults.github_url = \"https://github.com/polaris-hub/polaris-hub\"\nresults.paper_url = \"https://polarishub.io/\"\nresults.description = \"Hello, World!\"\nresults.tags = [\"random_forest\", \"ecfp\"]\nresults.user_attributes = {\"Framework\": \"Scikit-learn\"}\n</pre> # For a complete list of metadata, check out the BenchmarkResults object results.name = \"hello-world-result\" results.github_url = \"https://github.com/polaris-hub/polaris-hub\" results.paper_url = \"https://polarishub.io/\" results.description = \"Hello, World!\" results.tags = [\"random_forest\", \"ecfp\"] results.user_attributes = {\"Framework\": \"Scikit-learn\"} <p>Finally, let's upload the results to the Hub!</p> In\u00a0[\u00a0]: Copied! <pre>results.upload_to_hub(owner=\"my-username\", access=\"public\")\n</pre> results.upload_to_hub(owner=\"my-username\", access=\"public\") <p>That's it! Just like that you have submitted a result to a Polaris benchmark</p> <p>The End.</p>"},{"location":"tutorials/submit_to_benchmark.html#login","title":"Login\u00b6","text":"<p>We first need to authenticate ourselves using our Polaris account. If you don't have an account yet, you can create one here.</p>"},{"location":"tutorials/submit_to_benchmark.html#load-from-the-hub","title":"Load from the Hub\u00b6","text":"<p>Datasets and benchmarks are identified by a <code>owner/slug</code> id.</p>"},{"location":"tutorials/submit_to_benchmark.html#the-benchmark-api","title":"The Benchmark API\u00b6","text":"<p>The benchmark object provides two main API endpoints.</p> <ul> <li><code>get_train_test_split()</code>: For creating objects through which we can access the different dataset partitions.</li> <li><code>evaluate()</code>: For evaluating a set of predictions in accordance with the benchmark protocol.</li> </ul>"},{"location":"tutorials/submit_to_benchmark.html#train-test-split","title":"Train-test split\u00b6","text":""},{"location":"tutorials/submit_to_benchmark.html#submit-your-results","title":"Submit your results\u00b6","text":"<p>In this example, we will train a simple Random Forest model on the ECFP representation through scikit-learn and datamol.</p>"},{"location":"tutorials/submit_to_competition.html","title":"Submit to a Competition","text":"<p>On Polaris, submitting to a competition is very similar to submitting to a benchmark.</p> <p>The main difference lies in how predictions are prepared and how they are evaluated</p> In\u00a0[\u00a0]: Copied! <pre>import polaris as po\n</pre> import polaris as po In\u00a0[\u00a0]: Copied! <pre>from polaris.hub.client import PolarisHubClient\n\nwith PolarisHubClient() as client:\n    client.login()\n</pre> from polaris.hub.client import PolarisHubClient  with PolarisHubClient() as client:     client.login() In\u00a0[10]: Copied! <pre>competition = po.load_competition(\"polaris/hello-world-competition\")\n</pre> competition = po.load_competition(\"polaris/hello-world-competition\") In\u00a0[\u00a0]: Copied! <pre>competition.submit_predictions(\n    predictions=predictions,\n    prediction_name=\"my-first-predictions\",\n    prediction_owner=\"my-username\",\n    report_url=\"https://www.example.com\", \n    # The below metadata is optional, but recommended.\n    github_url=\"https://github.com/polaris-hub/polaris\",\n    description=\"Just testing the Polaris API here!\",\n    tags=[\"tutorial\"],\n    user_attributes={\"Framework\": \"Scikit-learn\", \"Method\": \"Gradient Boosting\"}\n)\n</pre> competition.submit_predictions(     predictions=predictions,     prediction_name=\"my-first-predictions\",     prediction_owner=\"my-username\",     report_url=\"https://www.example.com\",      # The below metadata is optional, but recommended.     github_url=\"https://github.com/polaris-hub/polaris\",     description=\"Just testing the Polaris API here!\",     tags=[\"tutorial\"],     user_attributes={\"Framework\": \"Scikit-learn\", \"Method\": \"Gradient Boosting\"} ) <p>That's it! Just like that you have partaken in your first Polaris competition.</p> <p>Where are my results?</p> <p>The results will only be published at predetermined intervals, as detailed in the competition details. Keep an eye on that leaderboard when it goes public and best of luck!</p> <p>The End.</p>"},{"location":"tutorials/submit_to_competition.html#login","title":"Login\u00b6","text":"<p>As before, we first need to authenticate ourselves using our Polaris account. If you don't have an account yet, you can create one here.</p>"},{"location":"tutorials/submit_to_competition.html#load-the-competition","title":"Load the Competition\u00b6","text":"<p>As with regular benchmarks, a competition is identified by the <code>owner/slug</code> id.</p>"},{"location":"tutorials/submit_to_competition.html#the-competition-api","title":"The Competition API\u00b6","text":"<p>Similar to the benchmark API, the competition exposes two main API endpoints:</p> <ul> <li><code>get_train_test_split()</code>, which does exactly the same as for benchmarks.</li> <li><code>submit_predictions()</code>, which is used to submit your predictions to a competition.</li> </ul> <p>Note that different from regular benchmarks, competitions don't have an <code>evaluate()</code> endpoint.</p> <p>That's because the evaluation happens server side. This gives the competition organizers precise control over how and when the test set and associated results get published, providing a unique opportunity for unbiased evaluation and comparison of different methods.</p>"},{"location":"tutorials/submit_to_competition.html#submit-your-predictions","title":"Submit your predictions\u00b6","text":"<p>Similar to your actual results, you can also provide metadata about your predictions.</p>"}]}