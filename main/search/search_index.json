{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Introduction","text":"<p>Welcome to the Polaris documentation!</p>"},{"location":"index.html#what-is-polaris","title":"What is Polaris?","text":"<p>Our vision</p> <p>Polaris aims to foster the development of impactful AI models in drug discovery by establishing a new  and adaptive standard for measuring progress of computational tools in drug discovery.</p> <p>Polaris is a suite of tools to implement, host and run benchmarks in computational drug discovery. Existing benchmarks leave several key challenges - related to the characteristics of datasets in drug discovery - unaddressed. This can lead to a situation in which newly proposed models do not perform as well as advertised in real drug discovery programs, ultimately risking misalignment between the scientists developing the models and downstream users. With Polaris, we aim to further close that gap. </p>"},{"location":"index.html#polaris-hub","title":"Polaris Hub","text":"<p>A quick word on the Polaris Hub. The hub hosts a variety of high-quality benchmarks and datasets. While the hub is built to easily integrate with the Polaris library, you can use them independently.</p>"},{"location":"index.html#where-to-next","title":"Where to next?","text":"<p>  Quickstart</p> <p>If you are entirely new to Polaris, this is the place to start! Learn about the essential concepts and partake in your first benchmark.</p> <p> Let's get started</p> <p>  Tutorials</p> <p>Dive deeper into the Polaris code and learn about advanced concepts to create your own benchmarks and datasets. </p> <p> Let's get started</p> <p>  API Reference</p> <p>This is where you will find the technical documentation of the code itself. Learn the intricate details of how the various methods and classes work.</p> <p> Let's get started</p> <p>  Community</p> <p>Whether you are a first-time contributor or open-source veteran, we welcome any contribution to Polaris. Learn more about our community initiatives.</p> <p> Let's get started</p>"},{"location":"quickstart.html","title":"Quickstart","text":""},{"location":"quickstart.html#installation","title":"Installation","text":"<p>First things first, let's install Polaris! </p> <p>We highly recommend using a Conda Python distribution, such as <code>mamba</code>:</p> <pre><code>mamba install -c conda-forge polaris\n</code></pre> Other installation options <p>You can replace <code>mamba</code> by <code>conda</code>. The package is also pip installable if you need it: <code>pip install polaris</code>.</p>"},{"location":"quickstart.html#benchmarking-api","title":"Benchmarking API","text":"<p>At its core, Polaris is a benchmarking library. It provides a simple API to run benchmarks. While it can be used independently, it is built to easily integrate with the Polaris Hub. The hub hosts a variety of high-quality datasets, benchmarks and associated results. </p> <p>If all you care about is to partake in a benchmark that is hosted on the hub, it is as simple as: </p> <pre><code>import polaris as po\nbenchmark = po.load_benchmark(\"org_or_user/name\")\ntrain, test = benchmark.get_train_test_split()\ny_pred = ...  # Work your magic!\nresults = benchmark.evaluate(y_pred)\nresults.upload_to_hub()\n</code></pre> <p>That's all there is to it to partake in a benchmark. No complicated, custom data-loaders or evaluation protocol. With just a few lines of code, you can feel confident that you are properly evaluating your model and focus on what you do best: Solving the hard problems in our domain!  </p> <p>Similarly, you can easily access a dataset.</p> <pre><code>import polaris as po\ndataset = po.load_dataset(\"org_or_user/name\")\ndataset.get_data(col=..., row=...)\n</code></pre>"},{"location":"quickstart.html#core-concepts","title":"Core concepts","text":"<p>At the core of our API are 4 core concepts, each associated with a class: </p> <ol> <li><code>Dataset</code>: The dataset class is carefully designed data-structure, stress-tested on terra-bytes of data, to ensure whatever dataset you can think of, you can easily create, store and use it. </li> <li><code>BenchmarkSpecification</code>: The benchmark specification class wraps a <code>Dataset</code> with additional meta-data to produce a the benchmark. Specifically, it specifies how to evaluate a model's performance on the underlying dataset (e.g. the train-test split and metrics). It provides a simple API to run said evaluation protocol. </li> <li><code>Subset</code>: The subset class should be used as a starting-point for any framework-specific (e.g. PyTorch or Tensorflow) data loaders. To facilitate this, it abstracts away the non-trivial logic of accessing the data and provides several style of access to built upon.</li> <li><code>BenchmarkResults</code>: The benchmark results class stores the results of a benchmark, along with additional meta-data. This object can be easily uploaded to the Polaris Hub and shared with the broader community. </li> </ol>"},{"location":"quickstart.html#where-to-next","title":"Where to next?","text":"<p>Now that you've seen how easy it is to use Polaris, let's dive into the details through a set of tutorials!</p>"},{"location":"api/base.html","title":"Base classes","text":""},{"location":"api/base.html#polaris._artifact.BaseArtifactModel","title":"polaris._artifact.BaseArtifactModel","text":"<p>             Bases: <code>BaseModel</code></p> <p>Base class for all artifacts on the Hub. Specifies meta-data that is used by the Hub.</p> Optional <p>Despite all artifacts basing this class, note that all attributes are optional. This ensures the library can be used without the Polaris Hub. Only when uploading to the Hub, some of the attributes are required.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>Optional[SlugCompatibleStringType]</code> <p>A slug-compatible name for the dataset. Together with the owner, this is used by the Hub to uniquely identify the benchmark.</p> <code>description</code> <code>Optional[str]</code> <p>A beginner-friendly, short description of the dataset.</p> <code>tags</code> <code>list[str]</code> <p>A list of tags to categorize the benchmark by. This is used by the hub to search over benchmarks.</p> <code>user_attributes</code> <code>Dict[str, str]</code> <p>A dict with additional, textual user attributes.</p> <code>owner</code> <code>Optional[HubOwner]</code> <p>A slug-compatible name for the owner of the dataset. If the dataset comes from the Polaris Hub, this is the associated owner (organization or user). Together with the name, this is used by the Hub to uniquely identify the benchmark.</p> <code>_verified</code> <code>bool</code> <p>Whether the benchmark has been verified through the Polaris Hub.</p>"},{"location":"api/base.html#polaris._artifact.BaseArtifactModel.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(path: str)\n</code></pre> <p>Loads a benchmark from a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Loads a benchmark specification from a JSON file.</p> required"},{"location":"api/base.html#polaris._artifact.BaseArtifactModel.to_json","title":"to_json","text":"<pre><code>to_json(path: str)\n</code></pre> <p>Saves the benchmark to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Saves the benchmark specification to a JSON file.</p> required"},{"location":"api/benchmark.html","title":"Base class","text":""},{"location":"api/benchmark.html#polaris.benchmark.BenchmarkSpecification","title":"polaris.benchmark.BenchmarkSpecification","text":"<p>             Bases: <code>BaseArtifactModel</code></p> <p>This class wraps a <code>Dataset</code> with additional data  to specify the evaluation logic.</p> <p>Specifically, it specifies:</p> <ol> <li>Which dataset to use (see <code>Dataset</code>);</li> <li>Which columns are used as input and which columns are used as target;</li> <li>Which metrics should be used to evaluate performance on this task;</li> <li>A predefined, static train-test split to use during evaluation.</li> </ol> Subclasses <p>Polaris includes various subclasses of the <code>BenchmarkSpecification</code> that provide a more precise data-model or  additional logic, e.g. <code>SingleTaskBenchmarkSpecification</code>.</p> <p>Examples:</p> <p>Basic API usage: <pre><code>import polaris as po\nbenchmark = po.load_benchmark(\"/path/to/benchmark\")\ntrain, test = benchmark.get_train_test_split()\n# Work your magic\npredictions = ...\nbenchmark.evaluate(predictions)\n</code></pre></p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>Union[Dataset, str, dict[str, Any]]</code> <p>The dataset the benchmark specification is based on.</p> <code>target_cols</code> <code>ColumnsType</code> <p>The column(s) of the original dataset that should be used as target.</p> <code>input_cols</code> <code>ColumnsType</code> <p>The column(s) of the original dataset that should be used as input.</p> <code>split</code> <code>SplitType</code> <p>The predefined train-test split to use for evaluation.</p> <code>metrics</code> <code>Union[str, Metric, list[Union[str, Metric]]]</code> <p>The metrics to use for evaluating performance</p> <code>main_metric</code> <code>Optional[Union[str, Metric]]</code> <p>The main metric used to rank methods. If <code>None</code>, the first of the <code>metrics</code> field.</p> <code>md5sum</code> <code>Optional[str]</code> <p>The checksum is used to verify the version of the dataset specification. If specified, it will raise an error if the specified checksum doesn't match the computed checksum.</p> <p>For additional meta-data attributes, see the <code>BaseArtifactModel</code> class.</p>"},{"location":"api/benchmark.html#polaris.benchmark.BenchmarkSpecification.get_train_test_split","title":"get_train_test_split","text":"<pre><code>get_train_test_split(\ninput_format: DataFormat = \"dict\", target_format: DataFormat = \"dict\"\n) -&gt; tuple[Subset, Union[Subset, dict[str, Subset]]]\n</code></pre> <p>Construct the train and test sets, given the split in the benchmark specification.</p> <p>Returns <code>Subset</code> objects, which offer several ways of accessing the data and can thus easily serve as a basis to build framework-specific (e.g. PyTorch, Tensorflow) data-loaders on top of.</p> <p>Parameters:</p> Name Type Description Default <code>input_format</code> <code>DataFormat</code> <p>How the input data is returned from the <code>Subset</code> object.</p> <code>'dict'</code> <code>target_format</code> <code>DataFormat</code> <p>How the target data is returned from the <code>Subset</code> object. This will only affect the train set.</p> <code>'dict'</code> <p>Returns:</p> Type Description <code>tuple[Subset, Union[Subset, dict[str, Subset]]]</code> <p>A tuple with the train <code>Subset</code> and test <code>Subset</code> objects. If there are multiple test sets, these are returned in a dictionary and each test set has an associated name. The targets of the test set can not be accessed.</p>"},{"location":"api/benchmark.html#polaris.benchmark.BenchmarkSpecification.evaluate","title":"evaluate","text":"<pre><code>evaluate(y_pred: PredictionsType) -&gt; BenchmarkResults\n</code></pre> <p>Execute the evaluation protocol for the benchmark, given a set of predictions.</p> What about <code>y_true</code>? <p>Contrary to other frameworks that you might be familiar with, we opted for a signature that includes just the predictions. This reduces the chance of accidentally using the test targets during training.</p> <p>For this method, we make the following assumptions:</p> <ol> <li>There can be one or multiple test set(s);</li> <li>There can be one or multiple target(s);</li> <li>The metrics are constant across test sets;</li> <li>The metrics are constant across targets;</li> <li>There can be metrics which measure across tasks.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>y_pred</code> <code>PredictionsType</code> <p>The predictions for the test set, as NumPy arrays. If there are multiple test sets, this should be a dictionary with the test set names as keys.</p> required <p>Returns:</p> Type Description <code>BenchmarkResults</code> <p>A <code>BenchmarkResults</code> object. This object can be directly submitted to the Polaris Hub.</p>"},{"location":"api/benchmark.html#polaris.benchmark.BenchmarkSpecification.to_json","title":"to_json","text":"<pre><code>to_json(destination: str) -&gt; str\n</code></pre> <p>Save the benchmark to a destination directory as a JSON file.</p> Multiple files <p>Perhaps unintuitive, this method creates multiple files in the destination directory as it also saves the dataset it is based on to the specified destination. See the docstring of <code>Dataset.to_json</code> for more information.</p> <p>Parameters:</p> Name Type Description Default <code>destination</code> <code>str</code> <p>The directory to save the associated data to.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The path to the JSON file.</p>"},{"location":"api/benchmark.html#subclasses","title":"Subclasses","text":""},{"location":"api/benchmark.html#polaris.benchmark.SingleTaskBenchmarkSpecification","title":"polaris.benchmark.SingleTaskBenchmarkSpecification","text":"<p>             Bases: <code>BenchmarkSpecification</code></p> <p>Subclass for any single-task benchmark specification</p> <p>In addition to the data-model and logic of the base-class, this class verifies that there is just a single target-column.</p>"},{"location":"api/benchmark.html#polaris.benchmark.MultiTaskBenchmarkSpecification","title":"polaris.benchmark.MultiTaskBenchmarkSpecification","text":"<p>             Bases: <code>BenchmarkSpecification</code></p> <p>Subclass for any multi-task benchmark specification</p> <p>In addition to the data-model and logic of the base-class, this class verifies that there are multiple target-columns.</p>"},{"location":"api/dataset.html","title":"Dataset","text":""},{"location":"api/dataset.html#polaris.dataset.Dataset","title":"polaris.dataset.Dataset","text":"<p>             Bases: <code>BaseArtifactModel</code></p> <p>Basic data-model for a Polaris dataset, implemented as a Pydantic model.</p> <p>At its core, a dataset in Polaris is a tabular data structure that stores data-points in a row-wise manner. A Dataset can have multiple modalities or targets, can be sparse and can be part of one or multiple  <code>BenchmarkSpecification</code> objects.</p> Pointer columns <p>Whereas a <code>Dataset</code> contains all information required to construct a dataset, it is not ready yet. For complex data, such as images, we support storing the content in external blobs of data. In that case, the table contains pointers to these blobs that are dynamically loaded when needed.</p> <p>Attributes:</p> Name Type Description <code>table</code> <code>Union[DataFrame, str]</code> <p>The core data-structure, storing data-points in a row-wise manner. Can be specified as either a path to a <code>.parquet</code> file or a <code>pandas.DataFrame</code>.</p> <code>md5sum</code> <code>Optional[str]</code> <p>The checksum is used to verify the version of the dataset specification. If specified, it will raise an error if the specified checksum doesn't match the computed checksum.</p> <code>annotations</code> <code>Dict[str, ColumnAnnotation]</code> <p>Each column can be annotated with a <code>ColumnAnnotation</code> object. Importantly, this is used to annotate whether a column is a pointer column.</p> <code>source</code> <code>Optional[HttpUrlString]</code> <p>The data source, e.g. a DOI, Github repo or URI.</p> <p>For additional meta-data attributes, see the <code>BaseArtifactModel</code> class.</p> <p>Raises:</p> Type Description <code>InvalidDatasetError</code> <p>If the dataset does not conform to the Pydantic data-model specification.</p> <code>PolarisChecksumError</code> <p>If the specified checksum does not match the computed checksum.</p>"},{"location":"api/dataset.html#polaris.dataset.Dataset.get_data","title":"get_data","text":"<pre><code>get_data(row: Union[str, int], col: str) -&gt; np.ndarray\n</code></pre> <p>Since the dataset might contain pointers to external files, data retrieval is more complicated than just indexing the <code>table</code> attribute. This method provides an end-point for seamlessly accessing the underlying data.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Union[str, int]</code> <p>The row index in the <code>Dataset.table</code> attribute</p> required <code>col</code> <code>str</code> <p>The column index in the <code>Dataset.table</code> attribute</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A numpy array with the data at the specified indices. If the column is a pointer column, the content of the referenced file is loaded to memory.</p>"},{"location":"api/dataset.html#polaris.dataset.Dataset.from_zarr","title":"from_zarr  <code>classmethod</code>","text":"<pre><code>from_zarr(path: str) -&gt; Dataset\n</code></pre> <p>Parse a .zarr hierarchy into a Polaris <code>Dataset</code>.</p> <p>In short: A <code>.zarr</code> file can contain groups and arrays, where each group can again contain groups and arrays. Additional user attributes (for any array or group) are saved as JSON files.</p> <p>Within Polaris:</p> <ol> <li>Each subgroup of the root group corresponds to a single column.</li> <li>Each subgroup can contain:<ul> <li>A single array with all datapoints.</li> <li>A single array per datapoint.</li> </ul> </li> <li>Additional meta-data is saved to the user attributes of the root group.</li> <li>The indices are required to be integers.</li> </ol> Tutorial <p>To learn more about the zarr format, see the tutorial.</p> Beta functionality <p>This feature is still in beta and the API will likely change. Please report any issues you encounter.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the root of the <code>.zarr</code> directory. Should be compatible with fsspec.</p> required"},{"location":"api/dataset.html#polaris.dataset.Dataset.to_zarr","title":"to_zarr","text":"<pre><code>to_zarr(\ndestination: str, array_mode: Dict[str, Literal[\"single\", \"multiple\"]]\n) -&gt; str\n</code></pre> <p>Saves a dataset to a .zarr file. For more information on the resulting structure, see <code>from_zarr</code>.</p> Tutorial <p>To learn more about the zarr format, see the tutorial.</p> Beta functionality <p>This feature is still in beta and the API will likely change. Please report any issues you encounter.</p> <p>Parameters:</p> Name Type Description Default <code>destination</code> <code>str</code> <p>The directory to save the associated data to.</p> required <code>array_mode</code> <code>Dict[str, Literal['single', 'multiple']]</code> <p>For each of the columns, whether to save all datapoints in a single array or create an array per datapoint. Should be one of \"single\" or \"multiple\".</p> required <p>Returns:</p> Type Description <code>str</code> <p>The path to the root zarr file.</p>"},{"location":"api/dataset.html#polaris.dataset.Dataset.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(path: str)\n</code></pre> <p>Loads a benchmark from a JSON file. Overrides the method from the base class to remove the caching dir from the file to load from, as that should be user dependent.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Loads a benchmark specification from a JSON file.</p> required"},{"location":"api/dataset.html#polaris.dataset.Dataset.to_json","title":"to_json","text":"<pre><code>to_json(destination: str) -&gt; str\n</code></pre> <p>Save the dataset to a destination directory as a JSON file.</p> Multiple files <p>Perhaps unintuitive, this method creates multiple files.</p> <ol> <li><code>/path/to/destination/dataset.json</code>: This file can be loaded with     <code>Dataset.from_json</code>.</li> <li><code>/path/to/destination/table.parquet</code>: The <code>Dataset.table</code> attribute is saved here.</li> <li>(Optional) <code>/path/to/destination/data/*</code>: Any additional blobs of data referenced by the         pointer columns will be stored here.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>destination</code> <code>str</code> <p>The directory to save the associated data to.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The path to the JSON file.</p>"},{"location":"api/dataset.html#polaris.dataset.Dataset.cache","title":"cache","text":"<pre><code>cache(cache_dir: Optional[str] = None) -&gt; str\n</code></pre> <p>Caches the dataset by downloading all additional data for pointer columns to a local directory.</p> <p>Parameters:</p> Name Type Description Default <code>cache_dir</code> <code>Optional[str]</code> <p>The directory to cache the data to. If not provided, this will fall back to the <code>Dataset.cache_dir</code> attribute</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The path to the cache directory.</p>"},{"location":"api/dataset.html#polaris.dataset.ColumnAnnotation","title":"polaris.dataset.ColumnAnnotation","text":"<p>             Bases: <code>BaseModel</code></p> <p>The <code>ColumnAnnotation</code> class is used to annotate the columns of the <code>Dataset</code> object. This mostly just stores meta-data and does not affect the logic. The exception is the <code>is_pointer</code> attribute.</p> <p>Attributes:</p> Name Type Description <code>is_pointer</code> <code>bool</code> <p>Annotates whether a column is a pointer column. If so, it does not contain data, but rather contains references to blobs of data from which the data is loaded.</p> <code>modality</code> <code>Union[str, Modality]</code> <p>The data modality describes the data type and is used to categorize datasets on the hub and while it does not affect logic in this library, it does affect the logic of the hub.</p> <code>description</code> <code>Optional[str]</code> <p>Describes how the data was generated.</p> <code>user_attributes</code> <code>Dict[str, str]</code> <p>Any additional meta-data can be stored in the user attributes.</p>"},{"location":"api/evaluation.html","title":"Evaluation","text":""},{"location":"api/evaluation.html#polaris.evaluate.BenchmarkResults","title":"polaris.evaluate.BenchmarkResults","text":"<p>             Bases: <code>BaseArtifactModel</code></p> <p>Class for saving benchmarking results</p> <p>This object is returned by <code>BenchmarkSpecification.evaluate</code>. In addition to the metrics on the test set, it contains additional meta-data and logic to integrate the results with the Polaris Hub.</p> Categorizing methods <p>An open question is how to best categorize a methodology (e.g. a model). This is needed since we would like to be able to aggregate results across benchmarks too, to say something about which (type of) methods performs best in general.</p> <p>Attributes:</p> Name Type Description <code>results</code> <code>ResultsType</code> <p>Benchmark results are stored as a dictionary</p> <code>benchmark_name</code> <code>str</code> <p>The name of the benchmark for which these results were generated. Together with the benchmark owner, this uniquely identifies the benchmark on the Hub.</p> <code>benchmark_owner</code> <code>Optional[HubOwner]</code> <p>The owner of the benchmark for which these results were generated. Together with the benchmark name, this uniquely identifies the benchmark on the Hub.</p> <code>github_url</code> <code>Optional[HttpUrlString]</code> <p>The URL to the GitHub repository of the code used to generate these results.</p> <code>paper_url</code> <code>Optional[HttpUrlString]</code> <p>The URL to the paper describing the methodology used to generate these results.</p> <code>contributors</code> <code>Optional[list[HubUser]]</code> <p>The users that are credited for these results.</p> <code>_created_at</code> <code>datetime</code> <p>The time-stamp at which the results were created. Automatically set.</p> <p>For additional meta-data attributes, see the <code>BaseArtifactModel</code> class.</p>"},{"location":"api/evaluation.html#polaris.evaluate.BenchmarkResults.serialize_results","title":"serialize_results","text":"<pre><code>serialize_results(value: ResultsType)\n</code></pre> <p>Change from the Metric enum to a string representation</p>"},{"location":"api/evaluation.html#polaris.evaluate.MetricInfo","title":"polaris.evaluate.MetricInfo","text":"<p>             Bases: <code>BaseModel</code></p> <p>Metric metadata</p> <p>Attributes:</p> Name Type Description <code>fn</code> <code>Callable</code> <p>The callable that actually computes the metric</p> <code>is_multitask</code> <code>bool</code> <p>Whether the metric expects a single set of predictions or a dict of predictions.</p>"},{"location":"api/evaluation.html#polaris.evaluate.Metric","title":"polaris.evaluate.Metric","text":"<p>             Bases: <code>Enum</code></p> <p>A metric within the Polaris ecosystem is uniquely identified by its name and is associated with additional metadata in a <code>MetricInfo</code> instance.</p> <p>Implemented as an enum.</p>"},{"location":"api/evaluation.html#polaris.evaluate.Metric.score","title":"score","text":"<pre><code>score(y_true: np.ndarray, y_pred: np.ndarray) -&gt; float\n</code></pre> <p>Endpoint for computing the metric.</p> <p>For convenience, calling a <code>Metric</code> will result in this method being called.</p> <pre><code>metric = Metric.mean_absolute_error\nassert metric.score(y_true=first, y_pred=second) == metric(y_true=first, y_pred=second)\n</code></pre>"},{"location":"api/hub.client.html","title":"Client","text":""},{"location":"api/hub.client.html#polaris.hub.settings.PolarisHubSettings","title":"polaris.hub.settings.PolarisHubSettings","text":"<p>             Bases: <code>BaseSettings</code></p> <p>Settings for the OAuth2 Polaris Hub API Client.</p> Secrecy of these settings <p>Since the Polaris Hub uses PCKE (Proof Key for Code Exchange) for OAuth2, these values thus do not have to be kept secret. See RFC 7636 for more info.</p> <p>Attributes:</p> Name Type Description <code>hub_url</code> <code>HttpUrlString</code> <p>The URL to the main page of the Polaris Hub.</p> <code>api_url</code> <code>Optional[HttpUrlString]</code> <p>The URL to the main entrypoint of the Polaris API.</p> <code>authorize_url</code> <code>HttpUrlString</code> <p>The URL of the OAuth2 authorization endpoint.</p> <code>callback_url</code> <code>HttpUrlString</code> <p>The URL to which the user is redirected after authorization.</p> <code>token_fetch_url</code> <code>HttpUrlString</code> <p>The URL of the OAuth2 token endpoint.</p> <code>user_info_url</code> <code>HttpUrlString</code> <p>The URL of the OAuth2 user info endpoint.</p> <code>scopes</code> <code>str</code> <p>The OAuth2 scopes that are requested.</p> <code>client_id</code> <code>str</code> <p>The OAuth2 client ID.</p> <code>ca_bundle</code> <code>Optional[Union[str, bool]]</code> <p>The path to a CA bundle file for requests. Allows for custom SSL certificates to be used.</p>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient","title":"polaris.hub.client.PolarisHubClient","text":"<pre><code>PolarisHubClient(\nenv_file: Optional[Union[str, os.PathLike]] = None,\nsettings: Optional[PolarisHubSettings] = None,\ncache_auth_token: bool = True,\n**kwargs: dict\n)\n</code></pre> <p>             Bases: <code>OAuth2Client</code></p> <p>A client for the Polaris Hub API. The Polaris Hub is a central repository of datasets, benchmarks and results. Visit it here: https://polaris-hub.vercel.app/.</p> <p>Bases the <code>authlib</code> client, which in turns bases the <code>httpx</code> client. See the relevant docs to learn more about how to use these clients outside of the integration with the Polaris Hub.</p> Closing the client <p>The client should be closed after all requests have been made. For convenience, you can also use the client as a context manager to automatically close the client when the context is exited. Note that once the client has been closed, it cannot be used anymore.</p> <pre><code># Make sure to close the client once finished\nclient = PolarisHubClient()\nclient.get(...)\nclient.close()\n# Or use the client as a context manager\nwith PolarisHubClient() as client:\nclient.get(...)\n</code></pre> Async Client <p><code>authlib</code> also supports an async client. Since we don't expect to make multiple requests to the Hub in parallel and due to the added complexity stemming from using the Python asyncio API, we are sticking to the sync client - at least for now.</p> <p>Parameters:</p> Name Type Description Default <code>env_file</code> <code>Optional[Union[str, PathLike]]</code> <p>Path to a <code>.env</code> file containing the settings, used to initialize a <code>PolarisHubSettings</code> instance. If not provided, the default settings are used.</p> <code>None</code> <code>settings</code> <code>Optional[PolarisHubSettings]</code> <p>A <code>PolarisHubSettings</code> instance. If provided, takes precedence over <code>env_file</code>.</p> <code>None</code> <code>cache_auth_token</code> <code>bool</code> <p>Whether to cache the auth token to a file.</p> <code>True</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to the authlib <code>OAuth2Client</code> constructor.</p> <code>{}</code>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.user_info","title":"user_info  <code>property</code>","text":"<pre><code>user_info: dict\n</code></pre> <p>Get information about the currently logged in user through the OAuth2 User Info flow.</p>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.login","title":"login","text":"<pre><code>login(overwrite: bool = False, auto_open_browser: bool = True)\n</code></pre> <p>Login to the Polaris Hub using the OAuth2 protocol.</p> Headless authentication <p>It is currently not possible to login to the Polaris Hub without a browser. See this Github issue for more info.</p> <p>Parameters:</p> Name Type Description Default <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the current token if the user is already logged in.</p> <code>False</code> <code>auto_open_browser</code> <code>bool</code> <p>Whether to automatically open the browser to visit the authorization URL.</p> <code>True</code>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.list_datasets","title":"list_datasets","text":"<pre><code>list_datasets(limit: int = 100, offset: int = 0) -&gt; list[str]\n</code></pre> <p>List all available datasets on the Polaris Hub.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>The maximum number of datasets to return.</p> <code>100</code> <code>offset</code> <code>int</code> <p>The offset from which to start returning datasets.</p> <code>0</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of dataset names in the format <code>owner/dataset_name</code>.</p>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(owner: Union[str, HubOwner], name: str) -&gt; Dataset\n</code></pre> <p>Load a dataset from the Polaris Hub.</p> <p>Parameters:</p> Name Type Description Default <code>owner</code> <code>Union[str, HubOwner]</code> <p>The owner of the dataset. Can be either a user or organization from the Polaris Hub.</p> required <code>name</code> <code>str</code> <p>The name of the dataset.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>A <code>Dataset</code> instance, if it exists.</p>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.list_benchmarks","title":"list_benchmarks","text":"<pre><code>list_benchmarks(limit: int = 100, offset: int = 0) -&gt; list[str]\n</code></pre> <p>List all available benchmarks on the Polaris Hub.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>The maximum number of benchmarks to return.</p> <code>100</code> <code>offset</code> <code>int</code> <p>The offset from which to start returning benchmarks.</p> <code>0</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of benchmark names in the format <code>owner/benchmark_name</code>.</p>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.get_benchmark","title":"get_benchmark","text":"<pre><code>get_benchmark(owner: Union[str, HubOwner], name: str) -&gt; BenchmarkSpecification\n</code></pre> <p>Load a benchmark from the Polaris Hub.</p> <p>Parameters:</p> Name Type Description Default <code>owner</code> <code>Union[str, HubOwner]</code> <p>The owner of the benchmark. Can be either a user or organization from the Polaris Hub.</p> required <code>name</code> <code>str</code> <p>The name of the benchmark.</p> required <p>Returns:</p> Type Description <code>BenchmarkSpecification</code> <p>A <code>BenchmarkSpecification</code> instance, if it exists.</p>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.upload_results","title":"upload_results","text":"<pre><code>upload_results(results: BenchmarkResults)\n</code></pre> <p>Upload the results to the Polaris Hub.</p> Benchmark name and owner <p>Importantly, <code>results.benchmark_name</code> and <code>results.benchmark_owner</code> must be specified and match an existing benchmark on the Polaris Hub. If these results were generated by <code>benchmark.evaluate(...)</code>, this is done automatically.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>BenchmarkResults</code> <p>The results to upload.</p> required"},{"location":"api/subset.html","title":"Subset","text":""},{"location":"api/subset.html#polaris.dataset.Subset","title":"polaris.dataset.Subset","text":"<p>The <code>Subset</code> class provides easy access to a single partition of a split dataset.</p> <p>This should be the starting point for any framework-specific (e.g. PyTorch, Tensorflow) data-loader implementation. How the data is loaded in Polaris can be non-trivial, so this class is provided to abstract away the details. To easily build framework-specific data-loaders, a <code>Subset</code> supports various styles of accessing the data:</p> <ol> <li>In memory: Loads the entire dataset in memory and returns a single array with all datapoints,     this style is accessible through the <code>subset.targets</code> and <code>subset.inputs</code> properties.</li> <li>List: Index the subset like a list, this style is accessible through the <code>subset[idx]</code> syntax.</li> <li>Iterator: Iterate over the subset, this style is accessible through the <code>iter(subset)</code> syntax.</li> </ol> <p>Examples:</p> <p>The different styles of accessing the data:</p> <pre><code>import polaris as po\nbenchmark = po.load_benchmark(...)\ntrain, test = benchmark.get_train_test_split()\n# Load the entire dataset in memory, useful for e.g. scikit-learn.\nX = train.inputs\ny = train.targets\n# Access a single datapoint as with a list, useful for e.g. PyTorch.\nx, y = train[0]\n# Iterate over the dataset, useful for very large datasets.\nfor x, y in train:\n...\n</code></pre> <p>Raises:</p> Type Description <code>TestAccessError</code> <p>When trying to access the targets of the test set (specified by the <code>hide_targets</code> attribute).</p>"},{"location":"api/utils.types.html","title":"Types","text":""},{"location":"api/utils.types.html#polaris.utils.types.SplitIndicesType","title":"SplitIndicesType  <code>module-attribute</code>","text":"<pre><code>SplitIndicesType: TypeAlias = list[int]\n</code></pre> <p>A split is defined by a sequence of integers.</p>"},{"location":"api/utils.types.html#polaris.utils.types.SplitType","title":"SplitType  <code>module-attribute</code>","text":"<pre><code>SplitType: TypeAlias = tuple[\nSplitIndicesType, Union[SplitIndicesType, dict[str, SplitIndicesType]]\n]\n</code></pre> <p>A split is a pair of which the first item is always assumed to be the train set. The second item can either be a single test set or a dictionary with multiple, named test sets.</p>"},{"location":"api/utils.types.html#polaris.utils.types.PredictionsType","title":"PredictionsType  <code>module-attribute</code>","text":"<pre><code>PredictionsType: TypeAlias = Union[\nnp.ndarray, dict[str, Union[np.ndarray, dict[str, np.ndarray]]]\n]\n</code></pre> <p>A prediction is one of three things:</p> <ul> <li>A single array (single-task, single test set)</li> <li>A dictionary of arrays (single-task, multiple test sets) </li> <li>A dictionary of dictionaries of arrays (multi-task, multiple test sets)</li> </ul>"},{"location":"api/utils.types.html#polaris.utils.types.DatapointType","title":"DatapointType  <code>module-attribute</code>","text":"<pre><code>DatapointType: TypeAlias = tuple[DatapointPartType, DatapointPartType]\n</code></pre> <p>A datapoint has:</p> <ul> <li>A single input or multiple inputs (either as dict or tuple)</li> <li>No target, a single target or a multiple targets (either as dict or tuple)</li> </ul>"},{"location":"api/utils.types.html#polaris.utils.types.DataFormat","title":"DataFormat  <code>module-attribute</code>","text":"<pre><code>DataFormat: TypeAlias = Literal['dict', 'tuple']\n</code></pre> <p>The target formats that are supported by the <code>Subset</code> class.</p>"},{"location":"api/utils.types.html#polaris.utils.types.SlugCompatibleStringType","title":"SlugCompatibleStringType  <code>module-attribute</code>","text":"<pre><code>SlugCompatibleStringType: TypeAlias = constr(\npattern=\"^[A-Za-z0-9_-]+$\", min_length=4, max_length=64\n)\n</code></pre> <p>A URL-compatible string that can serve as slug on the hub.</p> <p>Can only use alpha-numeric characters, underscores and dashes.  The string must be at least 4 and at most 64 characters long.</p>"},{"location":"api/utils.types.html#polaris.utils.types.HubUser","title":"HubUser  <code>module-attribute</code>","text":"<pre><code>HubUser: TypeAlias = SlugCompatibleStringType\n</code></pre> <p>A user on the Polaris Hub is identified by a username,  which is a <code>SlugCompatibleStringType</code>.</p>"},{"location":"api/utils.types.html#polaris.utils.types.HttpUrlString","title":"HttpUrlString  <code>module-attribute</code>","text":"<pre><code>HttpUrlString: TypeAlias = Annotated[HttpUrl, AfterValidator(str)]\n</code></pre> <p>A validated URL that will be turned into a string. This is useful for interactions with httpx and authlib, who have their own URL types.</p>"},{"location":"api/utils.types.html#polaris.utils.types.HubOwner","title":"HubOwner","text":"<p>             Bases: <code>BaseModel</code></p> <p>An owner of an artifact on the Polaris Hub</p> <p>Either specifies an organization or a user, but not both. The username is specified as a <code>SlugCompatibleStringType</code>, whereas the organization is specified as a string that can contain only alpha-numeric characters, underscores and dashes. Contrary to the username, an organization name can currently be of arbitrary length.</p>"},{"location":"api/utils.types.html#polaris.utils.types.License","title":"License","text":"<p>             Bases: <code>BaseModel</code></p> <p>An artifact license.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>The license ID. Either from SPDX or custom.</p> <code>reference</code> <code>Optional[HttpUrlString]</code> <p>A reference to the license text. If the ID is found in SPDX, this is automatically set. Else it is required to manually specify this.</p>"},{"location":"tutorials/custom_dataset_benchmark.html","title":"Custom datasets and benchmarks","text":"In\u00a0[1]: Copied! <pre>import platformdirs\nimport datamol as dm\n\n# We will save the data for this tutorial to our cache dir!\nSAVE_DIR = dm.fs.join(platformdirs.user_cache_dir(appname=\"polaris-tutorials\"), \"001\")\n</pre> import platformdirs import datamol as dm  # We will save the data for this tutorial to our cache dir! SAVE_DIR = dm.fs.join(platformdirs.user_cache_dir(appname=\"polaris-tutorials\"), \"001\") In\u00a0[2]: Copied! <pre>import pandas as pd\n\nPATH = \"https://raw.githubusercontent.com/molecularinformatics/Computational-ADME/main/ADME_public_set_3521.csv\"\ntable = pd.read_csv(PATH)\n</pre> import pandas as pd  PATH = \"https://raw.githubusercontent.com/molecularinformatics/Computational-ADME/main/ADME_public_set_3521.csv\" table = pd.read_csv(PATH) <p>Since all data fits is contained within the table, creating a dataset is simple. While optional, we will specify some additional meta-data to demonstrate the API.</p> In\u00a0[3]: Copied! <pre>from polaris.dataset import Dataset, ColumnAnnotation\n\ndataset = Dataset(\n    # The table is the core data-structure required to construct a dataset\n    table=table,\n\n    # All other arguments provide additional meta-data and are optional.\n    # The exception is the `is_pointer` attribute in the `ColumnAnnotation` object, which\n    # we will get back to in a later tutorial.\n    name=\"Fang_2023_DMPK\",\n    description=\"120 prospective data sets, collected over 20 months across six ADME in vitro endpoints\",\n    source=\"https://doi.org/10.1021/acs.jcim.3c00160\",\n    annotations={\n        \"SMILES\": ColumnAnnotation(modality=\"molecule\"),\n        \"LOG HLM_CLint (mL/min/kg)\": ColumnAnnotation(user_attributes={\"unit\": \"mL/min/kg\"}),\n        \"LOG SOLUBILITY PH 6.8 (ug/mL)\": ColumnAnnotation(\n            protocol=\"Solubility was measured after equilibrium between the dissolved and solid state\"\n        ),\n    }\n)\n</pre> from polaris.dataset import Dataset, ColumnAnnotation  dataset = Dataset(     # The table is the core data-structure required to construct a dataset     table=table,      # All other arguments provide additional meta-data and are optional.     # The exception is the `is_pointer` attribute in the `ColumnAnnotation` object, which     # we will get back to in a later tutorial.     name=\"Fang_2023_DMPK\",     description=\"120 prospective data sets, collected over 20 months across six ADME in vitro endpoints\",     source=\"https://doi.org/10.1021/acs.jcim.3c00160\",     annotations={         \"SMILES\": ColumnAnnotation(modality=\"molecule\"),         \"LOG HLM_CLint (mL/min/kg)\": ColumnAnnotation(user_attributes={\"unit\": \"mL/min/kg\"}),         \"LOG SOLUBILITY PH 6.8 (ug/mL)\": ColumnAnnotation(             protocol=\"Solubility was measured after equilibrium between the dissolved and solid state\"         ),     } ) In\u00a0[4]: Copied! <pre>import numpy as np\nfrom polaris.benchmark import SingleTaskBenchmarkSpecification\n\n# For the sake of simplicity, we use a very simple, ordered split\nsplit = (\n    np.arange(3000).tolist(), # train\n    (np.arange(521) + 3000).tolist()  # test\n)\n\nbenchmark = SingleTaskBenchmarkSpecification(\n    dataset=dataset,\n    target_cols=\"LOG SOLUBILITY PH 6.8 (ug/mL)\",\n    input_cols=\"SMILES\",\n    split=split,\n    metrics=\"mean_absolute_error\",\n)\n</pre> import numpy as np from polaris.benchmark import SingleTaskBenchmarkSpecification  # For the sake of simplicity, we use a very simple, ordered split split = (     np.arange(3000).tolist(), # train     (np.arange(521) + 3000).tolist()  # test )  benchmark = SingleTaskBenchmarkSpecification(     dataset=dataset,     target_cols=\"LOG SOLUBILITY PH 6.8 (ug/mL)\",     input_cols=\"SMILES\",     split=split,     metrics=\"mean_absolute_error\", ) <p>Metrics should be supported in the polaris framework For more information, see the <code>Metric</code> class.</p> In\u00a0[5]: Copied! <pre>from polaris.evaluate import Metric\n\nlist(Metric)\n</pre> from polaris.evaluate import Metric  list(Metric) Out[5]: <pre>['accuracy', 'mean_absolute_error', 'mean_squared_error']</pre> <p>To support to vast flexibility in specifying a benchmark, we have different classes that correspond to different types of benchmarks. Each of these sub-classes make the data-model or logic more specific to a particular case. For example, trying to create a multi-task benchmark with the same arguments will throw an error as there is just a single target column specified.</p> In\u00a0[6]: Copied! <pre>from polaris.benchmark import MultiTaskBenchmarkSpecification\n\nbenchmark = MultiTaskBenchmarkSpecification(\n    dataset=dataset,\n    target_cols=\"LOG SOLUBILITY PH 6.8 (ug/mL)\",\n    input_cols=\"SMILES\",\n    split=split,\n    metrics=\"mean_absolute_error\",\n)\n</pre> from polaris.benchmark import MultiTaskBenchmarkSpecification  benchmark = MultiTaskBenchmarkSpecification(     dataset=dataset,     target_cols=\"LOG SOLUBILITY PH 6.8 (ug/mL)\",     input_cols=\"SMILES\",     split=split,     metrics=\"mean_absolute_error\", ) <pre>\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\nCell In[6], line 3\n      1 from polaris.benchmark import MultiTaskBenchmarkSpecification\n----&gt; 3 benchmark = MultiTaskBenchmarkSpecification(\n      4 dataset=dataset,       5 target_cols=\"LOG SOLUBILITY PH 6.8 (ug/mL)\",\n      6 input_cols=\"SMILES\",\n      7 split=split,\n      8 metrics=\"mean_absolute_error\",\n      9 )\n\nFile ~/local/conda/envs/polaris/lib/python3.11/site-packages/pydantic/main.py:150, in BaseModel.__init__(__pydantic_self__, **data)\n    148 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    149 __tracebackhide__ = True\n--&gt; 150 __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)\n\nValidationError: 1 validation error for MultiTaskBenchmarkSpecification\ntarget_cols\n  Value error, A multi-task benchmark should specify at least two target columns [type=value_error, input_value='LOG SOLUBILITY PH 6.8 (ug/mL)', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.1.2/v/value_error</pre> In\u00a0[7]: Copied! <pre># Let's try that again, but now with two target columns\nbenchmark = MultiTaskBenchmarkSpecification(\n    dataset=dataset,\n    target_cols=[\"LOG SOLUBILITY PH 6.8 (ug/mL)\", \"LOG HLM_CLint (mL/min/kg)\"],\n    input_cols=\"SMILES\",\n    split=split,\n    metrics=\"mean_absolute_error\",\n)\n</pre> # Let's try that again, but now with two target columns benchmark = MultiTaskBenchmarkSpecification(     dataset=dataset,     target_cols=[\"LOG SOLUBILITY PH 6.8 (ug/mL)\", \"LOG HLM_CLint (mL/min/kg)\"],     input_cols=\"SMILES\",     split=split,     metrics=\"mean_absolute_error\", ) In\u00a0[8]: Copied! <pre>path = benchmark.to_json(SAVE_DIR)\n</pre> path = benchmark.to_json(SAVE_DIR) In\u00a0[9]: Copied! <pre>fs = dm.fs.get_mapper(SAVE_DIR).fs\nfs.ls(SAVE_DIR)\n</pre> fs = dm.fs.get_mapper(SAVE_DIR).fs fs.ls(SAVE_DIR) Out[9]: <pre>['/home/cas/.cache/polaris-tutorials/001/benchmark.json',\n '/home/cas/.cache/polaris-tutorials/001/dataset.json',\n '/home/cas/.cache/polaris-tutorials/001/table.parquet']</pre> <p>This created three files. Two <code>json</code> files and a single <code>parquet</code> file. The <code>parquet</code> file saves the tabular structure at the base of the <code>Dataset</code> class, whereas the <code>json</code> files save all the meta-data for the <code>Dataset</code> and <code>BenchmarkSpecification</code>.</p> In\u00a0[10]: Copied! <pre>import polaris as po\n\nbenchmark = po.load_benchmark(path)\n</pre> import polaris as po  benchmark = po.load_benchmark(path) In\u00a0[11]: Copied! <pre>train, test = benchmark.get_train_test_split()\n</pre> train, test = benchmark.get_train_test_split() <p>The created objects support various flavours to access the data.</p> <ol> <li>The objects are iterable;</li> <li>The objects can be indexed;</li> <li>The objects have properties to access all data at once.</li> </ol> In\u00a0[12]: Copied! <pre>for x, y in train:\n    pass\n</pre> for x, y in train:     pass In\u00a0[13]: Copied! <pre>for i in range(len(train)):\n    x, y = train[i]\n</pre> for i in range(len(train)):     x, y = train[i] In\u00a0[14]: Copied! <pre>x = train.inputs\ny = train.targets\n</pre> x = train.inputs y = train.targets <p>To avoid accidental access to the test targets, the test object does not expose the labels and will throw an error if you try access them explicitly.</p> In\u00a0[15]: Copied! <pre>for x in test:\n    pass\n</pre> for x in test:     pass In\u00a0[16]: Copied! <pre>for i in range(len(test)):\n    x = test[i]\n</pre> for i in range(len(test)):     x = test[i] In\u00a0[17]: Copied! <pre>x = test.inputs\ny = test.targets\n</pre> x = test.inputs y = test.targets <pre>\n---------------------------------------------------------------------------\nTestAccessError                           Traceback (most recent call last)\nCell In[17], line 2\n      1 x = test.inputs\n----&gt; 2 y = test.targets\n\nFile ~/Documents/repositories/polaris/polaris/dataset/_subset.py:103, in Subset.targets(self)\n     97 @property\n     98 def targets(self):\n     99 \"\"\"\n    100     Scikit-learn style access to the targets.\n    101     If the dataset is multi-target, this will return a dict of arrays.\n    102     \"\"\"\n--&gt; 103     return self.as_array(\"y\")\n\nFile ~/Documents/repositories/polaris/polaris/dataset/_subset.py:139, in Subset.as_array(self, data_type)\n    136     return self.as_array(\"x\"), self.as_array(\"y\")\n    138 if data_type == \"y\" and self._hide_targets:\n--&gt; 139     raise TestAccessError(\"Within Polaris, you should not need to access the targets of the test set\")\n    141 if not self.is_multi_task:\n    142     return np.array([self._extract(ret, data_type) for ret in self])\n\nTestAccessError: Within Polaris, you should not need to access the targets of the test set</pre> In\u00a0[18]: Copied! <pre># Since we have a multi-task dataset, we should provide predictions for both targets\ny_pred = {\n    \"LOG SOLUBILITY PH 6.8 (ug/mL)\": np.random.random(len(test)),\n    \"LOG HLM_CLint (mL/min/kg)\": np.random.random(len(test)),\n}\n\nresults = benchmark.evaluate(y_pred)\n</pre> # Since we have a multi-task dataset, we should provide predictions for both targets y_pred = {     \"LOG SOLUBILITY PH 6.8 (ug/mL)\": np.random.random(len(test)),     \"LOG HLM_CLint (mL/min/kg)\": np.random.random(len(test)), }  results = benchmark.evaluate(y_pred) <p>The resulting object does not just store the results, but also allows for additional meta-data that can be uploaded to the hub.</p> In\u00a0[19]: Copied! <pre>results.results\n</pre> results.results Out[19]: <pre>{'LOG SOLUBILITY PH 6.8 (ug/mL)': {'mean_absolute_error': 0.9311836043999315},\n 'LOG HLM_CLint (mL/min/kg)': {'mean_absolute_error': 0.7306570958492925}}</pre> In\u00a0[20]: Copied! <pre>results._created_at\n</pre> results._created_at Out[20]: <pre>datetime.datetime(2023, 7, 20, 16, 36, 20, 24731)</pre> <p>This will currently fail as we do not have a client (or a Hub for that matter) yet.</p> In\u00a0[21]: Copied! <pre># results.upload_to_hub()\n</pre> # results.upload_to_hub() <p>The End.</p>"},{"location":"tutorials/custom_dataset_benchmark.html#custom-datasets-and-benchmarks","title":"Custom datasets and benchmarks\u00b6","text":"<p>We have already seen how easy it is to load a benchmark or dataset from the Polaris Hub. Let's now see how you could create your own!</p>"},{"location":"tutorials/custom_dataset_benchmark.html#create-the-dataset","title":"Create the dataset\u00b6","text":"<p>A dataset in Polaris is at its core a tabular data-structure in which each row stores a single datapoint. For this example, we will process a multi-task DMPK dataset from <code>Fang et al.</code>. For the sake of simplicity, we don't do any curation and download the dataset as is from their Github.</p> <p>The importance of curation</p> <p>While we do not address it in this tutorial, data curation is essential to an impactful benchmark. Because of this, we have not just made several high-quality benchmarks readily available on the Polaris Hub, but also open-sourced some of the tools we've built to curate these datasets.</p>"},{"location":"tutorials/custom_dataset_benchmark.html#create-the-benchmark-specification","title":"Create the benchmark specification\u00b6","text":"<p>A benchmark is represented by the <code>BenchmarkSpecification</code>, which wraps a <code>Dataset</code> with additional data to produce a benchmark.</p> <p>Specifically, it specifies:</p> <ol> <li>Which dataset to use (see Dataset);</li> <li>Which columns are used as input and which columns are used as target;</li> <li>Which metrics should be used to evaluate performance on this task;</li> <li>A predefined, static train-test split to use during evaluation.</li> </ol>"},{"location":"tutorials/custom_dataset_benchmark.html#save-the-benchmark","title":"Save the benchmark\u00b6","text":"<p>Saving the benchmark is easy and can be done with a single line of code.</p>"},{"location":"tutorials/custom_dataset_benchmark.html#load-the-benchmark","title":"Load the benchmark\u00b6","text":"<p>Loading the benchmark is easy!</p>"},{"location":"tutorials/custom_dataset_benchmark.html#use-the-benchmark","title":"Use the benchmark\u00b6","text":"<p>Using your custom benchmark is seamless. It supports the exact same API as any benchmark that would be loaded through the hub:</p> <ol> <li><code>get_train_test_split()</code>: For creating objects through which we can access the different dataset partitions.</li> <li><code>evaluate()</code>: For evaluating a set of predictions in accordance with the benchmark protocol.</li> </ol>"},{"location":"tutorials/custom_dataset_benchmark.html#data-access","title":"Data access\u00b6","text":""},{"location":"tutorials/custom_dataset_benchmark.html#evaluation","title":"Evaluation\u00b6","text":"<p>To evaluate a set of predictions within Polaris, you should use the <code>evaluate()</code> endpoint.This requires you to just provide the predictions. The targets of the test set are automatically extract so that the chance of the user accessing the test labels is minimal.</p>"},{"location":"tutorials/dataset_zarr.html","title":"Creating datasets with zarr","text":"In\u00a0[1]: Copied! <pre>import zarr\nimport platformdirs\n\nimport numpy as np\nimport datamol as dm\nimport pandas as pd\n\nSAVE_DIR = dm.fs.join(platformdirs.user_cache_dir(appname=\"polaris-tutorials\"), \"002\")\n</pre> import zarr import platformdirs  import numpy as np import datamol as dm import pandas as pd  SAVE_DIR = dm.fs.join(platformdirs.user_cache_dir(appname=\"polaris-tutorials\"), \"002\") In\u00a0[2]: Copied! <pre># Create a single image and save it to a .zarr directory\nimages = np.random.random((2, 64, 64, 3))\n\ntrain_path = dm.fs.join(SAVE_DIR, \"single_train.zarr\")\nzarr.save(train_path, images[0])\n\ntest_path = dm.fs.join(SAVE_DIR, \"single_test.zarr\")\nzarr.save(test_path, images[1])\n</pre> # Create a single image and save it to a .zarr directory images = np.random.random((2, 64, 64, 3))  train_path = dm.fs.join(SAVE_DIR, \"single_train.zarr\") zarr.save(train_path, images[0])  test_path = dm.fs.join(SAVE_DIR, \"single_test.zarr\") zarr.save(test_path, images[1]) In\u00a0[3]: Copied! <pre>table = pd.DataFrame({\n    \"images\": [train_path, test_path],  # Instead of the content, we specify paths \n    \"target\": np.random.random(2)\n})\n</pre> table = pd.DataFrame({     \"images\": [train_path, test_path],  # Instead of the content, we specify paths      \"target\": np.random.random(2) }) In\u00a0[4]: Copied! <pre>from polaris.dataset import Dataset, ColumnAnnotation\n\ndataset = Dataset(\n    table=table,\n    # To indicate that we are dealing with a pointer column here, \n    # we need to annotate the column. \n    annotations={\"images\": ColumnAnnotation(is_pointer=True)}\n)\n</pre> from polaris.dataset import Dataset, ColumnAnnotation  dataset = Dataset(     table=table,     # To indicate that we are dealing with a pointer column here,      # we need to annotate the column.      annotations={\"images\": ColumnAnnotation(is_pointer=True)} ) <p>Note how the table does not contain the image data, but rather stores a path.</p> In\u00a0[5]: Copied! <pre>dataset.table.loc[0, \"images\"]\n</pre> dataset.table.loc[0, \"images\"] Out[5]: <pre>'/home/cas/.cache/polaris-tutorials/002/single_train.zarr'</pre> <p>To load the data that is being pointed to, you can simply use the <code>Dataset.get_data()</code> utility method.</p> In\u00a0[6]: Copied! <pre>dataset.get_data(col=\"images\", row=0).shape\n</pre> dataset.get_data(col=\"images\", row=0).shape Out[6]: <pre>(64, 64, 3)</pre> <p>Creating a benchmark and the associated <code>Subset</code> objects will automatically do so!</p> In\u00a0[7]: Copied! <pre>from polaris.benchmark import SingleTaskBenchmarkSpecification\n\nbenchmark = SingleTaskBenchmarkSpecification(\n    dataset=dataset, \n    input_cols=\"images\",\n    target_cols=\"target\",\n    metrics=\"mean_absolute_error\",\n    split=([0], [1])\n)\n</pre> from polaris.benchmark import SingleTaskBenchmarkSpecification  benchmark = SingleTaskBenchmarkSpecification(     dataset=dataset,      input_cols=\"images\",     target_cols=\"target\",     metrics=\"mean_absolute_error\",     split=([0], [1]) ) In\u00a0[8]: Copied! <pre>train, test = benchmark.get_train_test_split()\n\nfor x, y in train: \n    # At this point, the content is loaded from the path specified in the table\n    print(x.shape)\n</pre> train, test = benchmark.get_train_test_split()  for x, y in train:      # At this point, the content is loaded from the path specified in the table     print(x.shape) <pre>(64, 64, 3)\n</pre> In\u00a0[9]: Copied! <pre># Let's first create some dummy dataset with 1000 64x64 \"images\"\nimages = np.random.random((1000, 64, 64, 3))\n</pre> # Let's first create some dummy dataset with 1000 64x64 \"images\" images = np.random.random((1000, 64, 64, 3)) <p>To be able to use these images in Polaris, we need to save them in the zarr hierarchy.</p> In\u00a0[10]: Copied! <pre>path = dm.fs.join(SAVE_DIR, \"zarr\", \"archive_multi.zarr\")\n\nwith zarr.open(path, \"w\") as root: \n    with root.create_group(\"images\") as group: \n        for i, arr in enumerate(images): \n            # If you're saving an array per datapoint, \n            # the name of the array needs to be an integer\n            group.array(i, arr)\n    \n    #he root directory can furthermore contain all additional meta-data in its user attributes. \n    root.attrs[\"name\"] = \"dummy_image_dataset\"\n    root.attrs[\"description\"] = \"Randomly generated 64x64 images\"\n    root.attrs[\"source\"] = \"https://doi.org/xx.xxxx\"\n\n    # To ensure proper processing, it is important that we annotate the column. \n    # As this has to be JSON serializable, we create a dict instead of the object. \n    # Due to using Pydantic, this will work seamlessly. \n    root.attrs[\"annotations\"] = {\"images\": {\"is_pointer\": True}}\n</pre> path = dm.fs.join(SAVE_DIR, \"zarr\", \"archive_multi.zarr\")  with zarr.open(path, \"w\") as root:      with root.create_group(\"images\") as group:          for i, arr in enumerate(images):              # If you're saving an array per datapoint,              # the name of the array needs to be an integer             group.array(i, arr)          #he root directory can furthermore contain all additional meta-data in its user attributes.      root.attrs[\"name\"] = \"dummy_image_dataset\"     root.attrs[\"description\"] = \"Randomly generated 64x64 images\"     root.attrs[\"source\"] = \"https://doi.org/xx.xxxx\"      # To ensure proper processing, it is important that we annotate the column.      # As this has to be JSON serializable, we create a dict instead of the object.      # Due to using Pydantic, this will work seamlessly.      root.attrs[\"annotations\"] = {\"images\": {\"is_pointer\": True}} In\u00a0[11]: Copied! <pre>dataset = Dataset.from_zarr(path)\n</pre> dataset = Dataset.from_zarr(path) In\u00a0[12]: Copied! <pre>dataset.get_data(col=\"images\", row=0).shape\n</pre> dataset.get_data(col=\"images\", row=0).shape Out[12]: <pre>(64, 64, 3)</pre> In\u00a0[13]: Copied! <pre>path = dm.fs.join(SAVE_DIR, \"zarr\", \"archive_single.zarr\")\n\nwith zarr.open(path, \"w\") as root: \n    with root.create_group(\"images\") as group: \n        group.array(\"data\", images)\n</pre> path = dm.fs.join(SAVE_DIR, \"zarr\", \"archive_single.zarr\")  with zarr.open(path, \"w\") as root:      with root.create_group(\"images\") as group:          group.array(\"data\", images) In\u00a0[14]: Copied! <pre>dataset = Dataset.from_zarr(path)\n\n# The path refers to the original zarr directory we created in the above code block\ndataset.table.iloc[0][\"images\"]\n</pre> dataset = Dataset.from_zarr(path)  # The path refers to the original zarr directory we created in the above code block dataset.table.iloc[0][\"images\"] Out[14]: <pre>'/home/cas/.cache/polaris-tutorials/002/zarr/archive_single.zarr//images/data#0'</pre> In\u00a0[15]: Copied! <pre>dataset.get_data(col=\"images\", row=0).shape\n</pre> dataset.get_data(col=\"images\", row=0).shape Out[15]: <pre>(64, 64, 3)</pre> In\u00a0[16]: Copied! <pre>savedir = dm.fs.join(SAVE_DIR, \"json\")\njson_path = dataset.to_json(savedir)\n</pre> savedir = dm.fs.join(SAVE_DIR, \"json\") json_path = dataset.to_json(savedir) In\u00a0[17]: Copied! <pre>fs = dm.fs.get_mapper(path).fs\nfs.ls(SAVE_DIR)\n</pre> fs = dm.fs.get_mapper(path).fs fs.ls(SAVE_DIR) Out[17]: <pre>['/home/cas/.cache/polaris-tutorials/002/benchmark.json',\n '/home/cas/.cache/polaris-tutorials/002/single_train.zarr',\n '/home/cas/.cache/polaris-tutorials/002/dataset.json',\n '/home/cas/.cache/polaris-tutorials/002/table.parquet',\n '/home/cas/.cache/polaris-tutorials/002/zarr',\n '/home/cas/.cache/polaris-tutorials/002/single_test.zarr',\n '/home/cas/.cache/polaris-tutorials/002/json']</pre> <p>Besides the <code>table.parquet</code> and <code>dataset.yaml</code>, we can now also see a <code>data</code> folder which stores the content for the additional content from the pointer columns. Instead, we might want to rather save as a single <code>.zarr</code> file. With the <code>array_mode</code> argument, we can choose between the two structures we outlined in this repository.</p> In\u00a0[18]: Copied! <pre>savedir = dm.fs.join(SAVE_DIR, \"zarr\")\nzarr_path = dataset.to_zarr(savedir, array_mode=\"single\")\n</pre> savedir = dm.fs.join(SAVE_DIR, \"zarr\") zarr_path = dataset.to_zarr(savedir, array_mode=\"single\") In\u00a0[19]: Copied! <pre>Dataset.from_json(json_path)\n</pre> Dataset.from_json(json_path) In\u00a0[20]: Copied! <pre>Dataset.from_zarr(zarr_path)\n</pre> Dataset.from_zarr(zarr_path) <p>The End.</p>"},{"location":"tutorials/dataset_zarr.html#creating-datasets-with-zarr","title":"Creating datasets with zarr\u00b6","text":""},{"location":"tutorials/dataset_zarr.html#pointer-columns","title":"Pointer columns\u00b6","text":"<p>Not all data might fit the tabular format, e.g. images or conformers. In that case, we have pointer columns. Pointer columns do not contain the data itself, but rather store a reference to an external file from which the content can be loaded.</p> <p>For now, we only support <code>.zarr</code> files as references. To learn more about <code>.zarr</code>, visit their documentation. Their tutorial is a specifically good read to better understand the main features.</p>"},{"location":"tutorials/dataset_zarr.html#dummy-example","title":"Dummy example\u00b6","text":"<p>For the sake of simplicity, let's assume we have just two datapoints. We will use this to demonstrate the idea behind pointer columns.</p>"},{"location":"tutorials/dataset_zarr.html#creating-datasets-from-zarr-arrays","title":"Creating datasets from <code>.zarr</code> arrays\u00b6","text":"<p>While the above example works, creating the table with all paths from scratch is time-consuming when datasets get large. Instead, you can also automatically parse a <code>.zarr</code> hierarchy into the expected tabular data structure.</p> <p>A little more about zarr: A <code>.zarr</code> file can contain groups and arrays, where each group can again contain groups and arrays. Each array can be saved as one or multiple chunks. Additional user attributes (for any array or group) are saved as JSON files.</p> <p>Within Polaris:</p> <ol> <li>Each subgroup of the root group corresponds to a single column.</li> <li>Each subgroup can contain:<ul> <li>A single array with all datapoints.</li> <li>A single array per datapoint.</li> </ul> </li> <li>Additional meta-data is saved to the user attributes of the root group.</li> <li>The indices are required to be integers.</li> </ol> <p>To better explain what this works, let's look at two examples corresponding to the two cases in point 2 above.</p>"},{"location":"tutorials/dataset_zarr.html#a-single-array-per-data-point","title":"A single array per data point\u00b6","text":"<p>In this first example we will create a zarr array per data point. The structure of the zarr will look like:</p> <pre><code>/\n  column_a/\n      array_1\n      array_2\n      ...\n      array_N\n</code></pre> <p>and as we will see, this will get parsed into</p> column_a /path/to/root.zarr/column_a/array_1 /path/to/root.zarr/column_a/array_2 ... /path/to/root.zarr/column_a/array_N <p>Note</p> <p>Notice dataset now no longer stores the content of the array itself, but rather a reference to the array.</p>"},{"location":"tutorials/dataset_zarr.html#a-single-array-for-all-datapoints","title":"A single array for all datapoints\u00b6","text":"<p>Instead of having an array per datapoint, you might also batch all arrays in a single array. This could for example speed up compression.</p> <p>In this case, our zarr hierarchy will look like this:</p> <pre><code>/\n  column_a/\n      array\n</code></pre> <p>Which will get parsed into a table like:</p> column_a /path/to/root.zarr/column_a/array#1 /path/to/root.zarr/column_a/array#2 ... /path/to/root.zarr/column_a/array#N <p>Note</p> <p>Notice the # suffix in the path, which indicates the index at which the data-point is stored within the big array. </p>"},{"location":"tutorials/dataset_zarr.html#saving-the-dataset","title":"Saving the dataset\u00b6","text":"<p>We can still easily save the dataset. All the pointer columns will be automatically updated.</p>"},{"location":"tutorials/dataset_zarr.html#load-the-dataset","title":"Load the dataset\u00b6","text":""}]}