{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Introduction","text":"<p>Welcome to the Polaris documentation!</p>"},{"location":"index.html#what-is-polaris","title":"What is Polaris?","text":"<p>Our vision</p> <p>Polaris aims to foster the development of impactful AI models in drug discovery by establishing a new  and adaptive standard for measuring progress of computational tools in drug discovery.</p> <p>Polaris is a suite of tools to implement, host and run benchmarks in computational drug discovery. Existing benchmarks leave several key challenges - related to the characteristics of datasets in drug discovery - unaddressed. This can lead to a situation in which newly proposed models do not perform as well as advertised in real drug discovery programs, ultimately risking misalignment between the scientists developing the models and downstream users. With Polaris, we aim to further close that gap. </p>"},{"location":"index.html#polaris-hub","title":"Polaris Hub","text":"<p>A quick word on the Polaris Hub. The hub hosts a variety of high-quality benchmarks and datasets. While the hub is built to easily integrate with the Polaris library, you can use them independently.</p>"},{"location":"index.html#where-to-next","title":"Where to next?","text":"<p>  Quickstart</p> <p>If you are entirely new to Polaris, this is the place to start! Learn about the essential concepts and partake in your first benchmark.</p> <p> Let's get started</p> <p>  Tutorials</p> <p>Dive deeper into the Polaris code and learn about advanced concepts to create your own benchmarks and datasets. </p> <p> Let's get started</p> <p>  API Reference</p> <p>This is where you will find the technical documentation of the code itself. Learn the intricate details of how the various methods and classes work.</p> <p> Let's get started</p> <p>  Community</p> <p>Whether you are a first-time contributor or open-source veteran, we welcome any contribution to Polaris. Learn more about our community initiatives.</p> <p> Let's get started</p>"},{"location":"quickstart.html","title":"Quickstart","text":""},{"location":"quickstart.html#installation","title":"Installation","text":"<p>First things first, let's install Polaris!</p> <p>We highly recommend using a Conda Python distribution, such as <code>mamba</code>:</p> <pre><code>mamba install -c conda-forge polaris\n</code></pre> Other installation options <p>You can replace <code>mamba</code> by <code>conda</code>. The package is also pip installable if you need it: <code>pip install polaris-lib</code>.</p>"},{"location":"quickstart.html#benchmarking-api","title":"Benchmarking API","text":"<p>At its core, Polaris is a benchmarking library. It provides a simple API to run benchmarks. While it can be used independently, it is built to easily integrate with the Polaris Hub. The hub hosts a variety of high-quality datasets, benchmarks and associated results.</p> <p>If all you care about is to partake in a benchmark that is hosted on the hub, it is as simple as:</p> <pre><code>import polaris as po\n\n# Load the benchmark from the Hub\nbenchmark = po.load_benchmark(\"polaris/hello-world-benchmark\")\n\n# Get the train and test data-loaders\ntrain, test = benchmark.get_train_test_split()\n\n# Use the training data to train your model\n# Get the input as an array with 'train.inputs' and 'train.targets'  \n# Or simply iterate over the train object.\nfor x, y in train:\n    ...\n\n# Work your magic to accurately predict the test set\npredictions = [0.0 for x in test]\n\n# Evaluate your predictions\nresults = benchmark.evaluate(predictions)\n\n# Submit your results\nresults.upload_to_hub(owner=\"dummy-user\")\n</code></pre> <p>That's all there is to it to partake in a benchmark. No complicated, custom data-loaders or evaluation protocol. With just a few lines of code, you can feel confident that you are properly evaluating your model and focus on what you do best: Solving the hard problems in our domain!</p> <p>Similarly, you can easily access a dataset.</p> <pre><code>import polaris as po\n\n# Load the dataset from the hub\ndataset = po.load_dataset(\"polaris/hello-world\")\n\n# Get information on the dataset size\ndataset.size()\n\n# Load a datapoint in memory\ndataset.get_data(\n    row=dataset.rows[0],\n    col=dataset.columns[0],\n)\n\n# Or, similarly:\ndataset[dataset.rows[0], dataset.columns[0]]\n\n# Get the first 10 rows in memory\ndataset[:10]\n</code></pre>"},{"location":"quickstart.html#core-concepts","title":"Core concepts","text":"<p>At the core of our API are 4 core concepts, each associated with a class:</p> <ol> <li><code>Dataset</code>: The dataset class is carefully designed data-structure, stress-tested on terra-bytes of data, to ensure whatever dataset you can think of, you can easily create, store and use it.</li> <li><code>BenchmarkSpecification</code>: The benchmark specification class wraps a <code>Dataset</code> with additional meta-data to produce a the benchmark. Specifically, it specifies how to evaluate a model's performance on the underlying dataset (e.g. the train-test split and metrics). It provides a simple API to run said evaluation protocol.</li> <li><code>Subset</code>: The subset class should be used as a starting-point for any framework-specific (e.g. PyTorch or Tensorflow) data loaders. To facilitate this, it abstracts away the non-trivial logic of accessing the data and provides several style of access to built upon.</li> <li><code>BenchmarkResults</code>: The benchmark results class stores the results of a benchmark, along with additional meta-data. This object can be easily uploaded to the Polaris Hub and shared with the broader community.</li> </ol>"},{"location":"quickstart.html#where-to-next","title":"Where to next?","text":"<p>Now that you've seen how easy it is to use Polaris, let's dive into the details through a set of tutorials!</p>"},{"location":"api/adapters.html","title":"Data Adapters","text":""},{"location":"api/adapters.html#polaris.dataset._adapters","title":"polaris.dataset._adapters","text":""},{"location":"api/adapters.html#polaris.dataset._adapters.Adapter","title":"Adapter","text":"<p>               Bases: <code>Enum</code></p> <p>Adapters are predefined callables that change the format of the data. Adapters are serializable and can thus be saved alongside datasets.</p> <p>Attributes:</p> Name Type Description <code>SMILES_TO_MOL</code> <p>Convert a SMILES string to a RDKit molecule.</p> <code>BYTES_TO_MOL</code> <p>Convert a RDKit binary string to a RDKit molecule.</p>"},{"location":"api/base.html","title":"Base classes","text":""},{"location":"api/base.html#polaris._artifact.BaseArtifactModel","title":"polaris._artifact.BaseArtifactModel","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for all artifacts on the Hub. Specifies meta-data that is used by the Hub.</p> Optional <p>Despite all artifacts basing this class, note that all attributes are optional. This ensures the library can be used without the Polaris Hub. Only when uploading to the Hub, some of the attributes are required.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>Optional[SlugCompatibleStringType]</code> <p>A slug-compatible name for the dataset. Together with the owner, this is used by the Hub to uniquely identify the benchmark.</p> <code>description</code> <code>str</code> <p>A beginner-friendly, short description of the dataset.</p> <code>tags</code> <code>list[str]</code> <p>A list of tags to categorize the benchmark by. This is used by the hub to search over benchmarks.</p> <code>user_attributes</code> <code>Dict[str, str]</code> <p>A dict with additional, textual user attributes.</p> <code>owner</code> <code>Optional[HubOwner]</code> <p>A slug-compatible name for the owner of the dataset. If the dataset comes from the Polaris Hub, this is the associated owner (organization or user). Together with the name, this is used by the Hub to uniquely identify the benchmark.</p> <code>polaris_version</code> <code>str</code> <p>The version of the Polaris library that was used to create the artifact.</p>"},{"location":"api/base.html#polaris._artifact.BaseArtifactModel.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(path: str)\n</code></pre> <p>Loads a benchmark from a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Loads a benchmark specification from a JSON file.</p> required"},{"location":"api/base.html#polaris._artifact.BaseArtifactModel.to_json","title":"to_json","text":"<pre><code>to_json(path: str)\n</code></pre> <p>Saves the benchmark to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Saves the benchmark specification to a JSON file.</p> required"},{"location":"api/benchmark.html","title":"Base class","text":""},{"location":"api/benchmark.html#polaris.benchmark.BenchmarkSpecification","title":"polaris.benchmark.BenchmarkSpecification","text":"<p>               Bases: <code>BaseArtifactModel</code></p> <p>This class wraps a <code>Dataset</code> with additional data  to specify the evaluation logic.</p> <p>Specifically, it specifies:</p> <ol> <li>Which dataset to use (see <code>Dataset</code>);</li> <li>Which columns are used as input and which columns are used as target;</li> <li>Which metrics should be used to evaluate performance on this task;</li> <li>A predefined, static train-test split to use during evaluation.</li> </ol> Subclasses <p>Polaris includes various subclasses of the <code>BenchmarkSpecification</code> that provide a more precise data-model or  additional logic, e.g. <code>SingleTaskBenchmarkSpecification</code>.</p> <p>Examples:</p> <p>Basic API usage: <pre><code>import polaris as po\n\n# Load the benchmark from the Hub\nbenchmark = po.load_benchmark(\"polaris/hello-world-benchmark\")\n\n# Get the train and test data-loaders\ntrain, test = benchmark.get_train_test_split()\n\n# Use the training data to train your model\n# Get the input as an array with 'train.inputs' and 'train.targets'\n# Or simply iterate over the train object.\nfor x, y in train:\n    ...\n\n# Work your magic to accurately predict the test set\npredictions = [0.0 for x in test]\n\n# Evaluate your predictions\nresults = benchmark.evaluate(predictions)\n\n# Submit your results\nresults.upload_to_hub(owner=\"dummy-user\")\n</code></pre></p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>Union[Dataset, str, dict[str, Any]]</code> <p>The dataset the benchmark specification is based on.</p> <code>target_cols</code> <code>ColumnsType</code> <p>The column(s) of the original dataset that should be used as target.</p> <code>input_cols</code> <code>ColumnsType</code> <p>The column(s) of the original dataset that should be used as input.</p> <code>split</code> <code>SplitType</code> <p>The predefined train-test split to use for evaluation.</p> <code>metrics</code> <code>Union[str, Metric, list[Union[str, Metric]]]</code> <p>The metrics to use for evaluating performance</p> <code>main_metric</code> <code>Optional[Union[str, Metric]]</code> <p>The main metric used to rank methods. If <code>None</code>, the first of the <code>metrics</code> field.</p> <code>md5sum</code> <code>Optional[str]</code> <p>The checksum is used to verify the version of the dataset specification. If specified, it will raise an error if the specified checksum doesn't match the computed checksum.</p> <code>readme</code> <code>str</code> <p>Markdown text that can be used to provide a formatted description of the benchmark. If using the Polaris Hub, it is worth noting that this field is more easily edited through the Hub UI as it provides a rich text editor for writing markdown.</p> <code>target_types</code> <code>dict[str, Optional[Union[TargetType, str]]]</code> <p>A dictionary that maps target columns to their type. If not specified, this is automatically inferred.</p> <p>For additional meta-data attributes, see the <code>BaseArtifactModel</code> class.</p>"},{"location":"api/benchmark.html#polaris.benchmark.BenchmarkSpecification.n_train_datapoints","title":"n_train_datapoints  <code>property</code>","text":"<pre><code>n_train_datapoints: int\n</code></pre> <p>The size of the train set.</p>"},{"location":"api/benchmark.html#polaris.benchmark.BenchmarkSpecification.n_test_sets","title":"n_test_sets  <code>property</code>","text":"<pre><code>n_test_sets: int\n</code></pre> <p>The number of test sets</p>"},{"location":"api/benchmark.html#polaris.benchmark.BenchmarkSpecification.n_test_datapoints","title":"n_test_datapoints  <code>property</code>","text":"<pre><code>n_test_datapoints: dict[str, int]\n</code></pre> <p>The size of (each of) the test set(s).</p>"},{"location":"api/benchmark.html#polaris.benchmark.BenchmarkSpecification.n_classes","title":"n_classes  <code>property</code>","text":"<pre><code>n_classes: dict[str, int]\n</code></pre> <p>The number of classes for each of the target columns.</p>"},{"location":"api/benchmark.html#polaris.benchmark.BenchmarkSpecification.task_type","title":"task_type  <code>property</code>","text":"<pre><code>task_type: str\n</code></pre> <p>The high-level task type of the benchmark.</p>"},{"location":"api/benchmark.html#polaris.benchmark.BenchmarkSpecification.get_train_test_split","title":"get_train_test_split","text":"<pre><code>get_train_test_split(featurization_fn: Optional[Callable] = None) -&gt; tuple[Subset, Union[Subset, dict[str, Subset]]]\n</code></pre> <p>Construct the train and test sets, given the split in the benchmark specification.</p> <p>Returns <code>Subset</code> objects, which offer several ways of accessing the data and can thus easily serve as a basis to build framework-specific (e.g. PyTorch, Tensorflow) data-loaders on top of.</p> <p>Parameters:</p> Name Type Description Default <code>featurization_fn</code> <code>Optional[Callable]</code> <p>A function to apply to the input data. If a multi-input benchmark, this function expects an input in the format specified by the <code>input_format</code> parameter.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Subset, Union[Subset, dict[str, Subset]]]</code> <p>A tuple with the train <code>Subset</code> and test <code>Subset</code> objects. If there are multiple test sets, these are returned in a dictionary and each test set has an associated name. The targets of the test set can not be accessed.</p>"},{"location":"api/benchmark.html#polaris.benchmark.BenchmarkSpecification.evaluate","title":"evaluate","text":"<pre><code>evaluate(y_pred: Optional[PredictionsType] = None, y_prob: Optional[PredictionsType] = None) -&gt; BenchmarkResults\n</code></pre> <p>Execute the evaluation protocol for the benchmark, given a set of predictions.</p> What about <code>y_true</code>? <p>Contrary to other frameworks that you might be familiar with, we opted for a signature that includes just the predictions. This reduces the chance of accidentally using the test targets during training.</p> <p>For this method, we make the following assumptions:</p> <ol> <li>There can be one or multiple test set(s);</li> <li>There can be one or multiple target(s);</li> <li>The metrics are constant across test sets;</li> <li>The metrics are constant across targets;</li> <li>There can be metrics which measure across tasks.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>y_pred</code> <code>Optional[PredictionsType]</code> <p>The predictions for the test set, as NumPy arrays. If there are multiple targets, the predictions should be wrapped in a dictionary with the target labels as keys. If there are multiple test sets, the predictions should be further wrapped in a dictionary     with the test subset labels as keys.</p> <code>None</code> <code>y_prob</code> <code>Optional[PredictionsType]</code> <p>The predicted probabilities for the test set, as NumPy arrays.</p> <code>None</code> <p>Returns:</p> Type Description <code>BenchmarkResults</code> <p>A <code>BenchmarkResults</code> object. This object can be directly submitted to the Polaris Hub.</p>"},{"location":"api/benchmark.html#polaris.benchmark.BenchmarkSpecification.upload_to_hub","title":"upload_to_hub","text":"<pre><code>upload_to_hub(settings: Optional[PolarisHubSettings] = None, cache_auth_token: bool = True, access: Optional[AccessType] = 'private', owner: Optional[Union[HubOwner, str]] = None, **kwargs: dict)\n</code></pre> <p>Very light, convenient wrapper around the <code>PolarisHubClient.upload_benchmark</code> method.</p>"},{"location":"api/benchmark.html#polaris.benchmark.BenchmarkSpecification.to_json","title":"to_json","text":"<pre><code>to_json(destination: str) -&gt; str\n</code></pre> <p>Save the benchmark to a destination directory as a JSON file.</p> Multiple files <p>Perhaps unintuitive, this method creates multiple files in the destination directory as it also saves the dataset it is based on to the specified destination. See the docstring of <code>Dataset.to_json</code> for more information.</p> <p>Parameters:</p> Name Type Description Default <code>destination</code> <code>str</code> <p>The directory to save the associated data to.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The path to the JSON file.</p>"},{"location":"api/benchmark.html#subclasses","title":"Subclasses","text":""},{"location":"api/benchmark.html#polaris.benchmark.SingleTaskBenchmarkSpecification","title":"polaris.benchmark.SingleTaskBenchmarkSpecification","text":"<p>               Bases: <code>BenchmarkSpecification</code></p> <p>Subclass for any single-task benchmark specification</p> <p>In addition to the data-model and logic of the base-class, this class verifies that there is just a single target-column.</p>"},{"location":"api/benchmark.html#polaris.benchmark.SingleTaskBenchmarkSpecification.task_type","title":"task_type  <code>property</code>","text":"<pre><code>task_type: str\n</code></pre> <p>The high-level task type of the benchmark.</p>"},{"location":"api/benchmark.html#polaris.benchmark.MultiTaskBenchmarkSpecification","title":"polaris.benchmark.MultiTaskBenchmarkSpecification","text":"<p>               Bases: <code>BenchmarkSpecification</code></p> <p>Subclass for any multi-task benchmark specification</p> <p>In addition to the data-model and logic of the base-class, this class verifies that there are multiple target-columns.</p>"},{"location":"api/benchmark.html#polaris.benchmark.MultiTaskBenchmarkSpecification.task_type","title":"task_type  <code>property</code>","text":"<pre><code>task_type: str\n</code></pre> <p>The high-level task type of the benchmark.</p>"},{"location":"api/converters.html","title":"Data Converters","text":""},{"location":"api/converters.html#polaris.dataset.converters.Converter","title":"polaris.dataset.converters.Converter","text":"<p>               Bases: <code>ABC</code></p>"},{"location":"api/converters.html#polaris.dataset.converters.Converter.convert","title":"convert  <code>abstractmethod</code>","text":"<pre><code>convert(path: str) -&gt; FactoryProduct\n</code></pre> <p>This converts a file into a table and possibly annotations</p>"},{"location":"api/converters.html#polaris.dataset.converters.Converter.get_pointer","title":"get_pointer  <code>staticmethod</code>","text":"<pre><code>get_pointer(column: str, index: Union[int, slice]) -&gt; str\n</code></pre> <p>Creates a pointer.</p> <p>Parameters:</p> Name Type Description Default <code>column</code> <code>str</code> <p>The name of the column. Each column has its own group in the root.</p> required <code>index</code> <code>Union[int, slice]</code> <p>The index or slice of the pointer.</p> required"},{"location":"api/converters.html#polaris.dataset.converters.SDFConverter","title":"polaris.dataset.converters.SDFConverter","text":"<p>               Bases: <code>Converter</code></p> <p>Converts a SDF file into a Polaris dataset.</p> Binary strings for serialization <p>This class converts the molecules to binary strings (for ML purposes, this should be lossless). This might not be the most storage efficient, but is fastest and easiest to maintain. See this Github Discussion for more info.</p> <p>Properties defined on the molecule level in the SDF file can be extracted into separate columns or can be kept in the molecule object.</p> <p>Parameters:</p> Name Type Description Default <code>mol_column</code> <code>str</code> <p>The name of the column that will contain the pointers to the molecules.</p> <code>'molecule'</code> <code>smiles_column</code> <code>Optional[str]</code> <p>The name of the column that will contain the SMILES strings.</p> <code>'smiles'</code> <code>use_isomeric_smiles</code> <code>bool</code> <p>Whether to use isomeric SMILES.</p> <code>True</code> <code>mol_id_column</code> <code>Optional[str]</code> <p>The name of the column that will contain the molecule names.</p> <code>None</code> <code>mol_prop_as_cols</code> <code>bool</code> <p>Whether to extract properties defined on the molecule level in the SDF file into separate columns.</p> <code>True</code> <code>groupby_key</code> <code>Optional[str]</code> <p>The name of the column to group by. If set, the dataset can combine multiple pointers to the molecules into a single datapoint.</p> <code>None</code>"},{"location":"api/converters.html#polaris.dataset.converters.ZarrConverter","title":"polaris.dataset.converters.ZarrConverter","text":"<p>               Bases: <code>Converter</code></p> <p>Parse a .zarr archive into a Polaris <code>Dataset</code>.</p> Tutorial <p>To learn more about the zarr format, see the tutorial.</p> Loading from <code>.zarr</code> <p>Loading and saving datasets from and to <code>.zarr</code> is still experimental and currently not fully supported by the Hub.</p> <p>A <code>.zarr</code> file can contain groups and arrays, where each group can again contain groups and arrays. Within Polaris, the Zarr archive is expected to have a flat hierarchy where each array corresponds to a single column and each array contains the values for all datapoints in that column.</p>"},{"location":"api/dataset.html","title":"Dataset","text":""},{"location":"api/dataset.html#polaris.dataset.Dataset","title":"polaris.dataset.Dataset","text":"<p>               Bases: <code>BaseArtifactModel</code></p> <p>Basic data-model for a Polaris dataset, implemented as a Pydantic model.</p> <p>At its core, a dataset in Polaris is a tabular data structure that stores data-points in a row-wise manner. A Dataset can have multiple modalities or targets, can be sparse and can be part of one or multiple  <code>BenchmarkSpecification</code> objects.</p> Pointer columns <p>Whereas a <code>Dataset</code> contains all information required to construct a dataset, it is not ready yet. For complex data, such as images, we support storing the content in external blobs of data. In that case, the table contains pointers to these blobs that are dynamically loaded when needed.</p> <p>Attributes:</p> Name Type Description <code>table</code> <code>Union[DataFrame, str]</code> <p>The core data-structure, storing data-points in a row-wise manner. Can be specified as either a path to a <code>.parquet</code> file or a <code>pandas.DataFrame</code>.</p> <code>default_adapters</code> <code>Dict[str, Adapter]</code> <p>The adapters that the Dataset recommends to use by default to change the format of the data for specific columns.</p> <code>zarr_root_path</code> <code>Optional[str]</code> <p>The data for any pointer column should be saved in the Zarr archive this path points to.</p> <code>md5sum</code> <code>Optional[str]</code> <p>The checksum is used to verify the version of the dataset specification. If specified, it will raise an error if the specified checksum doesn't match the computed checksum.</p> <code>readme</code> <code>str</code> <p>Markdown text that can be used to provide a formatted description of the dataset. If using the Polaris Hub, it is worth noting that this field is more easily edited through the Hub UI as it provides a rich text editor for writing markdown.</p> <code>annotations</code> <code>Dict[str, ColumnAnnotation]</code> <p>Each column can be annotated with a <code>ColumnAnnotation</code> object. Importantly, this is used to annotate whether a column is a pointer column.</p> <code>source</code> <code>Optional[HttpUrlString]</code> <p>The data source, e.g. a DOI, Github repo or URI.</p> <code>license</code> <code>Optional[SupportedLicenseType]</code> <p>The dataset license. Polaris only supports some Creative Commons licenses. See <code>SupportedLicenseType</code> for accepted ID values.</p> <code>curation_reference</code> <code>Optional[HttpUrlString]</code> <p>A reference to the curation process, e.g. a DOI, Github repo or URI.</p> <p>For additional meta-data attributes, see the <code>BaseArtifactModel</code> class.</p> <p>Raises:</p> Type Description <code>InvalidDatasetError</code> <p>If the dataset does not conform to the Pydantic data-model specification.</p> <code>PolarisChecksumError</code> <p>If the specified checksum does not match the computed checksum.</p>"},{"location":"api/dataset.html#polaris.dataset.Dataset.client","title":"client  <code>property</code>","text":"<pre><code>client\n</code></pre> <p>The Polaris Hub client used to interact with the Polaris Hub.</p>"},{"location":"api/dataset.html#polaris.dataset.Dataset.zarr_data","title":"zarr_data  <code>property</code>","text":"<pre><code>zarr_data\n</code></pre> <p>Get the Zarr data.</p> <p>This is different from the Zarr Root, because to optimize the efficiency of data loading, a user can choose to load the data into memory as a numpy array</p> General purpose dataloader. <p>The goal with Polaris is to provide general purpose datasets that serve as good options for a wide variety of use cases. This also implies you should be able to optimize things further for a specific use case if needed.</p>"},{"location":"api/dataset.html#polaris.dataset.Dataset.zarr_root","title":"zarr_root  <code>property</code>","text":"<pre><code>zarr_root\n</code></pre> <p>Get the zarr Group object corresponding to the root.</p> <p>Opens the zarr archive in read-write mode if it is not already open.</p> Different to <code>zarr_data</code> <p>The <code>zarr_data</code> attribute references either to the Zarr archive or to a in-memory copy of the data. See also <code>Dataset.load_to_memory</code>.</p>"},{"location":"api/dataset.html#polaris.dataset.Dataset.n_rows","title":"n_rows  <code>property</code>","text":"<pre><code>n_rows: int\n</code></pre> <p>The number of rows in the dataset.</p>"},{"location":"api/dataset.html#polaris.dataset.Dataset.n_columns","title":"n_columns  <code>property</code>","text":"<pre><code>n_columns: int\n</code></pre> <p>The number of columns in the dataset.</p>"},{"location":"api/dataset.html#polaris.dataset.Dataset.rows","title":"rows  <code>property</code>","text":"<pre><code>rows: list\n</code></pre> <p>Return all row indices for the dataset</p>"},{"location":"api/dataset.html#polaris.dataset.Dataset.columns","title":"columns  <code>property</code>","text":"<pre><code>columns: list\n</code></pre> <p>Return all columns for the dataset</p>"},{"location":"api/dataset.html#polaris.dataset.Dataset.load_to_memory","title":"load_to_memory","text":"<pre><code>load_to_memory()\n</code></pre> <p>Pre-load the entire dataset into memory.</p> Make sure the uncompressed dataset fits in-memory. <p>This method will load the uncompressed dataset into memory. Make sure you actually have enough memory to store the dataset.</p>"},{"location":"api/dataset.html#polaris.dataset.Dataset.get_data","title":"get_data","text":"<pre><code>get_data(row: str | int, col: str, adapters: Optional[List[Adapter]] = None) -&gt; np.ndarray\n</code></pre> <p>Since the dataset might contain pointers to external files, data retrieval is more complicated than just indexing the <code>table</code> attribute. This method provides an end-point for seamlessly accessing the underlying data.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>str | int</code> <p>The row index in the <code>Dataset.table</code> attribute</p> required <code>col</code> <code>str</code> <p>The column index in the <code>Dataset.table</code> attribute</p> required <code>adapters</code> <code>Optional[List[Adapter]]</code> <p>The adapters to apply to the data before returning it. If None, will use the default adapters specified for the dataset.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A numpy array with the data at the specified indices. If the column is a pointer column, the content of the referenced file is loaded to memory.</p>"},{"location":"api/dataset.html#polaris.dataset.Dataset.upload_to_hub","title":"upload_to_hub","text":"<pre><code>upload_to_hub(access: Optional[AccessType] = 'private', owner: Optional[Union[HubOwner, str]] = None)\n</code></pre> <p>Very light, convenient wrapper around the <code>PolarisHubClient.upload_dataset</code> method.</p>"},{"location":"api/dataset.html#polaris.dataset.Dataset.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(path: str)\n</code></pre> <p>Loads a benchmark from a JSON file. Overrides the method from the base class to remove the caching dir from the file to load from, as that should be user dependent.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Loads a benchmark specification from a JSON file.</p> required"},{"location":"api/dataset.html#polaris.dataset.Dataset.to_json","title":"to_json","text":"<pre><code>to_json(destination: str) -&gt; str\n</code></pre> <p>Save the dataset to a destination directory as a JSON file.</p> Multiple files <p>Perhaps unintuitive, this method creates multiple files.</p> <ol> <li><code>/path/to/destination/dataset.json</code>: This file can be loaded with     <code>Dataset.from_json</code>.</li> <li><code>/path/to/destination/table.parquet</code>: The <code>Dataset.table</code> attribute is saved here.</li> <li>(Optional) <code>/path/to/destination/data/*</code>: Any additional blobs of data referenced by the         pointer columns will be stored here.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>destination</code> <code>str</code> <p>The directory to save the associated data to.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The path to the JSON file.</p>"},{"location":"api/dataset.html#polaris.dataset.Dataset.cache","title":"cache","text":"<pre><code>cache(cache_dir: Optional[str] = None) -&gt; str\n</code></pre> <p>Caches the dataset by downloading all additional data for pointer columns to a local directory.</p> <p>Parameters:</p> Name Type Description Default <code>cache_dir</code> <code>Optional[str]</code> <p>The directory to cache the data to. If not provided, this will fall back to the <code>Dataset.cache_dir</code> attribute</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The path to the cache directory.</p>"},{"location":"api/dataset.html#polaris.dataset.ColumnAnnotation","title":"polaris.dataset.ColumnAnnotation","text":"<p>               Bases: <code>BaseModel</code></p> <p>The <code>ColumnAnnotation</code> class is used to annotate the columns of the <code>Dataset</code> object. This mostly just stores meta-data and does not affect the logic. The exception is the <code>is_pointer</code> attribute.</p> <p>Attributes:</p> Name Type Description <code>is_pointer</code> <code>bool</code> <p>Annotates whether a column is a pointer column. If so, it does not contain data, but rather contains references to blobs of data from which the data is loaded.</p> <code>modality</code> <code>Union[str, Modality]</code> <p>The data modality describes the data type and is used to categorize datasets on the hub and while it does not affect logic in this library, it does affect the logic of the hub.</p> <code>description</code> <code>Optional[str]</code> <p>Describes how the data was generated.</p> <code>user_attributes</code> <code>Dict[str, str]</code> <p>Any additional meta-data can be stored in the user attributes.</p>"},{"location":"api/evaluation.html","title":"Evaluation","text":""},{"location":"api/evaluation.html#polaris.evaluate.BenchmarkResults","title":"polaris.evaluate.BenchmarkResults","text":"<p>               Bases: <code>BaseArtifactModel</code></p> <p>Class for saving benchmarking results</p> <p>This object is returned by <code>BenchmarkSpecification.evaluate</code>. In addition to the metrics on the test set, it contains additional meta-data and logic to integrate the results with the Polaris Hub.</p> <p>The actual results are saved in the <code>results</code> field using the following tabular format:</p> Test set Target label Metric Score test_iid EGFR_WT AUC 0.9 test_ood EGFR_WT AUC 0.75 ... ... ... ... test_ood EGFR_L858R AUC 0.79 Categorizing methods <p>An open question is how to best categorize a methodology (e.g. a model). This is needed since we would like to be able to aggregate results across benchmarks too, to say something about which (type of) methods performs best in general.</p> <p>Attributes:</p> Name Type Description <code>results</code> <code>ResultsType</code> <p>Benchmark results are stored directly in a dataframe or in a serialized, JSON compatible dict that can be decoded into the associated tabular format.</p> <code>benchmark_name</code> <code>SlugCompatibleStringType</code> <p>The name of the benchmark for which these results were generated. Together with the benchmark owner, this uniquely identifies the benchmark on the Hub.</p> <code>benchmark_owner</code> <code>Optional[HubOwner]</code> <p>The owner of the benchmark for which these results were generated. Together with the benchmark name, this uniquely identifies the benchmark on the Hub.</p> <code>github_url</code> <code>Optional[HttpUrlString]</code> <p>The URL to the GitHub repository of the code used to generate these results.</p> <code>paper_url</code> <code>Optional[HttpUrlString]</code> <p>The URL to the paper describing the methodology used to generate these results.</p> <code>contributors</code> <code>Optional[list[HubUser]]</code> <p>The users that are credited for these results.</p> <code>_created_at</code> <code>datetime</code> <p>The time-stamp at which the results were created. Automatically set.</p> <p>For additional meta-data attributes, see the <code>BaseArtifactModel</code> class.</p>"},{"location":"api/evaluation.html#polaris.evaluate.BenchmarkResults.upload_to_hub","title":"upload_to_hub","text":"<pre><code>upload_to_hub(settings: Optional[PolarisHubSettings] = None, cache_auth_token: bool = True, access: Optional[AccessType] = 'private', owner: Optional[Union[HubOwner, str]] = None, **kwargs: dict)\n</code></pre> <p>Very light, convenient wrapper around the <code>PolarisHubClient.upload_results</code> method.</p>"},{"location":"api/evaluation.html#polaris.evaluate.MetricInfo","title":"polaris.evaluate.MetricInfo","text":"<p>               Bases: <code>BaseModel</code></p> <p>Metric metadata</p> <p>Attributes:</p> Name Type Description <code>fn</code> <code>Callable</code> <p>The callable that actually computes the metric.</p> <code>is_multitask</code> <code>bool</code> <p>Whether the metric expects a single set of predictions or a dict of predictions.</p> <code>kwargs</code> <code>dict</code> <p>Additional parameters required for the metric.</p> <code>direction</code> <code>DirectionType</code> <p>The direction for ranking of the metric,  \"max\" for maximization and \"min\" for minimization.</p>"},{"location":"api/evaluation.html#polaris.evaluate._metric.absolute_average_fold_error","title":"polaris.evaluate._metric.absolute_average_fold_error","text":"<pre><code>absolute_average_fold_error(y_true: np.ndarray, y_pred: np.ndarray) -&gt; float\n</code></pre> <p>Calculate the Absolute Average Fold Error (AAFE) metric. It measures the fold change between predicted values and observed values. The implementation is based on this paper.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The true target values of shape (n_samples,)</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted target values of shape (n_samples,).</p> required <p>Returns:</p> Name Type Description <code>aafe</code> <code>float</code> <p>The Absolute Average Fold Error.</p>"},{"location":"api/evaluation.html#polaris.evaluate.Metric","title":"polaris.evaluate.Metric","text":"<p>               Bases: <code>Enum</code></p> <p>A metric within the Polaris ecosystem is uniquely identified by its name and is associated with additional metadata in a <code>MetricInfo</code> instance.</p> <p>Implemented as an enum.</p>"},{"location":"api/evaluation.html#polaris.evaluate.Metric.score","title":"score","text":"<pre><code>score(y_true: np.ndarray, y_pred: Optional[np.ndarray] = None, y_prob: Optional[np.ndarray] = None) -&gt; float\n</code></pre> <p>Endpoint for computing the metric.</p> <p>For convenience, calling a <code>Metric</code> will result in this method being called.</p> <pre><code>metric = Metric.mean_absolute_error\nassert metric.score(y_true=first, y_pred=second) == metric(y_true=first, y_pred=second)\n</code></pre>"},{"location":"api/factory.html","title":"Dataset Factory","text":""},{"location":"api/factory.html#polaris.dataset.DatasetFactory","title":"polaris.dataset.DatasetFactory","text":"<p>The <code>DatasetFactory</code> makes it easier to create complex datasets.</p> <p>It is based on the the factory design pattern and allows a user to specify specific handlers (i.e. <code>Converter</code> objects) for different file types. These converters are used to convert commonly used file types in drug discovery to something that can be used within Polaris while losing as little information as possible.</p> <p>In addition, it contains utility method to incrementally build out a dataset from different sources.</p> Try quickly converting one of your datasets <p>The <code>DatasetFactory</code> is designed to give you full control. If your dataset is saved in a single file and you don't need anything fancy, you can try use <code>create_dataset_from_file</code> instead.</p> <pre><code>from polaris.dataset import create_dataset_from_file\ndataset = create_dataset_from_file(\"path/to/my_dataset.sdf\")\n</code></pre> How to make adding meta-data easier? <p>The <code>DatasetFactory</code> is designed to more easily pull together data from different sources. However, adding meta-data remains a laborous process. How could we make this simpler through the Python API?</p>"},{"location":"api/factory.html#polaris.dataset.DatasetFactory.zarr_root_path","title":"zarr_root_path  <code>property</code>","text":"<pre><code>zarr_root_path: Group\n</code></pre> <p>The root of the zarr archive for the Dataset that is being built. All data for a single dataset is expected to be stored in the same Zarr archive.</p>"},{"location":"api/factory.html#polaris.dataset.DatasetFactory.zarr_root","title":"zarr_root  <code>property</code>","text":"<pre><code>zarr_root: Group\n</code></pre> <p>The root of the zarr archive for the Dataset that is being built. All data for a single dataset is expected to be stored in the same Zarr archive.</p>"},{"location":"api/factory.html#polaris.dataset.DatasetFactory.register_converter","title":"register_converter","text":"<pre><code>register_converter(ext: str, converter: Converter)\n</code></pre> <p>Registers a new converter for a specific file type.</p> <p>Parameters:</p> Name Type Description Default <code>ext</code> <code>str</code> <p>The file extension for which the converter should be used. There can only be a single converter per file extension.</p> required <code>converter</code> <code>Converter</code> <p>The handler for the file type. This should convert the file to a Polaris-compatible format.</p> required"},{"location":"api/factory.html#polaris.dataset.DatasetFactory.add_column","title":"add_column","text":"<pre><code>add_column(column: pd.Series, annotation: Optional[ColumnAnnotation] = None, adapters: Optional[Adapter] = None)\n</code></pre> <p>Add a single column to the DataFrame</p> <p>We require:</p> <ol> <li>The name attribute of the column to be set.</li> <li>The name attribute of the column to be unique.</li> <li>If the column is a pointer column, the <code>zarr_root_path</code> needs to be set.</li> <li>The length of the column to match the length of the alredy constructed table.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>column</code> <code>Series</code> <p>The column to add to the dataset.</p> required <code>annotation</code> <code>Optional[ColumnAnnotation]</code> <p>The annotation for the column. If None, a default annotation will be used.</p> <code>None</code>"},{"location":"api/factory.html#polaris.dataset.DatasetFactory.add_columns","title":"add_columns","text":"<pre><code>add_columns(df: pd.DataFrame, annotations: Optional[Dict[str, ColumnAnnotation]] = None, adapters: Optional[Dict[str, Adapter]] = None, merge_on: Optional[str] = None)\n</code></pre> <p>Add multiple columns to the dataset based on another dataframe.</p> <p>To have more control over how the two dataframes are combined, you can specify a column to merge on. This will always do an outer join.</p> <p>If not specifying a key to merge on, the columns will simply be added to the dataset that has been built so far without any reordering. They are therefore expected to meet all the same expectations as for <code>add_column</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Pandas DataFrame with the columns that we want to add to the dataset.</p> required <code>annotations</code> <code>Optional[Dict[str, ColumnAnnotation]]</code> <p>The annotations for the columns. If None, default annotations will be used.</p> <code>None</code> <code>merge_on</code> <code>Optional[str]</code> <p>The column to merge on, if any.</p> <code>None</code>"},{"location":"api/factory.html#polaris.dataset.DatasetFactory.add_from_file","title":"add_from_file","text":"<pre><code>add_from_file(path: str)\n</code></pre> <p>Uses the registered converters to parse the data from a specific file and add it to the dataset. If no converter is found for the file extension, it raises an error.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the file that should be parsed.</p> required"},{"location":"api/factory.html#polaris.dataset.DatasetFactory.build","title":"build","text":"<pre><code>build() -&gt; Dataset\n</code></pre> <p>Returns a Dataset based on the current state of the factory.</p>"},{"location":"api/factory.html#polaris.dataset.DatasetFactory.reset","title":"reset","text":"<pre><code>reset(zarr_root_path: Optional[str] = None)\n</code></pre> <p>Resets the factory to its initial state to start building the next dataset from scratch. Note that this will not reset the registered converters.</p> <p>Parameters:</p> Name Type Description Default <code>zarr_root_path</code> <code>Optional[str]</code> <p>The root path of the zarr hierarchy. If you want to use pointer columns for your next dataset, this arguments needs to be passed.</p> <code>None</code>"},{"location":"api/factory.html#polaris.dataset.create_dataset_from_file","title":"polaris.dataset.create_dataset_from_file","text":"<pre><code>create_dataset_from_file(path: str, zarr_root_path: Optional[str] = None) -&gt; Dataset\n</code></pre> <p>This function is a convenience function to create a dataset from a file.</p> <p>It sets up the dataset factory with sensible defaults for the converters. For creating more complicated datasets, please use the <code>DatasetFactory</code> directly.</p>"},{"location":"api/hub.client.html","title":"Client","text":""},{"location":"api/hub.client.html#polaris.hub.settings.PolarisHubSettings","title":"polaris.hub.settings.PolarisHubSettings","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Settings for the OAuth2 Polaris Hub API Client.</p> Secrecy of these settings <p>Since the Polaris Hub uses PCKE (Proof Key for Code Exchange) for OAuth2, these values thus do not have to be kept secret. See RFC 7636 for more info.</p> <p>Attributes:</p> Name Type Description <code>hub_url</code> <code>HttpUrlString</code> <p>The URL to the main page of the Polaris Hub.</p> <code>api_url</code> <code>HttpUrlString | None</code> <p>The URL to the main entrypoint of the Polaris API.</p> <code>authorize_url</code> <code>HttpUrlString</code> <p>The URL of the OAuth2 authorization endpoint.</p> <code>callback_url</code> <code>HttpUrlString</code> <p>The URL to which the user is redirected after authorization.</p> <code>token_fetch_url</code> <code>HttpUrlString</code> <p>The URL of the OAuth2 token endpoint.</p> <code>user_info_url</code> <code>HttpUrlString</code> <p>The URL of the OAuth2 user info endpoint.</p> <code>scopes</code> <code>str</code> <p>The OAuth2 scopes that are requested.</p> <code>client_id</code> <code>str</code> <p>The OAuth2 client ID.</p> <code>ca_bundle</code> <code>Union[str, bool, None]</code> <p>The path to a CA bundle file for requests. Allows for custom SSL certificates to be used.</p>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient","title":"polaris.hub.client.PolarisHubClient","text":"<pre><code>PolarisHubClient(settings: PolarisHubSettings | None = None, cache_auth_token: bool = True, **kwargs: dict)\n</code></pre> <p>               Bases: <code>OAuth2Client</code></p> <p>A client for the Polaris Hub API. The Polaris Hub is a central repository of datasets, benchmarks and results. Visit it here: https://polarishub.io/.</p> <p>Bases the <code>authlib</code> client, which in turns bases the <code>httpx</code> client. See the relevant docs to learn more about how to use these clients outside of the integration with the Polaris Hub.</p> Closing the client <p>The client should be closed after all requests have been made. For convenience, you can also use the client as a context manager to automatically close the client when the context is exited. Note that once the client has been closed, it cannot be used anymore.</p> <pre><code># Make sure to close the client once finished\nclient = PolarisHubClient()\nclient.get(...)\nclient.close()\n\n# Or use the client as a context manager\nwith PolarisHubClient() as client:\n    client.get(...)\n</code></pre> Async Client <p><code>authlib</code> also supports an async client. Since we don't expect to make multiple requests to the Hub in parallel and due to the added complexity stemming from using the Python asyncio API, we are sticking to the sync client - at least for now.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>PolarisHubSettings | None</code> <p>A <code>PolarisHubSettings</code> instance.</p> <code>None</code> <code>cache_auth_token</code> <code>bool</code> <p>Whether to cache the auth token to a file.</p> <code>True</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to the authlib <code>OAuth2Client</code> constructor.</p> <code>{}</code>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.login","title":"login","text":"<pre><code>login(overwrite: bool = False, auto_open_browser: bool = True)\n</code></pre> <p>Login to the Polaris Hub using the OAuth2 protocol.</p> Headless authentication <p>It is currently not possible to login to the Polaris Hub without a browser. See this Github issue for more info.</p> <p>Parameters:</p> Name Type Description Default <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the current token if the user is already logged in.</p> <code>False</code> <code>auto_open_browser</code> <code>bool</code> <p>Whether to automatically open the browser to visit the authorization URL.</p> <code>True</code>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.list_datasets","title":"list_datasets","text":"<pre><code>list_datasets(limit: int = 100, offset: int = 0) -&gt; list[str]\n</code></pre> <p>List all available datasets on the Polaris Hub.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>The maximum number of datasets to return.</p> <code>100</code> <code>offset</code> <code>int</code> <p>The offset from which to start returning datasets.</p> <code>0</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of dataset names in the format <code>owner/dataset_name</code>.</p>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(owner: str | HubOwner, name: str, verify_checksum: bool = True) -&gt; Dataset\n</code></pre> <p>Load a dataset from the Polaris Hub.</p> <p>Parameters:</p> Name Type Description Default <code>owner</code> <code>str | HubOwner</code> <p>The owner of the dataset. Can be either a user or organization from the Polaris Hub.</p> required <code>name</code> <code>str</code> <p>The name of the dataset.</p> required <code>verify_checksum</code> <code>bool</code> <p>Whether to use the checksum to verify the integrity of the dataset.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>A <code>Dataset</code> instance, if it exists.</p>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.open_zarr_file","title":"open_zarr_file","text":"<pre><code>open_zarr_file(owner: str | HubOwner, name: str, path: str, mode: IOMode, as_consolidated: bool = True) -&gt; zarr.hierarchy.Group\n</code></pre> <p>Open a Zarr file from a Polaris dataset</p> <p>Parameters:</p> Name Type Description Default <code>owner</code> <code>str | HubOwner</code> <p>Which Hub user or organization owns the artifact.</p> required <code>name</code> <code>str</code> <p>Name of the dataset.</p> required <code>path</code> <code>str</code> <p>Path to the Zarr file within the dataset.</p> required <code>mode</code> <code>IOMode</code> <p>The mode in which the file is opened.</p> required <code>as_consolidated</code> <code>bool</code> <p>Whether to open the store with consolidated metadata for optimized reading. This is only applicable in 'r' and 'r+' modes.</p> <code>True</code> <p>Returns:</p> Type Description <code>Group</code> <p>The Zarr object representing the dataset.</p>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.list_benchmarks","title":"list_benchmarks","text":"<pre><code>list_benchmarks(limit: int = 100, offset: int = 0) -&gt; list[str]\n</code></pre> <p>List all available benchmarks on the Polaris Hub.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>The maximum number of benchmarks to return.</p> <code>100</code> <code>offset</code> <code>int</code> <p>The offset from which to start returning benchmarks.</p> <code>0</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of benchmark names in the format <code>owner/benchmark_name</code>.</p>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.get_benchmark","title":"get_benchmark","text":"<pre><code>get_benchmark(owner: str | HubOwner, name: str, verify_checksum: bool = True) -&gt; BenchmarkSpecification\n</code></pre> <p>Load a benchmark from the Polaris Hub.</p> <p>Parameters:</p> Name Type Description Default <code>owner</code> <code>str | HubOwner</code> <p>The owner of the benchmark. Can be either a user or organization from the Polaris Hub.</p> required <code>name</code> <code>str</code> <p>The name of the benchmark.</p> required <code>verify_checksum</code> <code>bool</code> <p>Whether to use the checksum to verify the integrity of the dataset.</p> <code>True</code> <p>Returns:</p> Type Description <code>BenchmarkSpecification</code> <p>A <code>BenchmarkSpecification</code> instance, if it exists.</p>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.upload_results","title":"upload_results","text":"<pre><code>upload_results(results: BenchmarkResults, access: AccessType = 'private', owner: HubOwner | str | None = None)\n</code></pre> <p>Upload the results to the Polaris Hub.</p> Owner <p>The owner of the results will automatically be inferred by the hub from the user making the request. Contrary to other artifact types, an organization cannot own a set of results. However, you can specify the <code>BenchmarkResults.contributors</code> field to share credit with other hub users.</p> Required meta-data <p>The Polaris client and hub maintain different requirements as to which meta-data is required. The requirements by the hub are stricter, so when uploading to the hub you might get some errors on missing meta-data. Make sure to fill-in as much of the meta-data as possible before uploading.</p> Benchmark name and owner <p>Importantly, <code>results.benchmark_name</code> and <code>results.benchmark_owner</code> must be specified and match an existing benchmark on the Polaris Hub. If these results were generated by <code>benchmark.evaluate(...)</code>, this is done automatically.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>BenchmarkResults</code> <p>The results to upload.</p> required <code>access</code> <code>AccessType</code> <p>Grant public or private access to result</p> <code>'private'</code> <code>owner</code> <code>HubOwner | str | None</code> <p>Which Hub user or organization owns the artifact. Takes precedence over <code>results.owner</code>.</p> <code>None</code>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.upload_dataset","title":"upload_dataset","text":"<pre><code>upload_dataset(dataset: Dataset, access: AccessType = 'private', timeout: TimeoutTypes = (10, 200), owner: HubOwner | str | None = None, if_exists: ZarrConflictResolution = 'replace')\n</code></pre> <p>Upload the dataset to the Polaris Hub.</p> Owner <p>You have to manually specify the owner in the dataset data model. Because the owner could be a user or an organization, we cannot automatically infer this from just the logged-in user.</p> Required meta-data <p>The Polaris client and hub maintain different requirements as to which meta-data is required. The requirements by the hub are stricter, so when uploading to the hub you might get some errors on missing meta-data. Make sure to fill-in as much of the meta-data as possible before uploading.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset to upload.</p> required <code>access</code> <code>AccessType</code> <p>Grant public or private access to result</p> <code>'private'</code> <code>timeout</code> <code>TimeoutTypes</code> <p>Request timeout values. User can modify the value when uploading large dataset as needed. This can be a single value with the timeout in seconds for all IO operations, or a more granular tuple with (connect_timeout, write_timeout). The type of the the timout parameter comes from <code>httpx</code>. Since datasets can get large, it might be needed to increase the write timeout for larger datasets. See also: https://www.python-httpx.org/advanced/#timeout-configuration</p> <code>(10, 200)</code> <code>owner</code> <code>HubOwner | str | None</code> <p>Which Hub user or organization owns the artifact. Takes precedence over <code>dataset.owner</code>.</p> <code>None</code> <code>if_exists</code> <code>ZarrConflictResolution</code> <p>Action for handling existing files in the Zarr archive. Options are 'raise' to throw an error, 'replace' to overwrite, or 'skip' to proceed without altering the existing files.</p> <code>'replace'</code>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.upload_benchmark","title":"upload_benchmark","text":"<pre><code>upload_benchmark(benchmark: BenchmarkSpecification, access: AccessType = 'private', owner: HubOwner | str | None = None)\n</code></pre> <p>Upload the benchmark to the Polaris Hub.</p> Owner <p>You have to manually specify the owner in the benchmark data model. Because the owner could be a user or an organization, we cannot automatically infer this from the logged-in user.</p> Required meta-data <p>The Polaris client and hub maintain different requirements as to which meta-data is required. The requirements by the hub are stricter, so when uploading to the hub you might get some errors on missing meta-data. Make sure to fill-in as much of the meta-data as possible before uploading.</p> Non-existent datasets <p>The client will not upload the associated dataset to the hub if it does not yet exist. Make sure to specify an existing dataset or upload the dataset first.</p> <p>Parameters:</p> Name Type Description Default <code>benchmark</code> <code>BenchmarkSpecification</code> <p>The benchmark to upload.</p> required <code>access</code> <code>AccessType</code> <p>Grant public or private access to result</p> <code>'private'</code> <code>owner</code> <code>HubOwner | str | None</code> <p>Which Hub user or organization owns the artifact. Takes precedence over <code>benchmark.owner</code>.</p> <code>None</code>"},{"location":"api/hub.external_auth_client.html","title":"Hub.external auth client","text":""},{"location":"api/hub.external_auth_client.html#polaris.hub.external_auth_client.ExternalAuthClient","title":"polaris.hub.external_auth_client.ExternalAuthClient","text":"<pre><code>ExternalAuthClient(settings: PolarisHubSettings, cache_auth_token: bool = True, **kwargs: dict)\n</code></pre> <p>               Bases: <code>OAuth2Client</code></p> <p>This authentication client is used to obtain OAuth 2 tokens from Polaris's external OAuth2 server. These can in turn be used to obtain Polaris Hub tokens.</p> Internal use <p>This class is intended for internal use by the <code>PolarisHubClient</code> class, and you should not have to interact with it directly.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>PolarisHubSettings</code> <p>A <code>PolarisHubSettings</code> instance.</p> required <code>cache_auth_token</code> <code>bool</code> <p>Whether to cache the auth token to a file.</p> <code>True</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to the authlib <code>OAuth2Client</code> constructor.</p> <code>{}</code>"},{"location":"api/hub.external_auth_client.html#polaris.hub.external_auth_client.ExternalAuthClient.user_info","title":"user_info  <code>property</code>","text":"<pre><code>user_info: dict\n</code></pre> <p>Get information about the currently logged-in user through the OAuth2 User Info flow.</p>"},{"location":"api/hub.external_auth_client.html#polaris.hub.external_auth_client.ExternalAuthClient.interactive_login","title":"interactive_login","text":"<pre><code>interactive_login(overwrite: bool = False, auto_open_browser: bool = True)\n</code></pre> <p>Login to the Polaris Hub using an interactive flow, through a Web browser.</p> Headless authentication <p>It is currently not possible to log in to the Polaris Hub without a browser. See this GitHub issue for more info.</p> <p>Parameters:</p> Name Type Description Default <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the current token if the user is already logged in.</p> <code>False</code> <code>auto_open_browser</code> <code>bool</code> <p>Whether to automatically open the browser to visit the authorization URL.</p> <code>True</code>"},{"location":"api/hub.polarisfs.html","title":"PolarisFileSystem","text":""},{"location":"api/hub.polarisfs.html#polaris.hub.polarisfs.PolarisFileSystem","title":"polaris.hub.polarisfs.PolarisFileSystem","text":"<pre><code>PolarisFileSystem(polaris_client: PolarisHubClient, dataset_owner: str, dataset_name: str, **kwargs: dict)\n</code></pre> <p>               Bases: <code>AbstractFileSystem</code></p> <p>A file system interface for accessing datasets on the Polaris platform.</p> <p>This class extends <code>fsspec.AbstractFileSystem</code> and provides methods to list objects within a Polaris dataset and fetch the content of a file from the dataset.</p> Zarr Integration <p>This file system can be used with Zarr to load multidimensional array data stored in a Dataset from the Polaris infrastructure. This class is needed because we otherwise cannot generate signed URLs for folders and Zarr is a folder based data-format.</p> <pre><code>fs = PolarisFileSystem(...)\nstore = zarr.storage.FSStore(..., fs=polaris_fs)\nroot = zarr.open(store, mode=\"r\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>polaris_client</code> <code>PolarisHubClient</code> <p>The Polaris Hub client used to make API requests.</p> required <code>dataset_owner</code> <code>str</code> <p>The owner of the dataset.</p> required <code>dataset_name</code> <code>str</code> <p>The name of the dataset.</p> required"},{"location":"api/hub.polarisfs.html#polaris.hub.polarisfs.PolarisFileSystem.is_polarisfs_path","title":"is_polarisfs_path  <code>staticmethod</code>","text":"<pre><code>is_polarisfs_path(path: str) -&gt; bool\n</code></pre> <p>Check if the given path is a PolarisFS path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the path is a PolarisFS path; otherwise, False.</p>"},{"location":"api/hub.polarisfs.html#polaris.hub.polarisfs.PolarisFileSystem.ls","title":"ls","text":"<pre><code>ls(path: str, detail: bool = False, timeout: Optional[TimeoutTypes] = None, **kwargs: dict) -&gt; Union[List[str], List[Dict[str, Any]]]\n</code></pre> <p>List objects in the specified path within the Polaris dataset.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path within the dataset to list objects.</p> required <code>detail</code> <code>bool</code> <p>If True, returns detailed information about each object.</p> <code>False</code> <code>timeout</code> <code>Optional[TimeoutTypes]</code> <p>Maximum time (in seconds) to wait for the request to complete.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[List[str], List[Dict[str, Any]]]</code> <p>A list of dictionaries if detail is True; otherwise, a list of object names.</p>"},{"location":"api/hub.polarisfs.html#polaris.hub.polarisfs.PolarisFileSystem.cat_file","title":"cat_file","text":"<pre><code>cat_file(path: str, start: Union[int, None] = None, end: Union[int, None] = None, timeout: Optional[TimeoutTypes] = None, **kwargs: dict) -&gt; bytes\n</code></pre> <p>Fetches and returns the content of a file from the Polaris dataset.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the file within the dataset.</p> required <code>start</code> <code>Union[int, None]</code> <p>The starting index of the content to retrieve.</p> <code>None</code> <code>end</code> <code>Union[int, None]</code> <p>The ending index of the content to retrieve.</p> <code>None</code> <code>timeout</code> <code>Optional[TimeoutTypes]</code> <p>Maximum time (in seconds) to wait for the request to complete.</p> <code>None</code> <code>kwargs</code> <code>dict</code> <p>Extra arguments passed to <code>fsspec.open()</code></p> <code>{}</code> <p>Returns:</p> Type Description <code>bytes</code> <p>The content of the requested file.</p>"},{"location":"api/hub.polarisfs.html#polaris.hub.polarisfs.PolarisFileSystem.rm","title":"rm","text":"<pre><code>rm(path: str, recursive: bool = False, maxdepth: Optional[int] = None) -&gt; None\n</code></pre> <p>Remove a file or directory from the Polaris dataset.</p> <p>This method is provided for compatibility with the Zarr storage interface. It may be called by the Zarr store when removing a file or directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the file or directory to be removed.</p> required <code>recursive</code> <code>bool</code> <p>If True, remove directories and their contents recursively.</p> <code>False</code> <code>maxdepth</code> <code>Optional[int]</code> <p>The maximum depth to recurse when removing directories.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Note <p>This method currently it does not perform any removal operations and is included as a placeholder that aligns with the Zarr interface's expectations.</p>"},{"location":"api/hub.polarisfs.html#polaris.hub.polarisfs.PolarisFileSystem.pipe_file","title":"pipe_file","text":"<pre><code>pipe_file(path: str, content: Union[bytes, str], timeout: Optional[TimeoutTypes] = None, **kwargs: dict) -&gt; None\n</code></pre> <p>Pipes the content of a file to the Polaris dataset.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the file within the dataset.</p> required <code>content</code> <code>Union[bytes, str]</code> <p>The content to be piped into the file.</p> required <code>timeout</code> <code>Optional[TimeoutTypes]</code> <p>Maximum time (in seconds) to wait for the request to complete.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api/load.html","title":"Load","text":""},{"location":"api/load.html#polaris.load_dataset","title":"polaris.load_dataset","text":"<pre><code>load_dataset(path: str, verify_checksum: bool = True) -&gt; Dataset\n</code></pre> <p>Loads a Polaris dataset.</p> <p>In Polaris, a dataset is a tabular data structure that stores data-points in a row-wise manner. A dataset can have multiple modalities or targets, can be sparse and can be part of one or multiple benchmarks.</p> <p>The Polaris dataset can be loaded from the Hub or from a local or remote directory.</p> <ul> <li>Hub (recommended): When loading the dataset from the Hub, you can simply     provide the <code>owner/name</code> slug. This can be easily copied from the relevant dataset     page on the Hub.</li> <li>Directory: When loading the dataset from a directory, you should provide the path     as returned by <code>Dataset.to_json</code>.     The path can be local or remote.</li> </ul>"},{"location":"api/load.html#polaris.load_benchmark","title":"polaris.load_benchmark","text":"<pre><code>load_benchmark(path: str, verify_checksum: bool = True)\n</code></pre> <p>Loads a Polaris benchmark.</p> <p>In Polaris, a benchmark wraps a dataset with additional meta-data to specify the evaluation logic.</p> <p>The Polaris benchmark can be loaded from the Hub or from a local or remote directory.</p> Dataset is automatically loaded <p>The dataset underlying the benchmark is automatically loaded when loading the benchmark.</p> <ul> <li>Hub (recommended): When loading the benchmark from the Hub, you can simply     provide the <code>owner/name</code> slug. This can be easily copied from the relevant benchmark     page on the Hub.</li> <li>Directory: When loading the benchmark from a directory, you should provide the path     as returned by <code>BenchmarkSpecification.to_json</code>.     The path can be local or remote.</li> </ul>"},{"location":"api/subset.html","title":"Subset","text":""},{"location":"api/subset.html#polaris.dataset.Subset","title":"polaris.dataset.Subset","text":"<p>The <code>Subset</code> class provides easy access to a single partition of a split dataset.</p> No need to create this class manually <p>You should not have to create this class manually. In most use-cases, you can create a <code>Subset</code> through the <code>get_train_test_split</code> method of a <code>BenchmarkSpecification</code> object.</p> Featurize your inputs <p>Not all datasets are already featurized. For example, a small-molecule task might simply provide the SMILES string. To easily featurize the inputs, you can pass or set a transformation function. For example:</p> <pre><code>import datamol as dm\n\nbenchmark.get_train_test_split(..., featurization_fn=dm.to_fp)\n</code></pre> <p>This should be the starting point for any framework-specific (e.g. PyTorch, Tensorflow) data-loader implementation. How the data is loaded in Polaris can be non-trivial, so this class is provided to abstract away the details. To easily build framework-specific data-loaders, a <code>Subset</code> supports various styles of accessing the data:</p> <ol> <li>In memory: Loads the entire dataset in memory and returns a single array with all datapoints,     this style is accessible through the <code>subset.targets</code> and <code>subset.inputs</code> properties.</li> <li>List: Index the subset like a list, this style is accessible through the <code>subset[idx]</code> syntax.</li> <li>Iterator: Iterate over the subset, this style is accessible through the <code>iter(subset)</code> syntax.</li> </ol> <p>Examples:</p> <p>The different styles of accessing the data:</p> <pre><code>import polaris as po\n\nbenchmark = po.load_benchmark(...)\ntrain, test = benchmark.get_train_test_split()\n\n# Load the entire dataset in memory, useful for e.g. scikit-learn.\nX = train.inputs\ny = train.targets\n\n# Access a single datapoint as with a list, useful for e.g. PyTorch.\nx, y = train[0]\n\n# Iterate over the dataset, useful for very large datasets.\nfor x, y in train:\n    ...\n</code></pre> <p>Raises:</p> Type Description <code>TestAccessError</code> <p>When trying to access the targets of the test set (specified by the <code>hide_targets</code> attribute).</p>"},{"location":"api/utils.types.html","title":"Types","text":""},{"location":"api/utils.types.html#polaris.utils.types.SplitIndicesType","title":"SplitIndicesType  <code>module-attribute</code>","text":"<pre><code>SplitIndicesType: TypeAlias = list[int]\n</code></pre> <p>A split is defined by a sequence of integers.</p>"},{"location":"api/utils.types.html#polaris.utils.types.SplitType","title":"SplitType  <code>module-attribute</code>","text":"<pre><code>SplitType: TypeAlias = tuple[SplitIndicesType, Union[SplitIndicesType, dict[str, SplitIndicesType]]]\n</code></pre> <p>A split is a pair of which the first item is always assumed to be the train set. The second item can either be a single test set or a dictionary with multiple, named test sets.</p>"},{"location":"api/utils.types.html#polaris.utils.types.PredictionsType","title":"PredictionsType  <code>module-attribute</code>","text":"<pre><code>PredictionsType: TypeAlias = Union[ndarray, dict[str, Union[ndarray, dict[str, ndarray]]]]\n</code></pre> <p>A prediction is one of three things:</p> <ul> <li>A single array (single-task, single test set)</li> <li>A dictionary of arrays (single-task, multiple test sets) </li> <li>A dictionary of dictionaries of arrays (multi-task, multiple test sets)</li> </ul>"},{"location":"api/utils.types.html#polaris.utils.types.DatapointType","title":"DatapointType  <code>module-attribute</code>","text":"<pre><code>DatapointType: TypeAlias = tuple[DatapointPartType, DatapointPartType]\n</code></pre> <p>A datapoint has:</p> <ul> <li>A single input or multiple inputs (either as dict or tuple)</li> <li>No target, a single target or a multiple targets (either as dict or tuple)</li> </ul>"},{"location":"api/utils.types.html#polaris.utils.types.SlugStringType","title":"SlugStringType  <code>module-attribute</code>","text":"<pre><code>SlugStringType: TypeAlias = Annotated[str, StringConstraints(pattern='^[a-z0-9-]+$', min_length=4, max_length=64)]\n</code></pre> <p>A URL-compatible string that can serve as slug on the hub.</p>"},{"location":"api/utils.types.html#polaris.utils.types.SlugCompatibleStringType","title":"SlugCompatibleStringType  <code>module-attribute</code>","text":"<pre><code>SlugCompatibleStringType: TypeAlias = Annotated[str, StringConstraints(pattern='^[A-Za-z0-9_-]+$', min_length=4, max_length=64)]\n</code></pre> <p>A URL-compatible string that can be turned into a slug by the hub.</p> <p>Can only use alpha-numeric characters, underscores and dashes.  The string must be at least 4 and at most 64 characters long.</p>"},{"location":"api/utils.types.html#polaris.utils.types.HubUser","title":"HubUser  <code>module-attribute</code>","text":"<pre><code>HubUser: TypeAlias = SlugCompatibleStringType\n</code></pre> <p>A user on the Polaris Hub is identified by a username,  which is a <code>SlugCompatibleStringType</code>.</p>"},{"location":"api/utils.types.html#polaris.utils.types.HttpUrlString","title":"HttpUrlString  <code>module-attribute</code>","text":"<pre><code>HttpUrlString: TypeAlias = Annotated[str, BeforeValidator(lambda v: validate_python(v) and v)]\n</code></pre> <p>A validated URL that will be turned into a string. This is useful for interactions with httpx and authlib, who have their own URL types.</p>"},{"location":"api/utils.types.html#polaris.utils.types.DirectionType","title":"DirectionType  <code>module-attribute</code>","text":"<pre><code>DirectionType: TypeAlias = float | Literal['min', 'max']\n</code></pre> <p>The direction of any variable to be sorted. This can be used to sort the metric score, indicate the optmization direction of endpoint.</p>"},{"location":"api/utils.types.html#polaris.utils.types.AccessType","title":"AccessType  <code>module-attribute</code>","text":"<pre><code>AccessType: TypeAlias = Literal['public', 'private']\n</code></pre> <p>Type to specify access to a dataset, benchmark or result in the Hub.</p>"},{"location":"api/utils.types.html#polaris.utils.types.TimeoutTypes","title":"TimeoutTypes  <code>module-attribute</code>","text":"<pre><code>TimeoutTypes = Union[Tuple[int, int], Literal['timeout', 'never']]\n</code></pre> <p>Timeout types for specifying maximum wait times.</p>"},{"location":"api/utils.types.html#polaris.utils.types.IOMode","title":"IOMode  <code>module-attribute</code>","text":"<pre><code>IOMode: TypeAlias = Literal['r', 'r+', 'a', 'w', 'w-']\n</code></pre> <p>Type to specify the mode for input/output operations (I/O) when interacting with a file or resource.</p>"},{"location":"api/utils.types.html#polaris.utils.types.SupportedLicenseType","title":"SupportedLicenseType  <code>module-attribute</code>","text":"<pre><code>SupportedLicenseType: TypeAlias = Literal['CC-BY-4.0', 'CC-BY-SA-4.0', 'CC-BY-NC-4.0', 'CC-BY-NC-SA-4.0', 'CC0-1.0', 'MIT']\n</code></pre> <p>Supported license types for dataset uploads to Polaris Hub</p>"},{"location":"api/utils.types.html#polaris.utils.types.ZarrConflictResolution","title":"ZarrConflictResolution  <code>module-attribute</code>","text":"<pre><code>ZarrConflictResolution: TypeAlias = Literal['raise', 'replace', 'skip']\n</code></pre> <p>Type to specify which action to take when encountering existing files within a Zarr archive.</p>"},{"location":"api/utils.types.html#polaris.utils.types.HubOwner","title":"HubOwner","text":"<p>               Bases: <code>BaseModel</code></p> <p>An owner of an artifact on the Polaris Hub</p> <p>The slug is most important as it is the user-facing part of this data model. The externalId and type are added to be consistent with the model returned by the Polaris Hub .</p>"},{"location":"api/utils.types.html#polaris.utils.types.HubOwner.normalize","title":"normalize  <code>staticmethod</code>","text":"<pre><code>normalize(owner: Union[str, HubOwner]) -&gt; HubOwner\n</code></pre> <p>Normalize a string or <code>HubOwner</code> instance to a <code>HubOwner</code> instance.</p>"},{"location":"api/utils.types.html#polaris.utils.types.TargetType","title":"TargetType","text":"<p>               Bases: <code>Enum</code></p> <p>The high-level classification of different targets.</p>"},{"location":"api/utils.types.html#polaris.utils.types.TaskType","title":"TaskType","text":"<p>               Bases: <code>Enum</code></p> <p>The high-level classification of different tasks.</p>"},{"location":"tutorials/basics.html","title":"The Basics","text":"In\u00a0[2]: Copied! <pre>import polaris as po\nfrom polaris.hub.client import PolarisHubClient\n</pre> import polaris as po from polaris.hub.client import PolarisHubClient In\u00a0[\u00a0]: Copied! <pre>client = PolarisHubClient()\nclient.login()\n</pre> client = PolarisHubClient() client.login() <p>Instead of through the Python API, you could also use the Polaris CLI. See:</p> <pre>polaris login --help\n</pre> In\u00a0[4]: Copied! <pre>dataset = po.load_dataset(\"polaris/hello-world\")\nbenchmark = po.load_benchmark(\"polaris/hello-world-benchmark\")\n</pre> dataset = po.load_dataset(\"polaris/hello-world\") benchmark = po.load_benchmark(\"polaris/hello-world-benchmark\") <pre>2024-06-26 09:52:08.706 | INFO     | polaris._artifact:_validate_version:66 - The version of Polaris that was used to create the artifact (0.0.0) is different from the currently installed version of Polaris (0.0.2.dev191+g82e7db2).\n2024-06-26 09:52:10.327 | INFO     | polaris._artifact:_validate_version:66 - The version of Polaris that was used to create the artifact (0.0.0) is different from the currently installed version of Polaris (0.0.2.dev191+g82e7db2).\n2024-06-26 09:52:10.338 | INFO     | polaris._artifact:_validate_version:66 - The version of Polaris that was used to create the artifact (0.0.0) is different from the currently installed version of Polaris (0.0.2.dev191+g82e7db2).\n</pre> In\u00a0[5]: Copied! <pre>train, test = benchmark.get_train_test_split()\n</pre> train, test = benchmark.get_train_test_split() <p>The created objects support various flavours to access the data.</p> <ul> <li>The objects are iterable;</li> <li>The objects can be indexed;</li> <li>The objects have properties to access all data at once.</li> </ul> In\u00a0[6]: Copied! <pre>for x, y in train:\n    pass\n</pre> for x, y in train:     pass In\u00a0[7]: Copied! <pre>for i in range(len(train)):\n    x, y = train[i]\n</pre> for i in range(len(train)):     x, y = train[i] In\u00a0[8]: Copied! <pre>x = train.inputs\ny = train.targets\n</pre> x = train.inputs y = train.targets <p>To avoid accidental access to the test targets, the test object does not expose the labels and will throw an error if you try access them explicitly.</p> In\u00a0[9]: Copied! <pre>for x in test:\n    pass\n</pre> for x in test:     pass In\u00a0[10]: Copied! <pre>for i in range(len(test)):\n    x = test[i]\n</pre> for i in range(len(test)):     x = test[i] In\u00a0[11]: Copied! <pre>x = test.inputs\n\n# NOTE: The below will throw an error!\n# y = test.targets\n</pre> x = test.inputs  # NOTE: The below will throw an error! # y = test.targets In\u00a0[12]: Copied! <pre>import datamol as dm\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Load the benchmark (automatically loads the underlying dataset as well)\nbenchmark = po.load_benchmark(\"polaris/hello-world-benchmark\")\n\n# Get the split and convert SMILES to ECFP fingerprints by specifying an featurize function.\ntrain, test = benchmark.get_train_test_split(featurization_fn=dm.to_fp)\n\n# Define a model and train\nmodel = RandomForestRegressor(max_depth=2, random_state=0)\nmodel.fit(train.X, train.y)\n</pre> import datamol as dm from sklearn.ensemble import RandomForestRegressor  # Load the benchmark (automatically loads the underlying dataset as well) benchmark = po.load_benchmark(\"polaris/hello-world-benchmark\")  # Get the split and convert SMILES to ECFP fingerprints by specifying an featurize function. train, test = benchmark.get_train_test_split(featurization_fn=dm.to_fp)  # Define a model and train model = RandomForestRegressor(max_depth=2, random_state=0) model.fit(train.X, train.y) <pre>2024-06-26 09:52:12.003 | INFO     | polaris._artifact:_validate_version:66 - The version of Polaris that was used to create the artifact (0.0.0) is different from the currently installed version of Polaris (0.0.2.dev191+g82e7db2).\n2024-06-26 09:52:12.014 | INFO     | polaris._artifact:_validate_version:66 - The version of Polaris that was used to create the artifact (0.0.0) is different from the currently installed version of Polaris (0.0.2.dev191+g82e7db2).\n</pre> Out[12]: <pre>RandomForestRegressor(max_depth=2, random_state=0)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0RandomForestRegressor?Documentation for RandomForestRegressoriFitted<pre>RandomForestRegressor(max_depth=2, random_state=0)</pre> <p>To evaluate a model within Polaris, you should use the <code>evaluate()</code> endpoint. This requires you to just provide the predictions. The targets of the test set are automatically extracted so that the chance of the user accessing the test labels is minimal</p> In\u00a0[13]: Copied! <pre>predictions = model.predict(test.X)\n</pre> predictions = model.predict(test.X) In\u00a0[14]: Copied! <pre>results = benchmark.evaluate(predictions)\nresults\n</pre> results = benchmark.evaluate(predictions) results Out[14]: nameNonedescriptiontagsuser_attributesownerNonepolaris_version0.0.2.dev191+g82e7db2benchmark_namehello-world-benchmarkbenchmark_ownerslugpolarisexternal_idorg_2gtoaJIVrgRqiIR8Qm5BnpFCbxutypeorganizationgithub_urlNonepaper_urlNonecontributorsNoneartifact_idNonebenchmark_artifact_idpolaris/hello-world-benchmarkresultsTest setTarget labelMetricScoretestSOLmean_squared_error2.6875139821testSOLmean_absolute_error1.2735690161 <p>Before uploading the results to the Hub, you can provide some additional information about the results that will be displayed on the Polaris Hub.</p> In\u00a0[15]: Copied! <pre># For a complete list of meta-data, check out the BenchmarkResults object\nresults.name = \"hello-world-result\"\nresults.github_url = \"https://github.com/polaris-hub/polaris-hub\"\nresults.paper_url = \"https://polarishub.io/\"\nresults.description = \"Hello, World!\"\n</pre> # For a complete list of meta-data, check out the BenchmarkResults object results.name = \"hello-world-result\" results.github_url = \"https://github.com/polaris-hub/polaris-hub\" results.paper_url = \"https://polarishub.io/\" results.description = \"Hello, World!\" <p>Finally, let's upload the results to the Hub! The result will be private, but visiting the link in the logs you can decide to make it public through the Hub.</p> In\u00a0[16]: Copied! <pre>client.upload_results(results, owner=\"cwognum\")\nclient.close()\n</pre> client.upload_results(results, owner=\"cwognum\") client.close() <p>That's it! Just like that you have partaken in your first Polaris benchmark. In next tutorials, we will consider more advanced use cases of Polaris, such as creating and uploading your own datasets and benchmarks.</p> <p>The End.</p>"},{"location":"tutorials/basics.html#the-basics","title":"The Basics\u00b6","text":"<p>In short</p> <p>This tutorial walks you through the basic usage of Polaris. We will first login to the hub and will then see how easy it is to load a dataset or benchmark from it. Finally, we will train a simple baseline to submit a first set of results!</p> <p>Polaris is designed to standardize the process of constructing datasets, specifying benchmarks and evaluating novel machine learning techniques within the realm of drug discovery.</p> <p>While the Polaris library can be used independently from the Polaris Hub, the two were designed to seamlessly work together. The hub provides various pre-made, high quality datasets and benchmarks to develop and evaluate novel ML methods. In this tutorial, we will see how easy it is to load and use these datasets and benchmarks.</p>"},{"location":"tutorials/basics.html#login","title":"Login\u00b6","text":"<p>To be able to complete this step, you will require a Polaris Hub account. Go to https://polarishub.io/ to create one. You only have to log in once at the start or when you haven't used your account in a while.</p>"},{"location":"tutorials/basics.html#load-from-the-hub","title":"Load from the Hub\u00b6","text":"<p>Both datasets and benchmarks are identified by a <code>owner/name</code> id. You can easily find and copy these through the Hub. Once you have the id, loading a dataset or benchmark is incredibly easy.</p>"},{"location":"tutorials/basics.html#use-the-benchmark","title":"Use the benchmark\u00b6","text":"<p>The polaris library is designed to make it easy to participate in a benchmark. In just a few lines of code, we can get the train and test partition, access the associated data in various ways and evaluate our predictions. There's two main API endpoints.</p> <ul> <li><code>get_train_test_split()</code>: For creating objects through which we can access the different dataset partitions.</li> <li><code>evaluate()</code>: For evaluating a set of predictions in accordance with the benchmark protocol.</li> </ul>"},{"location":"tutorials/basics.html#partake-in-the-benchmark","title":"Partake in the benchmark\u00b6","text":"<p>To complete our example, let's participate in the benchmark. We will train a simple random forest model on the ECFP representation through scikit-learn and datamol.</p>"},{"location":"tutorials/custom_dataset_benchmark.html","title":"Data Models","text":"<p>In short</p> <p>This tutorial walks you through the dataset and benchmark data-structures. After creating our own custom dataset and benchmark, we will learn how to upload it to the Hub!</p> <p>We have already seen how easy it is to load a benchmark or dataset from the Polaris Hub. Let's now learn a bit more about the underlying data model by creating our own dataset and benchmark!</p> In\u00a0[3]: Copied! <pre>import pandas as pd\n\nPATH = (\n    \"https://raw.githubusercontent.com/molecularinformatics/Computational-ADME/main/ADME_public_set_3521.csv\"\n)\ntable = pd.read_csv(PATH)\ntable.head(5)\n</pre> import pandas as pd  PATH = (     \"https://raw.githubusercontent.com/molecularinformatics/Computational-ADME/main/ADME_public_set_3521.csv\" ) table = pd.read_csv(PATH) table.head(5) Out[3]: Internal ID Vendor ID SMILES CollectionName LOG HLM_CLint (mL/min/kg) LOG MDR1-MDCK ER (B-A/A-B) LOG SOLUBILITY PH 6.8 (ug/mL) LOG PLASMA PROTEIN BINDING (HUMAN) (% unbound) LOG PLASMA PROTEIN BINDING (RAT) (% unbound) LOG RLM_CLint (mL/min/kg) 0 Mol1 317714313 CNc1cc(Nc2cccn(-c3ccccn3)c2=O)nn2c(C(=O)N[C@@H... emolecules 0.675687 1.493167 0.089905 0.991226 0.518514 1.392169 1 Mol2 324056965 CCOc1cc2nn(CCC(C)(C)O)cc2cc1NC(=O)c1cccc(C(F)F)n1 emolecules 0.675687 1.040780 0.550228 0.099681 0.268344 1.027920 2 Mol3 304005766 CN(c1ncc(F)cn1)[C@H]1CCCNC1 emolecules 0.675687 -0.358806 NaN 2.000000 2.000000 1.027920 3 Mol4 194963090 CC(C)(Oc1ccc(-c2cnc(N)c(-c3ccc(Cl)cc3)c2)cc1)C... emolecules 0.675687 1.026662 1.657056 -1.158015 -1.403403 1.027920 4 Mol5 324059015 CC(C)(O)CCn1cc2cc(NC(=O)c3cccc(C(F)(F)F)n3)c(C... emolecules 0.996380 1.010597 NaN 1.015611 1.092264 1.629093 <p>While not required, a good dataset will specify additional meta-data to give further explanations on the data is contained within the dataset. This can be done on both the column level and on the dataset level.</p> In\u00a0[4]: Copied! <pre>from polaris.dataset import ColumnAnnotation\n\n# Additional meta-data on the column level\n# Of course, for a real dataset we should annotate all columns.\nannotations = {\n    \"LOG HLM_CLint (mL/min/kg)\": ColumnAnnotation(\n        desription=\"Microsomal stability\",\n        user_attributes={\"unit\": \"mL/min/kg\"},\n    ),\n    \"SMILES\": ColumnAnnotation(desription=\"Molecule SMILES string\", modality=\"molecule\"),\n}\n</pre> from polaris.dataset import ColumnAnnotation  # Additional meta-data on the column level # Of course, for a real dataset we should annotate all columns. annotations = {     \"LOG HLM_CLint (mL/min/kg)\": ColumnAnnotation(         desription=\"Microsomal stability\",         user_attributes={\"unit\": \"mL/min/kg\"},     ),     \"SMILES\": ColumnAnnotation(desription=\"Molecule SMILES string\", modality=\"molecule\"), } In\u00a0[5]: Copied! <pre>from polaris.dataset import Dataset\nfrom polaris.utils.types import HubOwner\n\ndataset = Dataset(\n    # The table is the core data-structure required to construct a dataset\n    table=table,\n    # Additional meta-data on the dataset level.\n    name=\"Fang_2023_DMPK\",\n    description=\"120 prospective data sets, collected over 20 months across six ADME in vitro endpoints\",\n    source=\"https://doi.org/10.1021/acs.jcim.3c00160\",\n    annotations=annotations,\n    tags=[\"DMPK\", \"ADME\"],\n    owner=HubOwner(user_id=\"cwognum\", slug=\"cwognum\"),\n    license=\"CC-BY-4.0\",\n    user_attributes={\"year\": \"2023\"},\n)\n</pre> from polaris.dataset import Dataset from polaris.utils.types import HubOwner  dataset = Dataset(     # The table is the core data-structure required to construct a dataset     table=table,     # Additional meta-data on the dataset level.     name=\"Fang_2023_DMPK\",     description=\"120 prospective data sets, collected over 20 months across six ADME in vitro endpoints\",     source=\"https://doi.org/10.1021/acs.jcim.3c00160\",     annotations=annotations,     tags=[\"DMPK\", \"ADME\"],     owner=HubOwner(user_id=\"cwognum\", slug=\"cwognum\"),     license=\"CC-BY-4.0\",     user_attributes={\"year\": \"2023\"}, ) In\u00a0[6]: Copied! <pre>import tempfile\n\ntemp_dir = tempfile.TemporaryDirectory().name\n</pre> import tempfile  temp_dir = tempfile.TemporaryDirectory().name In\u00a0[7]: Copied! <pre>import datamol as dm\n\nsave_dir = dm.fs.join(temp_dir, \"dataset\")\n</pre> import datamol as dm  save_dir = dm.fs.join(temp_dir, \"dataset\") In\u00a0[8]: Copied! <pre>path = dataset.to_json(save_dir)\n</pre> path = dataset.to_json(save_dir) <p>Looking at the save destination, we see this created two files: A JSON with all the meta-data and a <code>.parquet</code> file with the tabular data.</p> In\u00a0[9]: Copied! <pre>fs = dm.fs.get_mapper(save_dir).fs\nfs.ls(save_dir)\n</pre> fs = dm.fs.get_mapper(save_dir).fs fs.ls(save_dir) Out[9]: <pre>['/var/folders/1y/1v1blh6x56zdn027g5g9bwph0000gr/T/tmpe_g26lrl/dataset/table.parquet',\n '/var/folders/1y/1v1blh6x56zdn027g5g9bwph0000gr/T/tmpe_g26lrl/dataset/dataset.json']</pre> <p>Loading the dataset can be done through this JSON file.</p> In\u00a0[10]: Copied! <pre>import polaris as po\n\ndataset = po.load_dataset(path)\n</pre> import polaris as po  dataset = po.load_dataset(path) <p>We can also upload the dataset to the hub!</p> In\u00a0[11]: Copied! <pre>from polaris.hub.client import PolarisHubClient\n\n# NOTE: Commented out to not flood the DB\n# with PolarisHubClient() as client:\n#     client.upload_dataset(dataset=dataset)\n</pre> from polaris.hub.client import PolarisHubClient  # NOTE: Commented out to not flood the DB # with PolarisHubClient() as client: #     client.upload_dataset(dataset=dataset) In\u00a0[12]: Copied! <pre>import numpy as np\nfrom polaris.benchmark import SingleTaskBenchmarkSpecification\n\n# For the sake of simplicity, we use a very simple, ordered split\nsplit = (np.arange(3000).tolist(), (np.arange(521) + 3000).tolist())  # train  # test\n\nbenchmark = SingleTaskBenchmarkSpecification(\n    dataset=dataset,\n    target_cols=\"LOG SOLUBILITY PH 6.8 (ug/mL)\",\n    input_cols=\"SMILES\",\n    split=split,\n    metrics=\"mean_absolute_error\",\n)\n</pre> import numpy as np from polaris.benchmark import SingleTaskBenchmarkSpecification  # For the sake of simplicity, we use a very simple, ordered split split = (np.arange(3000).tolist(), (np.arange(521) + 3000).tolist())  # train  # test  benchmark = SingleTaskBenchmarkSpecification(     dataset=dataset,     target_cols=\"LOG SOLUBILITY PH 6.8 (ug/mL)\",     input_cols=\"SMILES\",     split=split,     metrics=\"mean_absolute_error\", ) <p>Metrics should be supported in the polaris framework.</p> <p>For more information, see the <code>Metric</code> class.</p> In\u00a0[13]: Copied! <pre>from polaris.evaluate import Metric\n\nlist(Metric)\n</pre> from polaris.evaluate import Metric  list(Metric) Out[13]: <pre>[&lt;Metric.mean_absolute_error: MetricInfo(fn=&lt;function mean_absolute_error at 0x169779c60&gt;, is_multitask=False)&gt;,\n &lt;Metric.mean_squared_error: MetricInfo(fn=&lt;function mean_squared_error at 0x16977a020&gt;, is_multitask=False)&gt;,\n &lt;Metric.accuracy: MetricInfo(fn=&lt;function accuracy_score at 0x169758540&gt;, is_multitask=False)&gt;]</pre> <p>To support the vast flexibility in specifying a benchmark, we have different classes that correspond to different types of benchmarks. Each of these subclasses makes the data-model or logic more specific to a particular case. For example, trying to create a multitask benchmark with the same arguments as we used above will throw an error as there is just a single target column specified.</p> In\u00a0[14]: Copied! <pre>from polaris.benchmark import MultiTaskBenchmarkSpecification\n\nbenchmark = MultiTaskBenchmarkSpecification(\n    dataset=dataset,\n    target_cols=\"LOG SOLUBILITY PH 6.8 (ug/mL)\",\n    input_cols=\"SMILES\",\n    split=split,\n    metrics=\"mean_absolute_error\",\n)\n</pre> from polaris.benchmark import MultiTaskBenchmarkSpecification  benchmark = MultiTaskBenchmarkSpecification(     dataset=dataset,     target_cols=\"LOG SOLUBILITY PH 6.8 (ug/mL)\",     input_cols=\"SMILES\",     split=split,     metrics=\"mean_absolute_error\", ) <pre>\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\n/Users/cas.wognum/Documents/repositories/polaris/docs/tutorials/custom_dataset_benchmark.ipynb Cell 25 line 3\n      &lt;a href='vscode-notebook-cell:/Users/cas.wognum/Documents/repositories/polaris/docs/tutorials/custom_dataset_benchmark.ipynb#X33sZmlsZQ%3D%3D?line=0'&gt;1&lt;/a&gt; from polaris.benchmark import MultiTaskBenchmarkSpecification\n----&gt; &lt;a href='vscode-notebook-cell:/Users/cas.wognum/Documents/repositories/polaris/docs/tutorials/custom_dataset_benchmark.ipynb#X33sZmlsZQ%3D%3D?line=2'&gt;3&lt;/a&gt; benchmark = MultiTaskBenchmarkSpecification(\n      &lt;a href='vscode-notebook-cell:/Users/cas.wognum/Documents/repositories/polaris/docs/tutorials/custom_dataset_benchmark.ipynb#X33sZmlsZQ%3D%3D?line=3'&gt;4&lt;/a&gt;     dataset=dataset,\n      &lt;a href='vscode-notebook-cell:/Users/cas.wognum/Documents/repositories/polaris/docs/tutorials/custom_dataset_benchmark.ipynb#X33sZmlsZQ%3D%3D?line=4'&gt;5&lt;/a&gt;     target_cols=\"LOG SOLUBILITY PH 6.8 (ug/mL)\",\n      &lt;a href='vscode-notebook-cell:/Users/cas.wognum/Documents/repositories/polaris/docs/tutorials/custom_dataset_benchmark.ipynb#X33sZmlsZQ%3D%3D?line=5'&gt;6&lt;/a&gt;     input_cols=\"SMILES\",\n      &lt;a href='vscode-notebook-cell:/Users/cas.wognum/Documents/repositories/polaris/docs/tutorials/custom_dataset_benchmark.ipynb#X33sZmlsZQ%3D%3D?line=6'&gt;7&lt;/a&gt;     split=split,\n      &lt;a href='vscode-notebook-cell:/Users/cas.wognum/Documents/repositories/polaris/docs/tutorials/custom_dataset_benchmark.ipynb#X33sZmlsZQ%3D%3D?line=7'&gt;8&lt;/a&gt;     metrics=\"mean_absolute_error\",\n      &lt;a href='vscode-notebook-cell:/Users/cas.wognum/Documents/repositories/polaris/docs/tutorials/custom_dataset_benchmark.ipynb#X33sZmlsZQ%3D%3D?line=8'&gt;9&lt;/a&gt; )\n\nFile ~/micromamba/envs/polaris/lib/python3.11/site-packages/pydantic/main.py:164, in BaseModel.__init__(__pydantic_self__, **data)\n    162 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    163 __tracebackhide__ = True\n--&gt; 164 __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)\n\nValidationError: 1 validation error for MultiTaskBenchmarkSpecification\ntarget_cols\n  Value error, A multi-task benchmark should specify at least two target columns [type=value_error, input_value='LOG SOLUBILITY PH 6.8 (ug/mL)', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error</pre> In\u00a0[15]: Copied! <pre>save_dir = dm.fs.join(temp_dir, \"benchmark\")\n</pre> save_dir = dm.fs.join(temp_dir, \"benchmark\") In\u00a0[16]: Copied! <pre>path = benchmark.to_json(save_dir)\n</pre> path = benchmark.to_json(save_dir) In\u00a0[17]: Copied! <pre>fs = dm.fs.get_mapper(save_dir).fs\nfs.ls(save_dir)\n</pre> fs = dm.fs.get_mapper(save_dir).fs fs.ls(save_dir) Out[17]: <pre>['/var/folders/1y/1v1blh6x56zdn027g5g9bwph0000gr/T/tmpe_g26lrl/benchmark/table.parquet',\n '/var/folders/1y/1v1blh6x56zdn027g5g9bwph0000gr/T/tmpe_g26lrl/benchmark/benchmark.json',\n '/var/folders/1y/1v1blh6x56zdn027g5g9bwph0000gr/T/tmpe_g26lrl/benchmark/dataset.json']</pre> <p>This created three files. Two <code>json</code> files and a single <code>parquet</code> file. The <code>parquet</code> file saves the tabular structure at the base of the <code>Dataset</code> class, whereas the <code>json</code> files save all the meta-data for the <code>Dataset</code> and <code>BenchmarkSpecification</code>.</p> <p>As before, loading the benchmark can be done through the JSON file.</p> In\u00a0[18]: Copied! <pre>benchmark = po.load_benchmark(path)\n</pre> benchmark = po.load_benchmark(path) <p>And as before, we can also upload the benchmark directly to the hub.</p> In\u00a0[19]: Copied! <pre># NOTE: Commented out to not flood the DB\n# with PolarisHubClient() as client:\n#     client.upload_benchmark(dataset=dataset)\n</pre> # NOTE: Commented out to not flood the DB # with PolarisHubClient() as client: #     client.upload_benchmark(dataset=dataset) <p>The End.</p>"},{"location":"tutorials/custom_dataset_benchmark.html#create-the-dataset","title":"Create the dataset\u00b6","text":"<p>A dataset in Polaris is at its core a tabular data-structure in which each row stores a single datapoint. For this example, we will process a multi-task DMPK dataset from Fang et al.. For the sake of simplicity, we don't do any curation and will just download the dataset as-is from their Github.</p> <p>The importance of curation</p> <p>While we do not address it in this tutorial, data curation is essential to an impactful benchmark. Because of this, we have not just made several high-quality benchmarks readily available on the Polaris Hub, but also open-sourced some of the tools we've built to curate these datasets.</p>"},{"location":"tutorials/custom_dataset_benchmark.html#save-and-load-the-dataset","title":"Save and load the dataset\u00b6","text":"<p>We can now save the dataset either to a local path or directly to the hub!</p>"},{"location":"tutorials/custom_dataset_benchmark.html#create-the-benchmark-specification","title":"Create the benchmark specification\u00b6","text":"<p>A benchmark is represented by the <code>BenchmarkSpecification</code>, which wraps a <code>Dataset</code> with additional data to produce a benchmark.</p> <p>It specifies:</p> <ol> <li>Which dataset to use (see Dataset);</li> <li>Which columns are used as input and which columns are used as target;</li> <li>Which metrics should be used to evaluate performance on this task;</li> <li>A predefined, static train-test split to use during evaluation.</li> </ol>"},{"location":"tutorials/custom_dataset_benchmark.html#save-and-load-the-benchmark","title":"Save and load the benchmark\u00b6","text":"<p>Saving the benchmark is easy and can be done with a single line of code.</p>"},{"location":"tutorials/dataset_factory.html","title":"Dataset Factory","text":"<p>In short</p> <p>This tutorial shows how we can create more complicated datasets by leveraging the dataset factory in Polaris.</p> <p>This feature is still very new</p> <p>The features we will show in this tutorial are still experimental. We would love to learn from the community how we can make it easier to create datasets.</p> In\u00a0[2]: Copied! <pre>import platformdirs\nimport datamol as dm\n</pre> import platformdirs import datamol as dm In\u00a0[3]: Copied! <pre>SAVE_DIR = dm.fs.join(platformdirs.user_cache_dir(appname=\"polaris-tutorials\"), \"003\")\n</pre> SAVE_DIR = dm.fs.join(platformdirs.user_cache_dir(appname=\"polaris-tutorials\"), \"003\") In\u00a0[4]: Copied! <pre># Let's generate a toy dataset with a single molecule\nsmiles = \"Cn1cnc2c1c(=O)n(C)c(=O)n2C\"\nmol = dm.to_mol(smiles)\n\n# We will generate 3D conformers for this molecule with some conformers\nmol = dm.conformers.generate(mol, align_conformers=True)\n\n# Let's also set a molecular property\nmol.SetProp(\"my_property\", \"my_value\")\n\nmol\n</pre> # Let's generate a toy dataset with a single molecule smiles = \"Cn1cnc2c1c(=O)n(C)c(=O)n2C\" mol = dm.to_mol(smiles)  # We will generate 3D conformers for this molecule with some conformers mol = dm.conformers.generate(mol, align_conformers=True)  # Let's also set a molecular property mol.SetProp(\"my_property\", \"my_value\")  mol Out[4]: my_propertymy_value In\u00a0[5]: Copied! <pre>path = dm.fs.join(SAVE_DIR, \"caffeine.sdf\")\ndm.to_sdf(mol, path)\n</pre> path = dm.fs.join(SAVE_DIR, \"caffeine.sdf\") dm.to_sdf(mol, path) <p>This being a toy example, it is a very small dataset. However, for many real-world datasets SDF files can quickly get large, at which point it is no longer efficient to store everything directly in the Pandas DataFrame. This is why Polaris supports pointer columns to store large data outside of the DataFrame in a Zarr archive. But... How to convert from SDF to Zarr?</p> <p>There are a lot of considerations here:</p> <ul> <li>You want read and write operations to be quick.</li> <li>You want to reduce the storage requirements.</li> <li>You want the conversion to be lossless.</li> </ul> <p>Chances are you've no in-depth understanding of how Zarr works, making it a big investment to convert your SDF dataset to Zarr.</p> <p><code>DatasetFactory</code> to the rescue!</p> In\u00a0[6]: Copied! <pre>from polaris.dataset import DatasetFactory\nfrom polaris.dataset.converters import SDFConverter\n\n# Create a new factory object\nsave_dst = dm.fs.join(SAVE_DIR, \"data.zarr\")\nfactory = DatasetFactory(zarr_root_path=save_dst)\n\n# Register a converter for the SDF file format\nfactory.register_converter(\"sdf\", SDFConverter())\n\n# Process your SDF file\nfactory.add_from_file(path)\n\n# Build the dataset\ndataset = factory.build()\n</pre> from polaris.dataset import DatasetFactory from polaris.dataset.converters import SDFConverter  # Create a new factory object save_dst = dm.fs.join(SAVE_DIR, \"data.zarr\") factory = DatasetFactory(zarr_root_path=save_dst)  # Register a converter for the SDF file format factory.register_converter(\"sdf\", SDFConverter())  # Process your SDF file factory.add_from_file(path)  # Build the dataset dataset = factory.build() <p>That's all! Let's take a closer look at what this has actually done.</p> In\u00a0[7]: Copied! <pre>dataset.annotations\n</pre> dataset.annotations Out[7]: <pre>{'smiles': ColumnAnnotation(is_pointer=False, modality=&lt;Modality.MOLECULE: 'molecule'&gt;, description=None, user_attributes={}, dtype=dtype('O')),\n 'my_property': ColumnAnnotation(is_pointer=False, modality=&lt;Modality.UNKNOWN: 'unknown'&gt;, description=None, user_attributes={}, dtype=dtype('O')),\n 'molecule': ColumnAnnotation(is_pointer=True, modality=&lt;Modality.MOLECULE_3D: 'molecule_3D'&gt;, description=None, user_attributes={}, dtype=dtype('O'))}</pre> In\u00a0[8]: Copied! <pre>dataset.get_data(row=0, col=\"molecule\")\n</pre> dataset.get_data(row=0, col=\"molecule\") Out[8]: <p>We can see that Polaris has:</p> <ul> <li>Saved the molecule in an external Zarr archive and set the column annotations accordingly.</li> <li>Has extracted the molecule-level properties as additional columns.</li> <li>Has added an additional column with the SMILES.</li> <li>Effortlessly saves and loads the molecule object from the Zarr.</li> </ul> In\u00a0[9]: Copied! <pre>from polaris.dataset import create_dataset_from_file\n\ndataset = create_dataset_from_file(path, save_dst)\ndataset.get_data(row=0, col=\"molecule\")\n</pre> from polaris.dataset import create_dataset_from_file  dataset = create_dataset_from_file(path, save_dst) dataset.get_data(row=0, col=\"molecule\") Out[9]: <p>The <code>DatasetFactory</code> is based on the factory design pattern. That way, you can easily create and add your own file converters. However, the defaults are set to be a good option for most people.</p> <p>Let's consider two cases that show the power of the <code>DatasetFactory</code> design.</p> In\u00a0[10]: Copied! <pre>save_dst = dm.fs.join(SAVE_DIR, \"data2.zarr\")\nfactory.reset(save_dst)\n\n# Configure the converter\nconverter = SDFConverter(mol_prop_as_cols=False)\n\n# Overwrite the converter for SDF files\nfactory.register_converter(\"sdf\", converter)\n\n# Process the SDF file again\nfactory.add_from_file(path)\n\n# Build the dataset\ndataset = factory.build()\n</pre> save_dst = dm.fs.join(SAVE_DIR, \"data2.zarr\") factory.reset(save_dst)  # Configure the converter converter = SDFConverter(mol_prop_as_cols=False)  # Overwrite the converter for SDF files factory.register_converter(\"sdf\", converter)  # Process the SDF file again factory.add_from_file(path)  # Build the dataset dataset = factory.build() <pre>2024-03-26 13:16:43.897 | INFO     | polaris.dataset._factory:register_converter:112 - You are overwriting the converter for the sdf extension.\n</pre> <p>And voila! The property is saved to the Zarr instead of to a separate column.</p> In\u00a0[11]: Copied! <pre>dataset.get_data(row=0, col=\"molecule\")\n</pre> dataset.get_data(row=0, col=\"molecule\") Out[11]: my_propertymy_value In\u00a0[12]: Copied! <pre>dataset.table\n</pre> dataset.table Out[12]: smiles molecule 0 CN1C=NC2=C1C(=O)N(C)C(=O)N2C molecule#0 In\u00a0[13]: Copied! <pre>save_dst = dm.fs.join(SAVE_DIR, \"data3.zarr\")\nfactory.reset(save_dst)\n\n# Let's pretend these are two different SDF files\nfactory.register_converter(\"sdf\", SDFConverter(mol_column=\"molecule1\", smiles_column=None))\nfactory.add_from_file(path)\n\n# We change the configuration between files\nfactory.register_converter(\"sdf\", SDFConverter(mol_column=\"molecule2\", mol_prop_as_cols=False))\nfactory.add_from_file(path)\n\ndataset = factory.build()\n</pre> save_dst = dm.fs.join(SAVE_DIR, \"data3.zarr\") factory.reset(save_dst)  # Let's pretend these are two different SDF files factory.register_converter(\"sdf\", SDFConverter(mol_column=\"molecule1\", smiles_column=None)) factory.add_from_file(path)  # We change the configuration between files factory.register_converter(\"sdf\", SDFConverter(mol_column=\"molecule2\", mol_prop_as_cols=False)) factory.add_from_file(path)  dataset = factory.build() <pre>2024-03-26 13:16:43.938 | INFO     | polaris.dataset._factory:register_converter:112 - You are overwriting the converter for the sdf extension.\n2024-03-26 13:16:43.945 | INFO     | polaris.dataset._factory:register_converter:112 - You are overwriting the converter for the sdf extension.\n</pre> In\u00a0[14]: Copied! <pre>dataset.table\n</pre> dataset.table Out[14]: my_property molecule1 smiles molecule2 0 my_value molecule1#0 CN1C=NC2=C1C(=O)N(C)C(=O)N2C molecule2#0 <p>The End.</p>"},{"location":"tutorials/dataset_factory.html#dataset-factory","title":"Dataset Factory\u00b6","text":"<p>Datasets in Polaris are expected to be saved in a very specific format. This format has been carefully designed to be as universal and performant as possible. Nevertheless, we expect very few datasets to be readily available in this format. We therefore provide the <code>DatasetFactory</code> as a way to more easily convert datasets to the Polaris specific format.</p> <p>Let's assume we have a dataset in the SDF format.</p>"},{"location":"tutorials/dataset_factory.html#factory-design-pattern","title":"Factory Design Pattern\u00b6","text":"<p>If you've been dilligently going through the tutorials, you might remember that there is a function that seems to be doing something similar. And you would be right!</p>"},{"location":"tutorials/dataset_factory.html#configuring-the-converter","title":"Configuring the converter\u00b6","text":"<p>Let's assume we do not want to extract the properties as separate columns, but rather keep them in the RDKit object. We cannot do this with the default converter, but we can configure its behavior to achieve this.</p>"},{"location":"tutorials/dataset_factory.html#merging-data-from-different-sources","title":"Merging data from different sources\u00b6","text":"<p>Another case is when you want to merge data from multiple sources. Maybe you have two different SDF files.</p>"},{"location":"tutorials/dataset_zarr.html","title":"Zarr Datasets","text":"<p>In short</p> <p>This tutorial shows how to create datasets with more advanced data-modalities through the .zarr format.</p> In\u00a0[2]: Copied! <pre>import zarr\nimport platformdirs\n\nimport numpy as np\nimport datamol as dm\nimport pandas as pd\n\nSAVE_DIR = dm.fs.join(platformdirs.user_cache_dir(appname=\"polaris-tutorials\"), \"002\")\n</pre> import zarr import platformdirs  import numpy as np import datamol as dm import pandas as pd  SAVE_DIR = dm.fs.join(platformdirs.user_cache_dir(appname=\"polaris-tutorials\"), \"002\") In\u00a0[3]: Copied! <pre># Create two images and save them to a Zarr archive\nbase_path = dm.fs.join(SAVE_DIR, \"data.zarr\")\ninp_col_name = \"images\"\n\nimages = np.random.random((2, 64, 64, 3))\nroot = zarr.open(base_path, \"w\")\nroot.array(inp_col_name, images)\n</pre> # Create two images and save them to a Zarr archive base_path = dm.fs.join(SAVE_DIR, \"data.zarr\") inp_col_name = \"images\"  images = np.random.random((2, 64, 64, 3)) root = zarr.open(base_path, \"w\") root.array(inp_col_name, images) Out[3]: <pre>&lt;zarr.core.Array '/images' (2, 64, 64, 3) float64&gt;</pre> In\u00a0[4]: Copied! <pre># For performance reasons, Polaris expects all data related to a column to be saved in a single Zarr array. \n# To index a specific element in that array, the pointer path can have a suffix to specify the index. \ntrain_path = f\"{inp_col_name}#0\"\ntest_path = f\"{inp_col_name}#1\"\n</pre> # For performance reasons, Polaris expects all data related to a column to be saved in a single Zarr array.  # To index a specific element in that array, the pointer path can have a suffix to specify the index.  train_path = f\"{inp_col_name}#0\" test_path = f\"{inp_col_name}#1\" In\u00a0[5]: Copied! <pre>tgt_col_name = \"target\"\n\ntable = pd.DataFrame(\n    {\n        inp_col_name: [train_path, test_path],  # Instead of the content, we specify paths\n        tgt_col_name: np.random.random(2),\n    }\n)\n</pre> tgt_col_name = \"target\"  table = pd.DataFrame(     {         inp_col_name: [train_path, test_path],  # Instead of the content, we specify paths         tgt_col_name: np.random.random(2),     } ) In\u00a0[6]: Copied! <pre>from polaris.dataset import Dataset, ColumnAnnotation\n\ndataset = Dataset(\n    table=table,\n    # To indicate that we are dealing with a pointer column here,\n    # we need to annotate the column.\n    annotations={\"images\": ColumnAnnotation(is_pointer=True)},\n    # We also need to specify the path to the root of the Zarr archive\n    zarr_root_path=base_path,\n)\n</pre> from polaris.dataset import Dataset, ColumnAnnotation  dataset = Dataset(     table=table,     # To indicate that we are dealing with a pointer column here,     # we need to annotate the column.     annotations={\"images\": ColumnAnnotation(is_pointer=True)},     # We also need to specify the path to the root of the Zarr archive     zarr_root_path=base_path, ) <p>Note how the table does not contain the image data, but rather stores a path relative to the root of the Zarr.</p> In\u00a0[7]: Copied! <pre>dataset.table.loc[0, \"images\"]\n</pre> dataset.table.loc[0, \"images\"] Out[7]: <pre>'images#0'</pre> <p>To load the data that is being pointed to, you can simply use the <code>Dataset.get_data()</code> utility method.</p> In\u00a0[8]: Copied! <pre>dataset.get_data(col=\"images\", row=0).shape\n</pre> dataset.get_data(col=\"images\", row=0).shape Out[8]: <pre>(64, 64, 3)</pre> <p>Creating a benchmark and the associated <code>Subset</code> objects will automatically do so!</p> In\u00a0[9]: Copied! <pre>from polaris.benchmark import SingleTaskBenchmarkSpecification\n\nbenchmark = SingleTaskBenchmarkSpecification(\n    dataset=dataset,\n    input_cols=inp_col_name,\n    target_cols=tgt_col_name,\n    metrics=\"mean_absolute_error\",\n    split=([0], [1]),\n)\n</pre> from polaris.benchmark import SingleTaskBenchmarkSpecification  benchmark = SingleTaskBenchmarkSpecification(     dataset=dataset,     input_cols=inp_col_name,     target_cols=tgt_col_name,     metrics=\"mean_absolute_error\",     split=([0], [1]), ) In\u00a0[10]: Copied! <pre>train, test = benchmark.get_train_test_split()\n\nfor x, y in train:\n    # At this point, the content is loaded from the path specified in the table\n    print(x.shape)\n</pre> train, test = benchmark.get_train_test_split()  for x, y in train:     # At this point, the content is loaded from the path specified in the table     print(x.shape) <pre>(64, 64, 3)\n</pre> In\u00a0[11]: Copied! <pre># Let's first create some dummy dataset with 1000 64x64 \"images\"\nimages = np.random.random((1000, 64, 64, 3))\n</pre> # Let's first create some dummy dataset with 1000 64x64 \"images\" images = np.random.random((1000, 64, 64, 3)) In\u00a0[12]: Copied! <pre>path = dm.fs.join(SAVE_DIR, \"zarr\", \"data.zarr\")\n\nwith zarr.open(path, \"w\") as root:\n    root.array(inp_col_name, images)\n</pre> path = dm.fs.join(SAVE_DIR, \"zarr\", \"data.zarr\")  with zarr.open(path, \"w\") as root:     root.array(inp_col_name, images) <p>To create a dataset from a Zarr archive, we can use the convenience function <code>create_dataset_from_file()</code>.</p> In\u00a0[13]: Copied! <pre>from polaris.dataset import create_dataset_from_file\n\n# Because Polaris might restructure the Zarr archive, \n# we need to specify a location to save the Zarr file to.\ndataset = create_dataset_from_file(path, zarr_root_path=dm.fs.join(SAVE_DIR, \"zarr\", \"processed.zarr\"))\n\n# The path refers to the original zarr directory we created in the above code block\ndataset.table.iloc[0][inp_col_name]\n</pre> from polaris.dataset import create_dataset_from_file  # Because Polaris might restructure the Zarr archive,  # we need to specify a location to save the Zarr file to. dataset = create_dataset_from_file(path, zarr_root_path=dm.fs.join(SAVE_DIR, \"zarr\", \"processed.zarr\"))  # The path refers to the original zarr directory we created in the above code block dataset.table.iloc[0][inp_col_name] Out[13]: <pre>'images#0'</pre> In\u00a0[14]: Copied! <pre>dataset.get_data(col=inp_col_name, row=0).shape\n</pre> dataset.get_data(col=inp_col_name, row=0).shape Out[14]: <pre>(64, 64, 3)</pre> In\u00a0[15]: Copied! <pre>savedir = dm.fs.join(SAVE_DIR, \"json\")\njson_path = dataset.to_json(savedir)\n</pre> savedir = dm.fs.join(SAVE_DIR, \"json\") json_path = dataset.to_json(savedir) In\u00a0[16]: Copied! <pre>fs = dm.fs.get_mapper(path).fs\nfs.ls(SAVE_DIR)\n</pre> fs = dm.fs.get_mapper(path).fs fs.ls(SAVE_DIR) Out[16]: <pre>['/home/cas/.cache/polaris-tutorials/002/zarr',\n '/home/cas/.cache/polaris-tutorials/002/json',\n '/home/cas/.cache/polaris-tutorials/002/data.zarr']</pre> <p>Besides the <code>table.parquet</code> and <code>dataset.yaml</code>, we can now also see a <code>data</code> folder which stores the content for the additional content from the pointer columns.</p> In\u00a0[17]: Copied! <pre>Dataset.from_json(json_path)\n</pre> Dataset.from_json(json_path) Out[17]: nameNonedescriptiontagsuser_attributesownerNonedefault_adapterszarr_root_path/home/cas/.cache/polaris-tutorials/002/json/data.zarrmd5sum5488b4909fd67d3208624288e720e1b8readmeannotationsimagesis_pointerTruemodalityUNKNOWNdescriptionNoneuser_attributesdtypeobjectsourceNonelicenseNonecuration_referenceNonecache_dir/home/cas/.cache/polaris/datasets/None/5488b4909fd67d3208624288e720e1b8artifact_idNonen_rows1000n_columns1 <p>The End.</p>"},{"location":"tutorials/dataset_zarr.html#pointer-columns","title":"Pointer columns\u00b6","text":"<p>Not all data might fit the tabular format, e.g. images or conformers. In that case, we have pointer columns. Pointer columns do not contain the data itself, but rather store a reference to an external file from which the content can be loaded.</p> <p>For now, we only support <code>.zarr</code> files as references. To learn more about <code>.zarr</code>, visit their documentation. Their tutorial specifically is a good read to better understand the main features.</p>"},{"location":"tutorials/dataset_zarr.html#dummy-example","title":"Dummy example\u00b6","text":"<p>For the sake of simplicity, let's assume we have just two datapoints. We will use this to demonstrate the idea behind pointer columns.</p>"},{"location":"tutorials/dataset_zarr.html#creating-datasets-from-zarr-arrays","title":"Creating datasets from <code>.zarr</code> arrays\u00b6","text":"<p>While the above example works, creating the table with all paths from scratch is time-consuming when datasets get large. Instead, you can also automatically parse a Zarr archive into the expected tabular data structure.</p> <p>A Zarr archive can contain groups and arrays, where each group can again contain groups and arrays. Within Polaris, we expect the root to be a flat hierarchy that contains a single array per column.</p>"},{"location":"tutorials/dataset_zarr.html#a-single-array-for-all-datapoints","title":"A single array for all datapoints\u00b6","text":"<p>Polaris expects a flat zarr hierarchy, with a single array per pointer column:</p> <pre><code>/\n  column_a\n</code></pre> <p>Which will get parsed into a table like:</p> column_a column_a/array#1 column_a/array#2 ... column_a/array#N <p>Note</p> <p>Notice the # suffix in the path, which indicates the index at which the data-point is stored within the big array. </p>"},{"location":"tutorials/dataset_zarr.html#saving-the-dataset","title":"Saving the dataset\u00b6","text":"<p>We can still easily save the dataset. All the pointer columns will be automatically updated.</p>"},{"location":"tutorials/dataset_zarr.html#load-the-dataset","title":"Load the dataset\u00b6","text":""},{"location":"tutorials/optimization.html","title":"Optimization","text":"<p>In short</p> <p>This tutorial shows how to optimize a Polaris dataset to improve its efficiency.</p> <p>No magic bullet</p> <p>What works best really depends on the specific dataset you're using and you will benefit from trying out different ways of storing the data.</p> In\u00a0[2]: Copied! <pre>import numpy as np\nimport pandas as pd\n</pre> import numpy as np import pandas as pd In\u00a0[3]: Copied! <pre># Let's create a dummy dataset with two columns \nrng = np.random.default_rng(0)\ncol_a = rng.choice(list(range(100)), 10000)\ncol_b = rng.random(10000)\ntable = pd.DataFrame({\"A\": col_a, \"B\": col_b})\n</pre> # Let's create a dummy dataset with two columns  rng = np.random.default_rng(0) col_a = rng.choice(list(range(100)), 10000) col_b = rng.random(10000) table = pd.DataFrame({\"A\": col_a, \"B\": col_b}) <p>By default, Pandas (and NumPy) use the largest dtype available.</p> In\u00a0[4]: Copied! <pre>table.dtypes\n</pre> table.dtypes Out[4]: <pre>A      int64\nB    float64\ndtype: object</pre> In\u00a0[5]: Copied! <pre>table.memory_usage().sum()\n</pre> table.memory_usage().sum() Out[5]: <pre>160132</pre> <p>However, we know that column A only has values between 0 and 99, so we won't need the full <code>int64</code> dtype. The <code>np.int16</code> is already more appropriate!</p> In\u00a0[6]: Copied! <pre>table[\"A\"] = table[\"A\"].astype(np.int16)\ntable.memory_usage().sum()\n</pre> table[\"A\"] = table[\"A\"].astype(np.int16) table.memory_usage().sum() Out[6]: <pre>100132</pre> <p>We managed to reduce the number of bytes by ~60k (or 60KB). That's 37.5% less!</p> <p>Now imagine we would be talking about gigabyte-sized dataset!</p> In\u00a0[7]: Copied! <pre>import os\nimport zarr\nfrom tempfile import mkdtemp\n\ntmpdir =  mkdtemp()\n\n# For the ones familiar with Zarr, this is not optimized at all. \n# If you wouldn't want to convert to NumPy, you would want to \n# optimize the chunking / compression.\n\npath = os.path.join(tmpdir, \"data.zarr\")\nroot = zarr.open(path, \"w\")\nroot.array(\"A\", rng.random(10000))\nroot.array(\"B\", rng.random(10000));\n</pre> import os import zarr from tempfile import mkdtemp  tmpdir =  mkdtemp()  # For the ones familiar with Zarr, this is not optimized at all.  # If you wouldn't want to convert to NumPy, you would want to  # optimize the chunking / compression.  path = os.path.join(tmpdir, \"data.zarr\") root = zarr.open(path, \"w\") root.array(\"A\", rng.random(10000)) root.array(\"B\", rng.random(10000)); In\u00a0[8]: Copied! <pre>from polaris.dataset import create_dataset_from_file\n\nroot_path = os.path.join(tmpdir, \"data\", \"data.zarr\")\ndataset = create_dataset_from_file(path, zarr_root_path=root_path)\n</pre> from polaris.dataset import create_dataset_from_file  root_path = os.path.join(tmpdir, \"data\", \"data.zarr\") dataset = create_dataset_from_file(path, zarr_root_path=root_path) In\u00a0[9]: Copied! <pre>from polaris.dataset import Subset\n\nsubset = Subset(dataset, np.arange(len(dataset)), \"A\", \"B\")\n</pre> from polaris.dataset import Subset  subset = Subset(dataset, np.arange(len(dataset)), \"A\", \"B\") <p>For the sake of this example, we will use PyTorch.</p> In\u00a0[10]: Copied! <pre>from torch.utils.data import DataLoader\n\ndataloader = DataLoader(subset, batch_size=64, shuffle=True)\n</pre> from torch.utils.data import DataLoader  dataloader = DataLoader(subset, batch_size=64, shuffle=True) <p>Let's see how fast this is!</p> In\u00a0[11]: Copied! <pre>%%timeit\nfor batch in dataloader: \n    pass\n</pre> %%timeit for batch in dataloader:      pass <pre>1.45 s \u00b1 22 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</pre> <p>That's pretty slow... Let's see if Polaris its optimization helps.</p> In\u00a0[12]: Copied! <pre>dataset.load_to_memory()\n</pre> dataset.load_to_memory() In\u00a0[13]: Copied! <pre>%%timeit\nfor batch in dataloader: \n    pass\n</pre> %%timeit for batch in dataloader:      pass <pre>99.4 ms \u00b1 2.45 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</pre> <p>That's a lot faster!</p> <p>Now all that's left to do, is to clean up the temporary directory.</p> In\u00a0[14]: Copied! <pre>from shutil import rmtree\nrmtree(tmpdir)\n</pre> from shutil import rmtree rmtree(tmpdir) <p>The End.</p>"},{"location":"tutorials/optimization.html#datasets-that-fit-in-memory","title":"Datasets that fit in memory\u00b6","text":"<p>Through the Polaris <code>Subset</code> class, we aim to provide a general purpose data loader that serves as a good default for a variety of use cases.</p> <p>As a dataset creator, it is important to be mindful of some design decisions you can make to improve performance for your downstream users. These design decisions are most impactful!</p> <p>As a dataset user, we provide the <code>Dataset.load_to_memory()</code> method to load the uncompressed dataset into memory. This is limited though, because there is only so much we can do automatically without risking data integrity.</p> <p>Despite our best efforts to provide a data loader that is as efficient as possible, you will always be able to optimize things further for a specific use case if needed.</p>"},{"location":"tutorials/optimization.html#without-zarr","title":"Without Zarr\u00b6","text":"<p>Without pointer columns, the best way to optimize your dataset's performance is by making sure you use the appropriate dtype. A smaller memory footprint not only reduces storage requirements, but also speeds up moving data around (e.g. to the GPU or to create <code>torch.Tensor</code> objects).</p>"},{"location":"tutorials/optimization.html#with-zarr","title":"With Zarr\u00b6","text":"<p>If part of the dataset is stored in a Zarr archive - and that Zarr archive fits in memory (remember to optimize the <code>dtype</code>) - the most efficient thing to do is to just convert from Zarr to a NumPy array. Zarr is not built to support this use case specifically and NumPy is optimized for it. For more information, see e.g. this Github issue.</p> <p>Luckily, you don't have to do this yourself. You can use Polaris its <code>Dataset.load_to_memory()</code>.</p> <p>Let's again start by creating a dummy dataset!</p>"},{"location":"tutorials/optimization.html#datasets-that-fit-on-a-local-disk","title":"Datasets that fit on a local disk\u00b6","text":"<p>For datasets that don't fit in memory, but that can be stored on a local disk, the most impactful design decision is how the dataset is chunked.</p> <p>Zarr datasets are chunked. When you try to load one piece of data, the entire chunk that data is part of has to be loaded into memory and decompressed. Remember that in ML, data access is typically random, which is a terrible access pattern because you are likely to reload chunks into memory.</p> <p>Most efficient is thus to chunk the data such that each chunk only contains a single data point.</p> <ul> <li>Benefit: No longer induce a performance penalty due to loading additional data into memory that it might not need.</li> <li>Downside: You might be able to compress the data more if you can consider similarities across data points while compressing.</li> </ul> <p>A note on rechunking: Within Polaris, you do not have control over how a dataset on the Hub is chunked. In that case, rechunking is needed. This can induce a one-time, but nevertheless big performance penalty (see also the Zarr docs). I don\u2019t expect this to be an issue in the short-term given the size of the dataset we will be working with, but Zarr recommends using the rechunker Python package to improve performance.</p>"},{"location":"tutorials/optimization.html#remote-datasets","title":"Remote Datasets\u00b6","text":"<p>In this case, you really benefit from improving memory storage by trying different compressors.</p> <p>See also this article.</p>"}]}