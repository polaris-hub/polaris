{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Introduction","text":"<p>Welcome to the Polaris documentation!</p>"},{"location":"index.html#what-is-polaris","title":"What is Polaris?","text":"<p>Our vision</p> <p>Polaris aims to foster the development of impactful AI models in drug discovery by establishing a new  and adaptive standard for measuring progress of computational tools in drug discovery.</p> <p>Polaris is a suite of tools to implement, host and run benchmarks in computational drug discovery. Existing benchmarks leave several key challenges - related to the characteristics of datasets in drug discovery - unaddressed. This can lead to a situation in which newly proposed models do not perform as well as advertised in real drug discovery programs, ultimately risking misalignment between the scientists developing the models and downstream users. With Polaris, we aim to further close that gap. </p>"},{"location":"index.html#polaris-hub","title":"Polaris Hub","text":"<p>A quick word on the Polaris Hub. The hub hosts a variety of high-quality benchmarks and datasets. While the hub is built to easily integrate with the Polaris library, you can use them independently.</p>"},{"location":"index.html#where-to-next","title":"Where to next?","text":"<p>  Quickstart</p> <p>If you are entirely new to Polaris, this is the place to start! Learn about the essential concepts and partake in your first benchmark.</p> <p> Let's get started</p> <p>  Tutorials</p> <p>Dive deeper into the Polaris code and learn about advanced concepts to create your own benchmarks and datasets. </p> <p> Let's get started</p> <p>  API Reference</p> <p>This is where you will find the technical documentation of the code itself. Learn the intricate details of how the various methods and classes work.</p> <p> Let's get started</p> <p>  Community</p> <p>Whether you are a first-time contributor or open-source veteran, we welcome any contribution to Polaris. Learn more about our community initiatives.</p> <p> Let's get started</p>"},{"location":"quickstart.html","title":"Quickstart","text":""},{"location":"quickstart.html#installation","title":"Installation","text":"<p>First things first, let's install Polaris!</p> <p>We highly recommend using a Conda Python distribution, such as <code>mamba</code>:</p> <pre><code>mamba install -c conda-forge polaris\n</code></pre> Other installation options <p>You can replace <code>mamba</code> by <code>conda</code>. The package is also pip installable if you need it: <code>pip install polaris-lib</code>.</p>"},{"location":"quickstart.html#authenticating-to-the-polaris-hub","title":"Authenticating to the Polaris Hub","text":"<p>To interact with the Polaris Hub from the client, you must first login. You can do this via the following command in your terminal:</p> <pre><code>polaris login\n</code></pre> <p>This will redirect you to a login page on the Polaris Hub where you can either sign in or sign up. Once either of these options have been completed, you will see an authorization code on your screen. Copy this and paste it  back into your terminal when prompted by the client.</p> <p>That's it! You're now all set to interact with datasets and benchmarks across Polaris.</p>"},{"location":"quickstart.html#benchmarking-api","title":"Benchmarking API","text":"<p>At its core, Polaris is a benchmarking library. It provides a simple API to run benchmarks. While it can be used independently, it is built to easily integrate with the Polaris Hub. The hub hosts a variety of high-quality datasets, benchmarks and associated results.</p> <p>If all you care about is to partake in a benchmark that is hosted on the hub, it is as simple as:</p> <pre><code>import polaris as po\n\n# Load the benchmark from the Hub\nbenchmark = po.load_benchmark(\"polaris/hello-world-benchmark\")\n\n# Get the train and test data-loaders\ntrain, test = benchmark.get_train_test_split()\n\n# Use the training data to train your model\n# Get the input as an array with 'train.inputs' and 'train.targets'  \n# Or simply iterate over the train object.\nfor x, y in train:\n    ...\n\n# Work your magic to accurately predict the test set\npredictions = [0.0 for x in test]\n\n# Evaluate your predictions\nresults = benchmark.evaluate(predictions)\n\n# Submit your results\nresults.upload_to_hub(owner=\"dummy-user\")\n</code></pre> <p>That's all there is to it to partake in a benchmark. No complicated, custom data-loaders or evaluation protocol. With just a few lines of code, you can feel confident that you are properly evaluating your model and focus on what you do best: Solving the hard problems in our domain!</p> <p>Similarly, you can easily access a dataset.</p> <pre><code>import polaris as po\n\n# Load the dataset from the hub\ndataset = po.load_dataset(\"polaris/hello-world\")\n\n# Get information on the dataset size\ndataset.size()\n\n# Load a datapoint in memory\ndataset.get_data(\n    row=dataset.rows[0],\n    col=dataset.columns[0],\n)\n\n# Or, similarly:\ndataset[dataset.rows[0], dataset.columns[0]]\n\n# Get an entire row\ndataset[0]\n</code></pre>"},{"location":"quickstart.html#core-concepts","title":"Core concepts","text":"<p>At the core of our API are 4 core concepts, each associated with a class:</p> <ol> <li><code>Dataset</code>: The dataset class is carefully designed data-structure, stress-tested on terra-bytes of data, to ensure whatever dataset you can think of, you can easily create, store and use it.</li> <li><code>BenchmarkSpecification</code>: The benchmark specification class wraps a <code>Dataset</code> with additional meta-data to produce a the benchmark. Specifically, it specifies how to evaluate a model's performance on the underlying dataset (e.g. the train-test split and metrics). It provides a simple API to run said evaluation protocol.</li> <li><code>Subset</code>: The subset class should be used as a starting-point for any framework-specific (e.g. PyTorch or Tensorflow) data loaders. To facilitate this, it abstracts away the non-trivial logic of accessing the data and provides several style of access to built upon.</li> <li><code>BenchmarkResults</code>: The benchmark results class stores the results of a benchmark, along with additional meta-data. This object can be easily uploaded to the Polaris Hub and shared with the broader community.</li> </ol>"},{"location":"quickstart.html#where-to-next","title":"Where to next?","text":"<p>Now that you've seen how easy it is to use Polaris, let's dive into the details through a set of tutorials!</p>"},{"location":"api/adapters.html","title":"Data Adapters","text":""},{"location":"api/adapters.html#polaris.dataset._adapters","title":"polaris.dataset._adapters","text":""},{"location":"api/adapters.html#polaris.dataset._adapters.Adapter","title":"Adapter","text":"<p>               Bases: <code>Enum</code></p> <p>Adapters are predefined callables that change the format of the data. Adapters are serializable and can thus be saved alongside datasets.</p> <p>Attributes:</p> Name Type Description <code>SMILES_TO_MOL</code> <p>Convert a SMILES string to a RDKit molecule.</p> <code>BYTES_TO_MOL</code> <p>Convert a RDKit binary string to a RDKit molecule.</p> <code>ARRAY_TO_PDB</code> <p>Convert a Zarr arrays to PDB arrays.</p>"},{"location":"api/base.html","title":"Base classes","text":""},{"location":"api/base.html#polaris._artifact.BaseArtifactModel","title":"polaris._artifact.BaseArtifactModel","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for all artifacts on the Hub. Specifies meta-data that is used by the Hub.</p> Optional <p>Despite all artifacts basing this class, note that all attributes are optional. This ensures the library can be used without the Polaris Hub. Only when uploading to the Hub, some of the attributes are required.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>SlugCompatibleStringType | None</code> <p>A slug-compatible name for the artifact. Together with the owner, this is used by the Hub to uniquely identify the artifact.</p> <code>description</code> <code>str</code> <p>A beginner-friendly, short description of the artifact.</p> <code>tags</code> <code>list[str]</code> <p>A list of tags to categorize the artifact by. This is used by the hub to search over artifacts.</p> <code>user_attributes</code> <code>dict[str, str]</code> <p>A dict with additional, textual user attributes.</p> <code>owner</code> <code>HubOwner | None</code> <p>A slug-compatible name for the owner of the artifact. If the artifact comes from the Polaris Hub, this is the associated owner (organization or user). Together with the name, this is used by the Hub to uniquely identify the artifact.</p> <code>polaris_version</code> <code>str</code> <p>The version of the Polaris library that was used to create the artifact.</p>"},{"location":"api/base.html#polaris._artifact.BaseArtifactModel.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(path: str) -&gt; Self\n</code></pre> <p>Loads an artifact from a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to a JSON file containing the artifact definition.</p> required"},{"location":"api/base.html#polaris._artifact.BaseArtifactModel.to_json","title":"to_json","text":"<pre><code>to_json(path: str) -&gt; None\n</code></pre> <p>Saves an artifact to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to save the artifact definition as JSON.</p> required"},{"location":"api/benchmark.html","title":"Base class","text":""},{"location":"api/benchmark.html#polaris.benchmark.BenchmarkSpecification","title":"polaris.benchmark.BenchmarkSpecification","text":"<p>               Bases: <code>PredictiveTaskSpecificationMixin</code>, <code>BaseArtifactModel</code>, <code>BaseSplitSpecificationMixin</code>, <code>ABC</code></p> <p>This class wraps a <code>Dataset</code> with additional data  to specify the evaluation logic.</p> <p>Specifically, it specifies:</p> <ol> <li>Which dataset to use (see <code>Dataset</code>);</li> <li>A task definition (we currently only support predictive tasks);</li> <li>A predefined, static train-test split to use during evaluation.</li> </ol> Subclasses <p>Polaris includes various subclasses of the <code>BenchmarkSpecification</code> that provide a more precise data-model or  additional logic, e.g. <code>SingleTaskBenchmarkSpecification</code>.</p> <p>Examples:</p> <p>Basic API usage: <pre><code>import polaris as po\n\n# Load the benchmark from the Hub\nbenchmark = po.load_benchmark(\"polaris/hello-world-benchmark\")\n\n# Get the train and test data-loaders\ntrain, test = benchmark.get_train_test_split()\n\n# Use the training data to train your model\n# Get the input as an array with 'train.inputs' and 'train.targets'\n# Or simply iterate over the train object.\nfor x, y in train:\n    ...\n\n# Work your magic to accurately predict the test set\npredictions = [0.0 for x in test]\n\n# Evaluate your predictions\nresults = benchmark.evaluate(predictions)\n\n# Submit your results\nresults.upload_to_hub(owner=\"dummy-user\")\n</code></pre></p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>BaseDataset</code> <p>The dataset the benchmark specification is based on.</p> <code>readme</code> <code>str</code> <p>Markdown text that can be used to provide a formatted description of the benchmark. If using the Polaris Hub, it is worth noting that this field is more easily edited through the Hub UI as it provides a rich text editor for writing markdown.</p> <p>For additional meta-data attributes, see the base classes.</p>"},{"location":"api/benchmark.html#polaris.benchmark.BenchmarkSpecification.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    y_pred: IncomingPredictionsType | None = None,\n    y_prob: IncomingPredictionsType | None = None,\n) -&gt; BenchmarkResults\n</code></pre> <p>Execute the evaluation protocol for the benchmark, given a set of predictions.</p> What about <code>y_true</code>? <p>Contrary to other frameworks that you might be familiar with, we opted for a signature that includes just the predictions. This reduces the chance of accidentally using the test targets during training.</p> <p>For this method, we make the following assumptions:</p> <ol> <li>There can be one or multiple test set(s);</li> <li>There can be one or multiple target(s);</li> <li>The metrics are constant across test sets;</li> <li>The metrics are constant across targets;</li> <li>There can be metrics which measure across tasks.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>y_pred</code> <code>IncomingPredictionsType | None</code> <p>The predictions for the test set, as NumPy arrays. If there are multiple targets, the predictions should be wrapped in a dictionary with the target labels as keys. If there are multiple test sets, the predictions should be further wrapped in a dictionary     with the test subset labels as keys.</p> <code>None</code> <code>y_prob</code> <code>IncomingPredictionsType | None</code> <p>The predicted probabilities for the test set, formatted similarly to predictions, based on the number of tasks and test sets.</p> <code>None</code> <p>Returns:</p> Type Description <code>BenchmarkResults</code> <p>A <code>BenchmarkResults</code> object. This object can be directly submitted to the Polaris Hub.</p> <p>Examples:</p> <ol> <li>For regression benchmarks:     pred_scores = your_model.predict_score(molecules) # predict continuous score values     benchmark.evaluate(y_pred=pred_scores)</li> <li>For classification benchmarks:<ul> <li>If <code>roc_auc</code> and <code>pr_auc</code> are in the metric list, both class probabilities and label predictions are required:     pred_probs = your_model.predict_proba(molecules) # predict probablities     pred_labels = your_model.predict_labels(molecules) # predict class labels     benchmark.evaluate(y_pred=pred_labels, y_prob=pred_probs)</li> <li>Otherwise:     benchmark.evaluate(y_pred=pred_labels)</li> </ul> </li> </ol>"},{"location":"api/benchmark.html#polaris.benchmark.BenchmarkSpecification.upload_to_hub","title":"upload_to_hub","text":"<pre><code>upload_to_hub(\n    settings: PolarisHubSettings | None = None,\n    cache_auth_token: bool = True,\n    access: AccessType = \"private\",\n    owner: HubOwner | str | None = None,\n    **kwargs: dict,\n)\n</code></pre> <p>Very light, convenient wrapper around the <code>PolarisHubClient.upload_benchmark</code> method.</p>"},{"location":"api/benchmark.html#polaris.benchmark.BenchmarkSpecification.to_json","title":"to_json","text":"<pre><code>to_json(destination: str) -&gt; str\n</code></pre> <p>Save the benchmark to a destination directory as a JSON file.</p> Multiple files <p>Perhaps unintuitive, this method creates multiple files in the destination directory as it also saves the dataset it is based on to the specified destination. See the docstring of <code>Dataset.to_json</code> for more information.</p> <p>Parameters:</p> Name Type Description Default <code>destination</code> <code>str</code> <p>The directory to save the associated data to.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The path to the JSON file.</p>"},{"location":"api/benchmark.html#subclasses","title":"Subclasses","text":""},{"location":"api/benchmark.html#polaris.benchmark.SingleTaskBenchmarkSpecification","title":"polaris.benchmark.SingleTaskBenchmarkSpecification","text":"<p>               Bases: <code>SingleTaskMixin</code>, <code>BenchmarkV1Specification</code></p> <p>Single-task benchmark for the base specification.</p>"},{"location":"api/benchmark.html#polaris.benchmark.MultiTaskBenchmarkSpecification","title":"polaris.benchmark.MultiTaskBenchmarkSpecification","text":"<p>               Bases: <code>MultiTaskMixin</code>, <code>BenchmarkV1Specification</code></p> <p>Multitask benchmark for the base specification.</p>"},{"location":"api/competition.evaluation.html","title":"Competition.evaluation","text":""},{"location":"api/competition.evaluation.html#polaris.evaluate.CompetitionPredictions","title":"polaris.evaluate.CompetitionPredictions","text":"<p>               Bases: <code>BenchmarkPredictions</code>, <code>ResultsMetadata</code></p> <p>Predictions for competition benchmarks.</p> <p>This object is to be used as input to <code>PolarisHubClient.submit_competition_predictions</code>. It is used to ensure that the structure of the predictions are compatible with evaluation methods on the Polaris Hub. In addition to the predictions, it contains meta-data that describes a predictions object.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>SlugCompatibleStringType</code> <p>A slug-compatible name for the artifact. It is redeclared here to be required.</p> <code>owner</code> <code>HubOwner</code> <p>A slug-compatible name for the owner of the artifact. It is redeclared here to be required.</p> <code>report_url</code> <code>HubOwner</code> <p>A URL to a report/paper/write-up which describes the methods used to generate the predictions.</p>"},{"location":"api/competition.html","title":"Competition","text":""},{"location":"api/competition.html#polaris.competition.CompetitionSpecification","title":"polaris.competition.CompetitionSpecification","text":"<p>               Bases: <code>DatasetV2</code>, <code>PredictiveTaskSpecificationMixin</code>, <code>SplitSpecificationV1Mixin</code></p> <p>An instance of this class represents a Polaris competition. It defines fields and functionality that in combination with the <code>DatasetV2</code> class, allow users to participate in competitions hosted on Polaris Hub.</p> <p>Examples:</p> <p>Basic API usage: <pre><code>import polaris as po\n\n# Load the benchmark from the Hub\ncompetition = po.load_competition(\"dummy-user/dummy-name\")\n\n# Get the train and test data-loaders\ntrain, test = competition.get_train_test_split()\n\n# Use the training data to train your model\n# Get the input as an array with 'train.inputs' and 'train.targets'\n# Or simply iterate over the train object.\nfor x, y in train:\n    ...\n\n# Work your magic to accurately predict the test set\nprediction_values = np.array([0.0 for x in test])\n\n# Submit your predictions\ncompetition.submit_predictions(\n    prediction_name=\"first-prediction\",\n    prediction_owner=\"dummy-user\",\n    report_url=\"REPORT_URL\",\n    predictions=prediction_values,\n)\n</code></pre></p> <p>Attributes:</p> Name Type Description <code>start_time</code> <code>datetime</code> <p>The time at which the competition starts accepting prediction submissions.</p> <code>end_time</code> <code>datetime</code> <p>The time at which the competition stops accepting prediction submissions.</p> <code>n_classes</code> <code>dict[ColumnName, int | None]</code> <p>The number of classes within each target column that defines a classification task.</p> <p>For additional meta-data attributes, see the base classes.</p>"},{"location":"api/competition.html#polaris.competition.CompetitionSpecification.get_train_test_split","title":"get_train_test_split","text":"<pre><code>get_train_test_split(\n    featurization_fn: Callable | None = None,\n) -&gt; tuple[Subset, Subset | dict[str, Subset]]\n</code></pre> <p>Construct the train and test sets, given the split in the competition specification.</p> <p>Returns <code>Subset</code> objects, which offer several ways of accessing the data and can thus easily serve as a basis to build framework-specific (e.g. PyTorch, Tensorflow) data-loaders on top of.</p> <p>Parameters:</p> Name Type Description Default <code>featurization_fn</code> <code>Callable | None</code> <p>A function to apply to the input data. If a multi-input benchmark, this function expects an input in the format specified by the <code>input_format</code> parameter.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Subset, Subset | dict[str, Subset]]</code> <p>A tuple with the train <code>Subset</code> and test <code>Subset</code> objects. If there are multiple test sets, these are returned in a dictionary and each test set has an associated name. The targets of the test set can not be accessed.</p>"},{"location":"api/competition.html#polaris.competition.CompetitionSpecification.submit_predictions","title":"submit_predictions","text":"<pre><code>submit_predictions(\n    predictions: IncomingPredictionsType,\n    prediction_name: SlugCompatibleStringType,\n    prediction_owner: str,\n    report_url: HttpUrlString,\n    contributors: list[HubUser] | None = None,\n    github_url: HttpUrlString | None = None,\n    description: str = \"\",\n    tags: list[str] | None = None,\n    user_attributes: dict[str, str] | None = None,\n) -&gt; None\n</code></pre> <p>Convenient wrapper around the <code>PolarisHubClient.submit_competition_predictions</code> method. It handles the creation of a standardized predictions object, which is expected by the Hub, automatically.</p> <p>Parameters:</p> Name Type Description Default <code>prediction_name</code> <code>SlugCompatibleStringType</code> <p>The name of the prediction.</p> required <code>prediction_owner</code> <code>str</code> <p>The slug of the user/organization which owns the prediction.</p> required <code>predictions</code> <code>IncomingPredictionsType</code> <p>The predictions for each test set defined in the competition.</p> required <code>report_url</code> <code>HttpUrlString</code> <p>A URL to a report/paper/write-up which describes the methods used to generate the predictions.</p> required <code>contributors</code> <code>list[HubUser] | None</code> <p>The users credited with generating these predictions.</p> <code>None</code> <code>github_url</code> <code>HttpUrlString | None</code> <p>An optional URL to a code repository containing the code used to generated these predictions.</p> <code>None</code> <code>description</code> <code>str</code> <p>An optional and short description of the predictions.</p> <code>''</code> <code>tags</code> <code>list[str] | None</code> <p>An optional list of tags to categorize the prediction by.</p> <code>None</code> <code>user_attributes</code> <code>dict[str, str] | None</code> <p>An optional dict with additional, textual user attributes.</p> <code>None</code>"},{"location":"api/converters.html","title":"Data Converters","text":""},{"location":"api/converters.html#polaris.dataset.converters.Converter","title":"polaris.dataset.converters.Converter","text":"<p>               Bases: <code>ABC</code></p>"},{"location":"api/converters.html#polaris.dataset.converters.Converter.convert","title":"convert  <code>abstractmethod</code>","text":"<pre><code>convert(path: str, append: bool = False) -&gt; FactoryProduct\n</code></pre> <p>This converts a file into a table and possibly annotations</p>"},{"location":"api/converters.html#polaris.dataset.converters.Converter.get_pointer","title":"get_pointer  <code>staticmethod</code>","text":"<pre><code>get_pointer(column: str, index: Union[int, slice]) -&gt; str\n</code></pre> <p>Creates a pointer.</p> <p>Parameters:</p> Name Type Description Default <code>column</code> <code>str</code> <p>The name of the column. Each column has its own group in the root.</p> required <code>index</code> <code>Union[int, slice]</code> <p>The index or slice of the pointer.</p> required"},{"location":"api/converters.html#polaris.dataset.converters.SDFConverter","title":"polaris.dataset.converters.SDFConverter","text":"<p>               Bases: <code>Converter</code></p> <p>Converts a SDF file into a Polaris dataset.</p> Binary strings for serialization <p>This class converts the molecules to binary strings (for ML purposes, this should be lossless). This might not be the most storage efficient, but is fastest and easiest to maintain. See this Github Discussion for more info.</p> <p>Properties defined on the molecule level in the SDF file can be extracted into separate columns or can be kept in the molecule object.</p> <p>Parameters:</p> Name Type Description Default <code>mol_column</code> <code>str</code> <p>The name of the column that will contain the pointers to the molecules.</p> <code>'molecule'</code> <code>smiles_column</code> <code>Optional[str]</code> <p>The name of the column that will contain the SMILES strings.</p> <code>'smiles'</code> <code>use_isomeric_smiles</code> <code>bool</code> <p>Whether to use isomeric SMILES.</p> <code>True</code> <code>mol_id_column</code> <code>Optional[str]</code> <p>The name of the column that will contain the molecule names.</p> <code>None</code> <code>mol_prop_as_cols</code> <code>bool</code> <p>Whether to extract properties defined on the molecule level in the SDF file into separate columns.</p> <code>True</code> <code>groupby_key</code> <code>Optional[str]</code> <p>The name of the column to group by. If set, the dataset can combine multiple pointers to the molecules into a single datapoint.</p> <code>None</code>"},{"location":"api/converters.html#polaris.dataset.converters.ZarrConverter","title":"polaris.dataset.converters.ZarrConverter","text":"<p>               Bases: <code>Converter</code></p> <p>Parse a .zarr archive into a Polaris <code>Dataset</code>.</p> Tutorial <p>To learn more about the zarr format, see the tutorial.</p> Loading from <code>.zarr</code> <p>Loading and saving datasets from and to <code>.zarr</code> is still experimental and currently not fully supported by the Hub.</p> <p>A <code>.zarr</code> file can contain groups and arrays, where each group can again contain groups and arrays. Within Polaris, the Zarr archive is expected to have a flat hierarchy where each array corresponds to a single column and each array contains the values for all datapoints in that column.</p>"},{"location":"api/converters.html#polaris.dataset.converters.PDBConverter","title":"polaris.dataset.converters.PDBConverter","text":"<p>               Bases: <code>Converter</code></p> <p>Converts PDB files into a Polaris dataset based on fastpdb.</p> Only the most essential structural information of a protein is retained <p>This conversion saves the 3D coordinates, chain ID, residue ID, insertion code, residue name, heteroatom indicator, atom name, element, atom ID, B-factor, occupancy, and charge. Records such as CONECT (connectivity information), ANISOU (anisotropic Temperature Factors), HETATM (heteroatoms and ligands) are handled by <code>fastpdb</code>. We believe this makes for a good ML-ready format, but let us know if you require any other information to be saved.</p> PDBs as ND-arrays using <code>biotite</code> <p>To save PDBs in a Polaris-compatible format, we convert them to ND-arrays using <code>fastpdb</code> and <code>biotite</code>. We then save these ND-arrays to Zarr archives. For more info, see fastpdb and biotite</p> <p>Parameters:</p> Name Type Description Default <code>pdb_column</code> <code>str</code> <p>The name of the column that will contain the pointers to the pdbs.</p> <code>'pdb'</code> <code>n_jobs</code> <code>int</code> <p>The number of jobs to run in parallel.</p> <code>1</code> <code>zarr_chunks</code> <code>Sequence[Optional[int]]</code> <p>The chunk size for the Zarr arrays.</p> <code>(1,)</code>"},{"location":"api/converters.html#polaris.dataset.converters.PDBConverter.convert","title":"convert","text":"<pre><code>convert(path, factory: DatasetFactory, append: bool = False) -&gt; FactoryProduct\n</code></pre> <p>Convert one or a list of PDB files into Zarr</p>"},{"location":"api/dataset.html","title":"Dataset","text":""},{"location":"api/dataset.html#polaris.dataset.Dataset","title":"polaris.dataset.Dataset","text":"<p>               Bases: <code>BaseDataset</code>, <code>ChecksumMixin</code></p> <p>First version of a Polaris Dataset.</p> <p>Stores datapoints in a Pandas DataFrame and implements pointer columns to support the storage of XL data outside the DataFrame in a Zarr archive.</p> Pointer columns <p>For complex data, such as images, we support storing the content in external blobs of data. In that case, the table contains pointers to these blobs that are dynamically loaded when needed.</p> <p>Attributes:</p> Name Type Description <code>table</code> <code>DataFrame</code> <p>The core data-structure, storing data-points in a row-wise manner. Can be specified as either a path to a <code>.parquet</code> file or a <code>pandas.DataFrame</code>.</p> <p>For additional meta-data attributes, see the base classes.</p> <p>Raises:</p> Type Description <code>InvalidDatasetError</code> <p>If the dataset does not conform to the Pydantic data-model specification.</p>"},{"location":"api/dataset.html#polaris.dataset.Dataset.zarr_md5sum_manifest","title":"zarr_md5sum_manifest  <code>property</code>","text":"<pre><code>zarr_md5sum_manifest: List[ZarrFileChecksum]\n</code></pre> <p>The Zarr Checksum manifest stores the checksums of all files in a Zarr archive. If the dataset doesn't use Zarr, this will simply return an empty list.</p>"},{"location":"api/dataset.html#polaris.dataset.Dataset.rows","title":"rows  <code>property</code>","text":"<pre><code>rows: list[str | int]\n</code></pre> <p>Return all row indices for the dataset</p>"},{"location":"api/dataset.html#polaris.dataset.Dataset.columns","title":"columns  <code>property</code>","text":"<pre><code>columns: list[str]\n</code></pre> <p>Return all columns for the dataset</p>"},{"location":"api/dataset.html#polaris.dataset.Dataset.dtypes","title":"dtypes  <code>property</code>","text":"<pre><code>dtypes: dict[str, dtype]\n</code></pre> <p>Return the dtype for each of the columns for the dataset</p>"},{"location":"api/dataset.html#polaris.dataset.Dataset.load_zarr_root_from_hub","title":"load_zarr_root_from_hub","text":"<pre><code>load_zarr_root_from_hub()\n</code></pre> <p>Loads a Zarr archive from the Hub.</p>"},{"location":"api/dataset.html#polaris.dataset.Dataset.get_data","title":"get_data","text":"<pre><code>get_data(\n    row: str | int, col: str, adapters: dict[str, Adapter] | None = None\n) -&gt; np.ndarray | Any\n</code></pre> <p>Since the dataset might contain pointers to external files, data retrieval is more complicated than just indexing the <code>table</code> attribute. This method provides an end-point for seamlessly accessing the underlying data.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>str | int</code> <p>The row index in the <code>Dataset.table</code> attribute</p> required <code>col</code> <code>str</code> <p>The column index in the <code>Dataset.table</code> attribute</p> required <code>adapters</code> <code>dict[str, Adapter] | None</code> <p>The adapters to apply to the data before returning it. If None, will use the default adapters specified for the dataset.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray | Any</code> <p>A numpy array with the data at the specified indices. If the column is a pointer column, the content of the referenced file is loaded to memory.</p>"},{"location":"api/dataset.html#polaris.dataset.Dataset.upload_to_hub","title":"upload_to_hub","text":"<pre><code>upload_to_hub(\n    access: AccessType = \"private\", owner: HubOwner | str | None = None\n)\n</code></pre> <p>Very light, convenient wrapper around the <code>PolarisHubClient.upload_dataset</code> method.</p>"},{"location":"api/dataset.html#polaris.dataset.Dataset.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(path: str)\n</code></pre> <p>Loads a dataset from a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the JSON file to load the dataset from .ColumnAnnotation</p> required"},{"location":"api/dataset.html#polaris.dataset.Dataset.to_json","title":"to_json","text":"<pre><code>to_json(\n    destination: str | Path, if_exists: ZarrConflictResolution = \"replace\"\n) -&gt; str\n</code></pre> <p>Save the dataset to a destination directory as a JSON file.</p> Multiple files <p>Perhaps unintuitive, this method creates multiple files.</p> <ol> <li><code>/path/to/destination/[dataset.slug].json</code>: This file can be loaded with <code>Dataset.from_json</code>.</li> <li><code>/path/to/destination/table.parquet</code>: The <code>Dataset.table</code> attribute is saved here.</li> <li>(Optional) <code>/path/to/destination/[dataset.zarr_root]</code>: Any additional blobs of data referenced by the         pointer columns will be stored here.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>destination</code> <code>str | Path</code> <p>The directory to save the associated data to.</p> required <code>if_exists</code> <code>ZarrConflictResolution</code> <p>Action for handling existing files in the Zarr archive. Options are 'raise' to throw an error, 'replace' to overwrite, or 'skip' to proceed without altering the existing files.</p> <code>'replace'</code> <p>Returns:</p> Type Description <code>str</code> <p>The path to the JSON file.</p>"},{"location":"api/dataset.html#polaris.dataset.Dataset.cache","title":"cache","text":"<pre><code>cache(\n    destination: str | PathLike | None = None,\n    if_exists: ZarrConflictResolution = \"replace\",\n    verify_checksum: bool = True,\n) -&gt; str\n</code></pre> <p>Caches the dataset by downloading all additional data for pointer columns to a local directory.</p> <p>Parameters:</p> Name Type Description Default <code>destination</code> <code>str | PathLike | None</code> <p>The directory to cache the data to. If None, will use the default cache directory.</p> <code>None</code> <code>if_exists</code> <code>ZarrConflictResolution</code> <p>Action for handling existing files at the destination. Options are 'raise' to throw an error, 'replace' to overwrite, or 'skip' to proceed without altering the existing files.</p> <code>'replace'</code> <code>verify_checksum</code> <code>bool</code> <p>Whether to verify the checksum of the dataset after caching.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>The path to the directory where data has been cached to.</p>"},{"location":"api/dataset.html#polaris.dataset.Dataset.should_verify_checksum","title":"should_verify_checksum","text":"<pre><code>should_verify_checksum(strategy: ChecksumStrategy) -&gt; bool\n</code></pre> <p>Determines whether to verify the checksum of the dataset based on the strategy.</p>"},{"location":"api/dataset.html#polaris.dataset.DatasetV2","title":"polaris.dataset.DatasetV2","text":"<p>               Bases: <code>BaseDataset</code></p> <p>Second version of a Polaris Dataset.</p> <p>This version gets rid of the DataFrame and stores all data in a Zarr archive.</p> <p>V1 stored all datapoints in a Pandas DataFrame. Because a DataFrame is always loaded to memory, this was a bottleneck when the number of data points grew large. Even with the pointer columns, you still need to load all pointers into memory. V2 therefore switches to a Zarr-only format.</p> <p>Attributes:</p> Name Type Description <code>zarr_root_path</code> <code>str</code> <p>Required path to a Zarr archive.</p> <p>For additional meta-data attributes, see the base classes.</p> <p>Raises:</p> Type Description <code>InvalidDatasetError</code> <p>If the dataset does not conform to the Pydantic data-model specification.</p>"},{"location":"api/dataset.html#polaris.dataset.DatasetV2.n_rows","title":"n_rows  <code>property</code>","text":"<pre><code>n_rows: int\n</code></pre> <p>Return all row indices for the dataset</p>"},{"location":"api/dataset.html#polaris.dataset.DatasetV2.rows","title":"rows  <code>property</code>","text":"<pre><code>rows: ndarray[int]\n</code></pre> <p>Return all row indices for the dataset</p> Memory consumption <p>This feature is added for completeness' sake, but it should be noted that large datasets could consume a lot of memory. E.g. storing a billion indices with np.in64 would consume 8GB of memory. Use with caution.</p>"},{"location":"api/dataset.html#polaris.dataset.DatasetV2.columns","title":"columns  <code>property</code>","text":"<pre><code>columns: list[str]\n</code></pre> <p>Return all columns for the dataset</p>"},{"location":"api/dataset.html#polaris.dataset.DatasetV2.dtypes","title":"dtypes  <code>property</code>","text":"<pre><code>dtypes: dict[str, dtype]\n</code></pre> <p>Return the dtype for each of the columns for the dataset</p>"},{"location":"api/dataset.html#polaris.dataset.DatasetV2.zarr_manifest_md5sum","title":"zarr_manifest_md5sum  <code>property</code> <code>writable</code>","text":"<pre><code>zarr_manifest_md5sum: str\n</code></pre> <p>Lazily compute the checksum once needed.</p> <p>The checksum of the DatasetV2 is computed from the Zarr Manifest and is not deterministic.</p>"},{"location":"api/dataset.html#polaris.dataset.DatasetV2.has_zarr_manifest_md5sum","title":"has_zarr_manifest_md5sum  <code>property</code>","text":"<pre><code>has_zarr_manifest_md5sum: bool\n</code></pre> <p>Whether the md5sum for this dataset's zarr manifest has been computed and stored.</p>"},{"location":"api/dataset.html#polaris.dataset.DatasetV2.load_zarr_root_from_hub","title":"load_zarr_root_from_hub","text":"<pre><code>load_zarr_root_from_hub()\n</code></pre> <p>Loads a Zarr archive from the Hub.</p>"},{"location":"api/dataset.html#polaris.dataset.DatasetV2.get_data","title":"get_data","text":"<pre><code>get_data(\n    row: int, col: str, adapters: dict[str, Adapter] | None = None\n) -&gt; np.ndarray | Any\n</code></pre> <p>Indexes the Zarr archive.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>int</code> <p>The index of the data to fetch.</p> required <code>col</code> <code>str</code> <p>The label of a direct child of the Zarr root.</p> required <code>adapters</code> <code>dict[str, Adapter] | None</code> <p>The adapters to apply to the data before returning it. If None, will use the default adapters specified for the dataset.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray | Any</code> <p>A numpy array with the data at the specified indices. If the column is a pointer column, the content of the referenced file is loaded to memory.</p>"},{"location":"api/dataset.html#polaris.dataset.DatasetV2.upload_to_hub","title":"upload_to_hub","text":"<pre><code>upload_to_hub(\n    access: AccessType = \"private\", owner: HubOwner | str | None = None\n)\n</code></pre> <p>Uploads the dataset to the Polaris Hub.</p>"},{"location":"api/dataset.html#polaris.dataset.DatasetV2.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(path: str)\n</code></pre> <p>Loads a dataset from a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the JSON file to load the dataset from.</p> required"},{"location":"api/dataset.html#polaris.dataset.DatasetV2.to_json","title":"to_json","text":"<pre><code>to_json(\n    destination: str | Path, if_exists: ZarrConflictResolution = \"replace\"\n) -&gt; str\n</code></pre> <p>Save the dataset to a destination directory as a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>destination</code> <code>str | Path</code> <p>The directory to save the associated data to.</p> required <code>if_exists</code> <code>ZarrConflictResolution</code> <p>Action for handling existing files in the Zarr archive. Options are 'raise' to throw an error, 'replace' to overwrite, or 'skip' to proceed without altering the existing files.</p> <code>'replace'</code> <p>Returns:</p> Type Description <code>str</code> <p>The path to the JSON file.</p>"},{"location":"api/dataset.html#polaris.dataset.DatasetV2.cache","title":"cache","text":"<pre><code>cache(\n    destination: str | PathLike | None = None,\n    if_exists: ZarrConflictResolution = \"replace\",\n) -&gt; str\n</code></pre> <p>Caches the dataset by downloading the Zarr archive to a local directory.</p> <p>Parameters:</p> Name Type Description Default <code>destination</code> <code>str | PathLike | None</code> <p>The directory to cache the data to. If None, will use the default cache directory.</p> <code>None</code> <code>if_exists</code> <code>ZarrConflictResolution</code> <p>Action for handling existing files at the destination. Options are 'raise' to throw an error, 'replace' to overwrite, or 'skip' to proceed without altering the existing files.</p> <code>'replace'</code> <p>Returns:</p> Type Description <code>str</code> <p>The path to the directory where data has been cached to.</p>"},{"location":"api/dataset.html#polaris.dataset.DatasetV2.should_verify_checksum","title":"should_verify_checksum","text":"<pre><code>should_verify_checksum(strategy: ChecksumStrategy) -&gt; bool\n</code></pre> <p>Determines whether to verify the checksum of the dataset based on the strategy.</p>"},{"location":"api/dataset.html#polaris.dataset._base.BaseDataset","title":"polaris.dataset._base.BaseDataset","text":"<p>               Bases: <code>BaseArtifactModel</code>, <code>ABC</code></p> <p>Base data-model for a Polaris dataset, implemented as a Pydantic model.</p> <p>At its core, a dataset in Polaris can conceptually be thought of as tabular data structure that stores data-points in a row-wise manner, where each column correspond to a variable associated with that datapoint.</p> <p>A Dataset can have multiple modalities or targets, can be sparse and can be part of one or multiple  <code>BenchmarkSpecification</code> objects.</p> <p>Attributes:</p> Name Type Description <code>default_adapters</code> <code>dict[str, Adapter]</code> <p>The adapters that the Dataset recommends to use by default to change the format of the data for specific columns.</p> <code>zarr_root_path</code> <code>str | None</code> <p>The data for any pointer column should be saved in the Zarr archive this path points to.</p> <code>readme</code> <code>str</code> <p>Markdown text that can be used to provide a formatted description of the dataset. If using the Polaris Hub, it is worth noting that this field is more easily edited through the Hub UI as it provides a rich text editor for writing markdown.</p> <code>annotations</code> <code>dict[str, ColumnAnnotation]</code> <p>Each column can be annotated with a <code>ColumnAnnotation</code> object. Importantly, this is used to annotate whether a column is a pointer column.</p> <code>source</code> <code>HttpUrlString | None</code> <p>The data source, e.g. a DOI, Github repo or URI.</p> <code>license</code> <code>SupportedLicenseType | None</code> <p>The dataset license. Polaris only supports some Creative Commons licenses. See <code>SupportedLicenseType</code> for accepted ID values.</p> <code>curation_reference</code> <code>HttpUrlString | None</code> <p>A reference to the curation process, e.g. a DOI, Github repo or URI.</p> <p>For additional meta-data attributes, see the base classes.</p> <p>Raises:</p> Type Description <code>InvalidDatasetError</code> <p>If the dataset does not conform to the Pydantic data-model specification.</p>"},{"location":"api/dataset.html#polaris.dataset._base.BaseDataset.uses_zarr","title":"uses_zarr  <code>property</code>","text":"<pre><code>uses_zarr: bool\n</code></pre> <p>Whether any of the data in this dataset is stored in a Zarr Archive.</p>"},{"location":"api/dataset.html#polaris.dataset._base.BaseDataset.zarr_data","title":"zarr_data  <code>property</code>","text":"<pre><code>zarr_data\n</code></pre> <p>Get the Zarr data.</p> <p>This is different from the Zarr Root, because to optimize the efficiency of data loading, a user can choose to load the data into memory as a numpy array</p> General purpose dataloader. <p>The goal with Polaris is to provide general purpose datasets that serve as good options for a wide variety of use cases. This also implies you should be able to optimize things further for a specific use case if needed.</p>"},{"location":"api/dataset.html#polaris.dataset._base.BaseDataset.zarr_root","title":"zarr_root  <code>property</code>","text":"<pre><code>zarr_root: Group | None\n</code></pre> <p>Get the zarr Group object corresponding to the root.</p> <p>Opens the zarr archive in read-write mode if it is not already open.</p> Different to <code>zarr_data</code> <p>The <code>zarr_data</code> attribute references either to the Zarr archive or to a in-memory copy of the data. See also <code>Dataset.load_to_memory</code>.</p>"},{"location":"api/dataset.html#polaris.dataset._base.BaseDataset.n_rows","title":"n_rows  <code>property</code>","text":"<pre><code>n_rows: int\n</code></pre> <p>The number of rows in the dataset.</p>"},{"location":"api/dataset.html#polaris.dataset._base.BaseDataset.n_columns","title":"n_columns  <code>property</code>","text":"<pre><code>n_columns: int\n</code></pre> <p>The number of columns in the dataset.</p>"},{"location":"api/dataset.html#polaris.dataset._base.BaseDataset.rows","title":"rows  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>rows: list[str | int]\n</code></pre> <p>Return all row indices for the dataset</p>"},{"location":"api/dataset.html#polaris.dataset._base.BaseDataset.columns","title":"columns  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>columns: list[str]\n</code></pre> <p>Return all columns for the dataset</p>"},{"location":"api/dataset.html#polaris.dataset._base.BaseDataset.dtypes","title":"dtypes  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>dtypes: dict[str, dtype]\n</code></pre> <p>Return the dtype for each of the columns for the dataset</p>"},{"location":"api/dataset.html#polaris.dataset._base.BaseDataset.load_zarr_root_from_hub","title":"load_zarr_root_from_hub  <code>abstractmethod</code>","text":"<pre><code>load_zarr_root_from_hub()\n</code></pre> <p>Loads a Zarr archive from the Polaris Hub.</p>"},{"location":"api/dataset.html#polaris.dataset._base.BaseDataset.load_zarr_root_from_local","title":"load_zarr_root_from_local","text":"<pre><code>load_zarr_root_from_local()\n</code></pre> <p>Loads a locally stored Zarr archive.</p> <p>We use memory mapping by default because our experiments show that it's consistently faster</p>"},{"location":"api/dataset.html#polaris.dataset._base.BaseDataset.load_to_memory","title":"load_to_memory","text":"<pre><code>load_to_memory()\n</code></pre> <p>Load data from zarr files to memeory</p> Make sure the uncompressed dataset fits in-memory. <p>This method will load the uncompressed dataset into memory. Make sure you actually have enough memory to store the dataset.</p>"},{"location":"api/dataset.html#polaris.dataset._base.BaseDataset.get_data","title":"get_data  <code>abstractmethod</code>","text":"<pre><code>get_data(\n    row: str | int, col: str, adapters: dict[str, Adapter] | None = None\n) -&gt; np.ndarray | Any\n</code></pre> <p>Since the dataset might contain pointers to external files, data retrieval is more complicated than just indexing the <code>table</code> attribute. This method provides an end-point for seamlessly accessing the underlying data.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>str | int</code> <p>The row index in the <code>Dataset.table</code> attribute</p> required <code>col</code> <code>str</code> <p>The column index in the <code>Dataset.table</code> attribute</p> required <code>adapters</code> <code>dict[str, Adapter] | None</code> <p>The adapters to apply to the data before returning it. If None, will use the default adapters specified for the dataset.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray | Any</code> <p>A numpy array with the data at the specified indices. If the column is a pointer column, the content of the referenced file is loaded to memory.</p>"},{"location":"api/dataset.html#polaris.dataset._base.BaseDataset.upload_to_hub","title":"upload_to_hub  <code>abstractmethod</code>","text":"<pre><code>upload_to_hub(\n    access: AccessType = \"private\", owner: HubOwner | str | None = None\n)\n</code></pre> <p>Uploads the dataset to the Polaris Hub.</p>"},{"location":"api/dataset.html#polaris.dataset._base.BaseDataset.from_json","title":"from_json  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>from_json(path: str)\n</code></pre> <p>Loads a dataset from a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the JSON file to load the dataset from.</p> required"},{"location":"api/dataset.html#polaris.dataset._base.BaseDataset.to_json","title":"to_json  <code>abstractmethod</code>","text":"<pre><code>to_json(\n    destination: str | Path, if_exists: ZarrConflictResolution = \"replace\"\n) -&gt; str\n</code></pre> <p>Save the dataset to a destination directory as a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>destination</code> <code>str | Path</code> <p>The directory to save the associated data to.</p> required <code>if_exists</code> <code>ZarrConflictResolution</code> <p>Action for handling existing files in the Zarr archive. Options are 'raise' to throw an error, 'replace' to overwrite, or 'skip' to proceed without altering the existing files.</p> <code>'replace'</code> <p>Returns:</p> Type Description <code>str</code> <p>The path to the JSON file.</p>"},{"location":"api/dataset.html#polaris.dataset.ColumnAnnotation","title":"polaris.dataset.ColumnAnnotation","text":"<p>               Bases: <code>BaseModel</code></p> <p>The <code>ColumnAnnotation</code> class is used to annotate the columns of the <code>Dataset</code> object. This mostly just stores meta-data and does not affect the logic. The exception is the <code>is_pointer</code> attribute.</p> <p>Attributes:</p> Name Type Description <code>is_pointer</code> <code>bool</code> <p>Annotates whether a column is a pointer column. If so, it does not contain data, but rather contains references to blobs of data from which the data is loaded.</p> <code>modality</code> <code>Modality</code> <p>The data modality describes the data type and is used to categorize datasets on the hub and while it does not affect logic in this library, it does affect the logic of the hub.</p> <code>description</code> <code>str | None</code> <p>Describes how the data was generated.</p> <code>user_attributes</code> <code>dict[str, str]</code> <p>Any additional meta-data can be stored in the user attributes.</p> <code>content_type</code> <code>KnownContentType | str | None</code> <p>Specify column's IANA content type. If the the content type matches with a known type for molecules (e.g. \"chemical/x-smiles\"), visualization for its content will be activated on the Hub side</p>"},{"location":"api/dataset.html#polaris.dataset.zarr","title":"polaris.dataset.zarr","text":""},{"location":"api/dataset.html#polaris.dataset.zarr.ZarrFileChecksum","title":"ZarrFileChecksum","text":"<p>               Bases: <code>BaseModel</code></p> <p>This data is sent to the Hub to verify the integrity of the Zarr archive on upload.</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>str</code> <p>The path of the file relative to the Zarr root.</p> <code>md5sum</code> <code>str</code> <p>The md5sum of the file.</p> <code>size</code> <code>int</code> <p>The size of the file in bytes.</p>"},{"location":"api/dataset.html#polaris.dataset.zarr.MemoryMappedDirectoryStore","title":"MemoryMappedDirectoryStore","text":"<p>               Bases: <code>DirectoryStore</code></p> <p>A Zarr Store to open chunks as memory-mapped files. See also this Github issue.</p> <p>Memory mapping leverages low-level OS functionality to reduce the time it takes to read the content of a file by directly mapping to memory.</p>"},{"location":"api/dataset.html#polaris.dataset.zarr.compute_zarr_checksum","title":"compute_zarr_checksum","text":"<pre><code>compute_zarr_checksum(\n    zarr_root_path: str,\n) -&gt; Tuple[_ZarrDirectoryDigest, List[ZarrFileChecksum]]\n</code></pre> <p>Implements an algorithm to compute the Zarr checksum.</p> This checksum is sensitive to Zarr configuration. <p>This checksum is sensitive to change in the Zarr structure. For example, if you change the chunk size,  the checksum will also change.</p> <p>To understand how this works, consider the following directory structure:</p> <pre><code>       . (root)\n      / \\\n     a   c\n    /\n   b\n</code></pre> <p>Within zarr, this would for example be:</p> <ul> <li><code>root</code>: A Zarr Group with a single Array.</li> <li><code>a</code>: A Zarr Array</li> <li><code>b</code>: A single chunk of the Zarr Array</li> <li><code>c</code>: A metadata file (i.e. .zarray, .zattrs or .zgroup) </li> </ul> <p>To compute the checksum, we first find all the trees in the node, in this case b and c.  We compute the hash of the content (the raw bytes) for each of these files.</p> <p>We then work our way up the tree. For any node (directory), we find all children of that node. In an sorted order, we then serialize a list with - for each of the children - the checksum, size, and number of children. The hash of the directory is then equal to the hash of the serialized JSON.</p> <p>The Polaris implementation is heavily based on the <code>zarr-checksum</code> package. This method is the biggest deviation of the original code.</p>"},{"location":"api/dataset.html#polaris.dataset.zarr.generate_zarr_manifest","title":"generate_zarr_manifest","text":"<pre><code>generate_zarr_manifest(zarr_root_path: str, output_dir: str) -&gt; str\n</code></pre> <p>Entry point function which triggers the creation of a Zarr manifest for a V2 dataset.</p> <p>Parameters:</p> Name Type Description Default <code>zarr_root_path</code> <code>str</code> <p>The path to the root of a Zarr archive</p> required <code>output_dir</code> <code>str</code> <p>The path to the directory which will hold the generated manifest</p> required"},{"location":"api/evaluation.html","title":"Evaluation","text":""},{"location":"api/evaluation.html#polaris.evaluate.BenchmarkPredictions","title":"polaris.evaluate.BenchmarkPredictions","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base model to represent predictions in the Polaris code base.</p> <p>Guided by Postel's Law, this class normalizes different formats to a single, internal representation.</p> <p>Attributes:</p> Name Type Description <code>predictions</code> <code>PredictionsType</code> <p>The predictions for the benchmark.</p> <code>target_labels</code> <code>list[str]</code> <p>The target columns for the associated benchmark.</p> <code>test_set_labels</code> <code>list[str]</code> <p>The names of the test sets for the associated benchmark.</p> <code>test_set_sizes</code> <code>dict[str, int]</code> <p>The number of rows in each test set for the associated benchmark.</p>"},{"location":"api/evaluation.html#polaris.evaluate.BenchmarkPredictions.check_test_set_size","title":"check_test_set_size","text":"<pre><code>check_test_set_size() -&gt; Self\n</code></pre> <p>Verify that the size of all predictions</p>"},{"location":"api/evaluation.html#polaris.evaluate.BenchmarkPredictions.get_subset","title":"get_subset","text":"<pre><code>get_subset(\n    test_set_subset: list[str] | None = None,\n    target_subset: list[str] | None = None,\n) -&gt; BenchmarkPredictions\n</code></pre> <p>Return a subset of the original predictions</p>"},{"location":"api/evaluation.html#polaris.evaluate.BenchmarkPredictions.get_size","title":"get_size","text":"<pre><code>get_size(\n    test_set_subset: list[str] | None = None,\n    target_subset: list[str] | None = None,\n) -&gt; int\n</code></pre> <p>Return the total number of predictions, allowing for filtering by test set and target</p>"},{"location":"api/evaluation.html#polaris.evaluate.BenchmarkPredictions.flatten","title":"flatten","text":"<pre><code>flatten() -&gt; np.ndarray\n</code></pre> <p>Return the predictions as a single, flat numpy array</p>"},{"location":"api/evaluation.html#polaris.evaluate.BenchmarkPredictions.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Return the total number of predictions</p>"},{"location":"api/evaluation.html#polaris.evaluate.ResultsMetadata","title":"polaris.evaluate.ResultsMetadata","text":"<p>               Bases: <code>BaseArtifactModel</code></p> <p>Base class for evaluation results</p> <p>Attributes:</p> Name Type Description <code>github_url</code> <code>HttpUrlString | None</code> <p>The URL to the code repository that was used to generate these results.</p> <code>paper_url</code> <code>HttpUrlString | None</code> <p>The URL to the paper describing the methodology used to generate these results.</p> <code>contributors</code> <code>list[HubUser]</code> <p>The users that are credited for these results.</p> <p>For additional meta-data attributes, see the base classes.</p>"},{"location":"api/evaluation.html#polaris.evaluate.EvaluationResult","title":"polaris.evaluate.EvaluationResult","text":"<p>               Bases: <code>ResultsMetadata</code></p> <p>Class for saving evaluation results</p> <p>The actual results are saved in the <code>results</code> field using the following tabular format:</p> Test set Target label Metric Score test_iid EGFR_WT AUC 0.9 test_ood EGFR_WT AUC 0.75 ... ... ... ... test_ood EGFR_L858R AUC 0.79 Categorizing methods <p>An open question is how to best categorize a methodology (e.g. a model). This is needed since we would like to be able to aggregate results across benchmarks too, to say something about which (type of) methods performs best in general.</p> <p>Attributes:</p> Name Type Description <code>results</code> <code>DataFrame</code> <p>Evaluation results are stored directly in a dataframe or in a serialized, JSON compatible dict that can be decoded into the associated tabular format.</p> <p>For additional meta-data attributes, see the base classes.</p>"},{"location":"api/evaluation.html#polaris.evaluate.BenchmarkResults","title":"polaris.evaluate.BenchmarkResults","text":"<p>               Bases: <code>EvaluationResult</code></p> <p>Class specific to results for standard benchmarks.</p> <p>This object is returned by <code>BenchmarkSpecification.evaluate</code>. In addition to the metrics on the test set, it contains additional meta-data and logic to integrate the results with the Polaris Hub.</p> The name of the benchmark for which these results were generated. <p>Together with the benchmark owner, this uniquely identifies the benchmark on the Hub.</p> <p>benchmark_owner: The owner of the benchmark for which these results were generated.     Together with the benchmark name, this uniquely identifies the benchmark on the Hub.</p>"},{"location":"api/evaluation.html#polaris.evaluate.BenchmarkResults.upload_to_hub","title":"upload_to_hub","text":"<pre><code>upload_to_hub(\n    access: AccessType = \"private\",\n    owner: HubOwner | str | None = None,\n    **kwargs: dict,\n) -&gt; BenchmarkResults\n</code></pre> <p>Very light, convenient wrapper around the <code>PolarisHubClient.upload_results</code> method.</p>"},{"location":"api/evaluation.html#polaris.evaluate.MetricInfo","title":"polaris.evaluate.MetricInfo","text":"<p>               Bases: <code>BaseModel</code></p> <p>Metric metadata</p> <p>Attributes:</p> Name Type Description <code>fn</code> <code>Callable</code> <p>The callable that actually computes the metric.</p> <code>is_multitask</code> <code>bool</code> <p>Whether the metric expects a single set of predictions or a dict of predictions.</p> <code>kwargs</code> <code>dict</code> <p>Additional parameters required for the metric.</p> <code>direction</code> <code>DirectionType</code> <p>The direction for ranking of the metric,  \"max\" for maximization and \"min\" for minimization.</p> <code>y_type</code> <code>PredictionKwargs</code> <p>The type of predictions expected by the metric interface.</p>"},{"location":"api/evaluation.html#polaris.evaluate.Metric","title":"polaris.evaluate.Metric","text":"<p>               Bases: <code>BaseModel</code></p> <p>A Metric in Polaris.</p> <p>A metric consists of a default metric, which is a callable labeled with additional metadata, as well as a config. The config can change how the metric is computed, for example by grouping the data before computing the metric.</p> <p>Attributes:</p> Name Type Description <code>label</code> <code>MetricLabel</code> <p>The actual callable that is at the core of the metric implementation.</p> <code>custom_name</code> <code>str | None</code> <p>A optional, custom name of the metric. Names should be unique within the context of a benchmark.</p> <code>config</code> <code>GroupedMetricConfig | None</code> <p>For more complex metrics, this object should hold all parameters for the metric.</p> <code>fn</code> <code>Callable</code> <p>The callable that actually computes the metric, automatically set based on the label.</p> <code>is_multitask</code> <code>bool</code> <p>Whether the metric expects a single set of predictions or a dict of predictions, automatically set based on the label.</p> <code>kwargs</code> <code>dict</code> <p>Additional parameters required for the metric, automatically set based on the label.</p> <code>direction</code> <code>DirectionType</code> <p>The direction for ranking of the metric,  \"max\" for maximization and \"min\" for minimization, automatically set based on the label.</p> <code>y_type</code> <code>PredictionKwargs</code> <p>The type of predictions expected by the metric interface, automatically set based on the label.</p>"},{"location":"api/evaluation.html#polaris.evaluate.Metric.score","title":"score","text":"<pre><code>score(\n    y_true: GroundTruth,\n    y_pred: BenchmarkPredictions | None = None,\n    y_prob: BenchmarkPredictions | None = None,\n) -&gt; float\n</code></pre> <p>Compute the metric.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>GroundTruth</code> <p>The true target values.</p> required <code>y_pred</code> <code>BenchmarkPredictions | None</code> <p>The predicted target values, if any.</p> <code>None</code> <code>y_prob</code> <code>BenchmarkPredictions | None</code> <p>The predicted target probabilities, if any.</p> <code>None</code>"},{"location":"api/evaluation.html#polaris.evaluate.metrics.generic_metrics","title":"polaris.evaluate.metrics.generic_metrics","text":""},{"location":"api/evaluation.html#polaris.evaluate.metrics.generic_metrics.pearsonr","title":"pearsonr","text":"<pre><code>pearsonr(y_true: np.ndarray, y_pred: np.ndarray)\n</code></pre> <p>Calculate a pearson r correlation</p>"},{"location":"api/evaluation.html#polaris.evaluate.metrics.generic_metrics.spearman","title":"spearman","text":"<pre><code>spearman(y_true: np.ndarray, y_pred: np.ndarray)\n</code></pre> <p>Calculate a Spearman correlation</p>"},{"location":"api/evaluation.html#polaris.evaluate.metrics.generic_metrics.absolute_average_fold_error","title":"absolute_average_fold_error","text":"<pre><code>absolute_average_fold_error(y_true: np.ndarray, y_pred: np.ndarray) -&gt; float\n</code></pre> <p>Calculate the Absolute Average Fold Error (AAFE) metric. It measures the fold change between predicted values and observed values. The implementation is based on this paper.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The true target values of shape (n_samples,)</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted target values of shape (n_samples,).</p> required <p>Returns:</p> Name Type Description <code>aafe</code> <code>float</code> <p>The Absolute Average Fold Error.</p>"},{"location":"api/evaluation.html#polaris.evaluate.metrics.generic_metrics.cohen_kappa_score","title":"cohen_kappa_score","text":"<pre><code>cohen_kappa_score(y_true, y_pred, **kwargs)\n</code></pre> <p>Scikit learn cohen_kappa_score wraper with renamed arguments</p>"},{"location":"api/evaluation.html#polaris.evaluate.metrics.generic_metrics.average_precision_score","title":"average_precision_score","text":"<pre><code>average_precision_score(y_true, y_score, **kwargs)\n</code></pre> <p>Scikit learn average_precision_score wrapper that throws an error if y_true has no positive class</p>"},{"location":"api/evaluation.html#polaris.evaluate.metrics.docking_metrics","title":"polaris.evaluate.metrics.docking_metrics","text":""},{"location":"api/evaluation.html#polaris.evaluate.metrics.docking_metrics.rmsd_coverage","title":"rmsd_coverage","text":"<pre><code>rmsd_coverage(\n    y_pred: Union[str, List[dm.Mol]],\n    y_true: Union[str, list[dm.Mol]],\n    max_rsmd: float = 2,\n)\n</code></pre> <p>Calculate the coverage of molecules with an RMSD less than a threshold (2 \u00c5 by default) compared to the reference molecule conformer.</p> <p>It is assumed that the predicted binding conformers are extracted from the docking output, where the receptor (protein) coordinates have been aligned with the original crystal structure.</p> <p>Attributes:</p> Name Type Description <code>y_pred</code> <p>List of predicted binding conformers.</p> <code>y_true</code> <p>List of ground truth binding confoermers.</p> <code>max_rsmd</code> <p>The threshold for determining acceptable rsmd.</p>"},{"location":"api/factory.html","title":"Dataset Factory","text":""},{"location":"api/factory.html#polaris.dataset.DatasetFactory","title":"polaris.dataset.DatasetFactory","text":"<p>The <code>DatasetFactory</code> makes it easier to create complex datasets.</p> <p>It is based on the factory design pattern and allows a user to specify specific handlers (i.e. <code>Converter</code> objects) for different file types. These converters are used to convert commonly used file types in drug discovery to something that can be used within Polaris while losing as little information as possible.</p> <p>In addition, it contains utility method to incrementally build out a dataset from different sources.</p> Try quickly converting one of your datasets <p>The <code>DatasetFactory</code> is designed to give you full control. If your dataset is saved in a single file and you don't need anything fancy, you can try use <code>create_dataset_from_file</code> instead.</p> <pre><code>from polaris.dataset import create_dataset_from_file\ndataset = create_dataset_from_file(\"path/to/my_dataset.sdf\")\n</code></pre> How to make adding meta-data easier? <p>The <code>DatasetFactory</code> is designed to more easily pull together data from different sources. However, adding meta-data remains a laborious process. How could we make this simpler through the Python API?</p>"},{"location":"api/factory.html#polaris.dataset.DatasetFactory.zarr_root_path","title":"zarr_root_path  <code>property</code>","text":"<pre><code>zarr_root_path: Group\n</code></pre> <p>The root of the zarr archive for the Dataset that is being built. All data for a single dataset is expected to be stored in the same Zarr archive.</p>"},{"location":"api/factory.html#polaris.dataset.DatasetFactory.zarr_root","title":"zarr_root  <code>property</code>","text":"<pre><code>zarr_root: Group\n</code></pre> <p>The root of the zarr archive for the Dataset that is being built. All data for a single dataset is expected to be stored in the same Zarr archive.</p>"},{"location":"api/factory.html#polaris.dataset.DatasetFactory.register_converter","title":"register_converter","text":"<pre><code>register_converter(ext: str, converter: Converter)\n</code></pre> <p>Registers a new converter for a specific file type.</p> <p>Parameters:</p> Name Type Description Default <code>ext</code> <code>str</code> <p>The file extension for which the converter should be used. There can only be a single converter per file extension.</p> required <code>converter</code> <code>Converter</code> <p>The handler for the file type. This should convert the file to a Polaris-compatible format.</p> required"},{"location":"api/factory.html#polaris.dataset.DatasetFactory.add_column","title":"add_column","text":"<pre><code>add_column(\n    column: pd.Series,\n    annotation: ColumnAnnotation | None = None,\n    adapters: Adapter | None = None,\n)\n</code></pre> <p>Add a single column to the DataFrame</p> <p>We require:</p> <ol> <li>The name attribute of the column to be set.</li> <li>The name attribute of the column to be unique.</li> <li>If the column is a pointer column, the <code>zarr_root_path</code> needs to be set.</li> <li>The length of the column to match the length of the already constructed table.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>column</code> <code>Series</code> <p>The column to add to the dataset.</p> required <code>annotation</code> <code>ColumnAnnotation | None</code> <p>The annotation for the column. If None, a default annotation will be used.</p> <code>None</code>"},{"location":"api/factory.html#polaris.dataset.DatasetFactory.add_columns","title":"add_columns","text":"<pre><code>add_columns(\n    df: pd.DataFrame,\n    annotations: dict[str, ColumnAnnotation] | None = None,\n    adapters: dict[str, Adapter] | None = None,\n    merge_on: str | None = None,\n)\n</code></pre> <p>Add multiple columns to the dataset based on another dataframe.</p> <p>To have more control over how the two dataframes are combined, you can specify a column to merge on. This will always do an outer join.</p> <p>If not specifying a key to merge on, the columns will simply be added to the dataset that has been built so far without any reordering. They are therefore expected to meet all the same expectations as for <code>add_column</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Pandas DataFrame with the columns that we want to add to the dataset.</p> required <code>annotations</code> <code>dict[str, ColumnAnnotation] | None</code> <p>The annotations for the columns. If None, default annotations will be used.</p> <code>None</code> <code>merge_on</code> <code>str | None</code> <p>The column to merge on, if any.</p> <code>None</code>"},{"location":"api/factory.html#polaris.dataset.DatasetFactory.add_from_file","title":"add_from_file","text":"<pre><code>add_from_file(path: str)\n</code></pre> <p>Uses the registered converters to parse the data from a specific file and add it to the dataset. If no converter is found for the file extension, it raises an error.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the file that should be parsed.</p> required"},{"location":"api/factory.html#polaris.dataset.DatasetFactory.add_from_files","title":"add_from_files","text":"<pre><code>add_from_files(paths: list[str], axis: Literal[0, 1, 'index', 'columns'])\n</code></pre> <p>Uses the registered converters to parse the data from a specific files and add them to the dataset. If no converter is found for the file extension, it raises an error.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>list[str]</code> <p>The list of paths that should be parsed.</p> required <code>axis</code> <code>Literal[0, 1, 'index', 'columns']</code> <p>Axis along which the files should be added. - 0 or 'index': append the rows with files. Files must be of the same type. - 1 or 'columns': append the columns with files. Files can be of the different types.</p> required"},{"location":"api/factory.html#polaris.dataset.DatasetFactory.build","title":"build","text":"<pre><code>build() -&gt; DatasetV1\n</code></pre> <p>Returns a Dataset based on the current state of the factory.</p>"},{"location":"api/factory.html#polaris.dataset.DatasetFactory.reset","title":"reset","text":"<pre><code>reset(zarr_root_path: str | None = None)\n</code></pre> <p>Resets the factory to its initial state to start building the next dataset from scratch. Note that this will not reset the registered converters.</p> <p>Parameters:</p> Name Type Description Default <code>zarr_root_path</code> <code>str | None</code> <p>The root path of the zarr hierarchy. If you want to use pointer columns for your next dataset, this arguments needs to be passed.</p> <code>None</code>"},{"location":"api/factory.html#polaris.dataset.create_dataset_from_file","title":"polaris.dataset.create_dataset_from_file","text":"<pre><code>create_dataset_from_file(\n    path: str, zarr_root_path: str | None = None\n) -&gt; DatasetV1\n</code></pre> <p>This function is a convenience function to create a dataset from a file.</p> <p>It sets up the dataset factory with sensible defaults for the converters. For creating more complicated datasets, please use the <code>DatasetFactory</code> directly.</p>"},{"location":"api/factory.html#polaris.dataset.create_dataset_from_files","title":"polaris.dataset.create_dataset_from_files","text":"<pre><code>create_dataset_from_files(\n    paths: list[str],\n    zarr_root_path: str | None = None,\n    axis: Literal[0, 1, \"index\", \"columns\"] = 0,\n) -&gt; DatasetV1\n</code></pre> <p>This function is a convenience function to create a dataset from multiple files.</p> <p>It sets up the dataset factory with sensible defaults for the converters. For creating more complicated datasets, please use the <code>DatasetFactory</code> directly.</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>Literal[0, 1, 'index', 'columns']</code> <p>Axis along which the files should be added. - 0 or 'index': append the rows with files. Files must be of the same type. - 1 or 'columns': append the columns with files. Files can be of the different types.</p> <code>0</code>"},{"location":"api/hub.client.html","title":"Client","text":""},{"location":"api/hub.client.html#polaris.hub.settings.PolarisHubSettings","title":"polaris.hub.settings.PolarisHubSettings","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Settings for the OAuth2 Polaris Hub API Client.</p> Secrecy of these settings <p>Since the Polaris Hub uses PCKE (Proof Key for Code Exchange) for OAuth2, these values thus do not have to be kept secret. See RFC 7636 for more info.</p> <p>Attributes:</p> Name Type Description <code>hub_url</code> <code>HttpUrlString</code> <p>The URL to the main page of the Polaris Hub.</p> <code>api_url</code> <code>HttpUrlString | None</code> <p>The URL to the main entrypoint of the Polaris API.</p> <code>authorize_url</code> <code>HttpUrlString</code> <p>The URL of the OAuth2 authorization endpoint.</p> <code>callback_url</code> <code>HttpUrlString | None</code> <p>The URL to which the user is redirected after authorization.</p> <code>token_fetch_url</code> <code>HttpUrlString</code> <p>The URL of the OAuth2 token endpoint.</p> <code>user_info_url</code> <code>HttpUrlString</code> <p>The URL of the OAuth2 user info endpoint.</p> <code>scopes</code> <code>str</code> <p>The OAuth2 scopes that are requested.</p> <code>client_id</code> <code>str</code> <p>The OAuth2 client ID.</p> <code>ca_bundle</code> <code>Union[str, bool, None]</code> <p>The path to a CA bundle file for requests. Allows for custom SSL certificates to be used.</p> <code>default_timeout</code> <code>TimeoutTypes</code> <p>The default timeout for requests.</p> <code>hub_token_url</code> <code>HttpUrlString | None</code> <p>The URL of the Polaris Hub token endpoint. A default value is generated based on the hub URL, and this should not need to be overridden.</p> <code>username</code> <code>str | None</code> <p>The username for the Polaris Hub, for the optional password-based authentication.</p> <code>password</code> <code>str | None</code> <p>The password for the specified username.</p>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient","title":"polaris.hub.client.PolarisHubClient","text":"<pre><code>PolarisHubClient(\n    settings: PolarisHubSettings | None = None,\n    cache_auth_token: bool = True,\n    **kwargs: dict,\n)\n</code></pre> <p>               Bases: <code>OAuth2Client</code></p> <p>A client for the Polaris Hub API. The Polaris Hub is a central repository of datasets, benchmarks and results. Visit it here: https://polarishub.io/.</p> <p>Bases the <code>authlib</code> client, which in turns bases the <code>httpx</code> client. See the relevant docs to learn more about how to use these clients outside of the integration with the Polaris Hub.</p> Closing the client <p>The client should be closed after all requests have been made. For convenience, you can also use the client as a context manager to automatically close the client when the context is exited. Note that once the client has been closed, it cannot be used anymore.</p> <pre><code># Make sure to close the client once finished\nclient = PolarisHubClient()\nclient.get(...)\nclient.close()\n\n# Or use the client as a context manager\nwith PolarisHubClient() as client:\n    client.get(...)\n</code></pre> Interacting with artifacts owned by an organization <p>Soon after being added to a new organization on Polaris, there may be a delay spanning some minutes where you cannot upload/download artifacts where the aforementioned organization is the owner. If this occurs, please re-login via <code>polaris login --overwrite</code> and try again.</p> Async Client <p><code>authlib</code> also supports an async client. Since we don't expect to make multiple requests to the Hub in parallel and due to the added complexity stemming from using the Python asyncio API, we are sticking to the sync client - at least for now.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>PolarisHubSettings | None</code> <p>A <code>PolarisHubSettings</code> instance.</p> <code>None</code> <code>cache_auth_token</code> <code>bool</code> <p>Whether to cache the auth token to a file.</p> <code>True</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to the authlib <code>OAuth2Client</code> constructor.</p> <code>{}</code>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.get_metadata_from_response","title":"get_metadata_from_response","text":"<pre><code>get_metadata_from_response(response: Response, key: str) -&gt; str | None\n</code></pre> <p>Get custom metadata saved to the R2 object from the headers.</p>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.login","title":"login","text":"<pre><code>login(overwrite: bool = False, auto_open_browser: bool = True)\n</code></pre> <p>Login to the Polaris Hub using the OAuth2 protocol.</p> Headless authentication <p>It is currently not possible to login to the Polaris Hub without a browser. See this Github issue for more info.</p> <p>Parameters:</p> Name Type Description Default <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the current token if the user is already logged in.</p> <code>False</code> <code>auto_open_browser</code> <code>bool</code> <p>Whether to automatically open the browser to visit the authorization URL.</p> <code>True</code>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.list_datasets","title":"list_datasets","text":"<pre><code>list_datasets(limit: int = 100, offset: int = 0) -&gt; list[str]\n</code></pre> <p>List all available datasets (v1 and v2) on the Polaris Hub. We prioritize v2 datasets over v1 datasets.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>The maximum number of datasets to return.</p> <code>100</code> <code>offset</code> <code>int</code> <p>The offset from which to start returning datasets.</p> <code>0</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of dataset names in the format <code>owner/dataset_name</code>.</p>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(\n    owner: str | HubOwner,\n    name: str,\n    verify_checksum: ChecksumStrategy = \"verify_unless_zarr\",\n) -&gt; DatasetV1 | DatasetV2\n</code></pre> <p>Load a standard dataset from the Polaris Hub.</p> <p>Parameters:</p> Name Type Description Default <code>owner</code> <code>str | HubOwner</code> <p>The owner of the dataset. Can be either a user or organization from the Polaris Hub.</p> required <code>name</code> <code>str</code> <p>The name of the dataset.</p> required <code>verify_checksum</code> <code>ChecksumStrategy</code> <p>Whether to use the checksum to verify the integrity of the dataset. If None, will infer a practical default based on the dataset's storage location.</p> <code>'verify_unless_zarr'</code> <p>Returns:</p> Type Description <code>DatasetV1 | DatasetV2</code> <p>A <code>Dataset</code> instance, if it exists.</p>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.list_benchmarks","title":"list_benchmarks","text":"<pre><code>list_benchmarks(limit: int = 100, offset: int = 0) -&gt; list[str]\n</code></pre> <p>List all available benchmarks on the Polaris Hub.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>The maximum number of benchmarks to return.</p> <code>100</code> <code>offset</code> <code>int</code> <p>The offset from which to start returning benchmarks.</p> <code>0</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of benchmark names in the format <code>owner/benchmark_name</code>.</p>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.get_benchmark","title":"get_benchmark","text":"<pre><code>get_benchmark(\n    owner: str | HubOwner,\n    name: str,\n    verify_checksum: ChecksumStrategy = \"verify_unless_zarr\",\n) -&gt; BenchmarkV1Specification | BenchmarkV2Specification\n</code></pre> <p>Load a benchmark from the Polaris Hub.</p> <p>Parameters:</p> Name Type Description Default <code>owner</code> <code>str | HubOwner</code> <p>The owner of the benchmark. Can be either a user or organization from the Polaris Hub.</p> required <code>name</code> <code>str</code> <p>The name of the benchmark.</p> required <code>verify_checksum</code> <code>ChecksumStrategy</code> <p>Whether to use the checksum to verify the integrity of the benchmark.</p> <code>'verify_unless_zarr'</code> <p>Returns:</p> Type Description <code>BenchmarkV1Specification | BenchmarkV2Specification</code> <p>A <code>BenchmarkSpecification</code> instance, if it exists.</p>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.upload_results","title":"upload_results","text":"<pre><code>upload_results(\n    results: BenchmarkResults,\n    access: AccessType = \"private\",\n    owner: HubOwner | str | None = None,\n)\n</code></pre> <p>Upload the results to the Polaris Hub.</p> Owner <p>The owner of the results will automatically be inferred by the hub from the user making the request. Contrary to other artifact types, an organization cannot own a set of results. However, you can specify the <code>BenchmarkResults.contributors</code> field to share credit with other hub users.</p> Required meta-data <p>The Polaris client and hub maintain different requirements as to which meta-data is required. The requirements by the hub are stricter, so when uploading to the hub you might get some errors on missing meta-data. Make sure to fill-in as much of the meta-data as possible before uploading.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>BenchmarkResults</code> <p>The results to upload.</p> required <code>access</code> <code>AccessType</code> <p>Grant public or private access to result</p> <code>'private'</code> <code>owner</code> <code>HubOwner | str | None</code> <p>Which Hub user or organization owns the artifact. Takes precedence over <code>results.owner</code>.</p> <code>None</code>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.upload_dataset","title":"upload_dataset","text":"<pre><code>upload_dataset(\n    dataset: DatasetV1 | DatasetV2,\n    access: AccessType = \"private\",\n    timeout: TimeoutTypes = (10, 200),\n    owner: HubOwner | str | None = None,\n    if_exists: ZarrConflictResolution = \"replace\",\n)\n</code></pre> <p>Upload a dataset to the Polaris Hub.</p> Owner <p>You have to manually specify the owner in the dataset data model. Because the owner could be a user or an organization, we cannot automatically infer this from just the logged-in user.</p> Required meta-data <p>The Polaris client and hub maintain different requirements as to which meta-data is required. The requirements by the hub are stricter, so when uploading to the hub you might get some errors on missing meta-data. Make sure to fill-in as much of the meta-data as possible before uploading.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DatasetV1 | DatasetV2</code> <p>The dataset to upload.</p> required <code>access</code> <code>AccessType</code> <p>Grant public or private access to result</p> <code>'private'</code> <code>timeout</code> <code>TimeoutTypes</code> <p>Request timeout values. User can modify the value when uploading large dataset as needed. This can be a single value with the timeout in seconds for all IO operations, or a more granular tuple with (connect_timeout, write_timeout). The type of the the timout parameter comes from <code>httpx</code>. Since datasets can get large, it might be needed to increase the write timeout for larger datasets. See also: https://www.python-httpx.org/advanced/#timeout-configuration</p> <code>(10, 200)</code> <code>owner</code> <code>HubOwner | str | None</code> <p>Which Hub user or organization owns the artifact. Takes precedence over <code>dataset.owner</code>.</p> <code>None</code> <code>if_exists</code> <code>ZarrConflictResolution</code> <p>Action for handling existing files in the Zarr archive. Options are 'raise' to throw an error, 'replace' to overwrite, or 'skip' to proceed without altering the existing files.</p> <code>'replace'</code>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.upload_benchmark","title":"upload_benchmark","text":"<pre><code>upload_benchmark(\n    benchmark: BenchmarkV1Specification | BenchmarkV2Specification,\n    access: AccessType = \"private\",\n    owner: HubOwner | str | None = None,\n)\n</code></pre> <p>Upload the benchmark to the Polaris Hub.</p> Owner <p>You have to manually specify the owner in the benchmark data model. Because the owner could be a user or an organization, we cannot automatically infer this from the logged-in user.</p> Required meta-data <p>The Polaris client and hub maintain different requirements as to which meta-data is required. The requirements by the hub are stricter, so when uploading to the hub you might get some errors on missing meta-data. Make sure to fill-in as much of the meta-data as possible before uploading.</p> Non-existent datasets <p>The client will not upload the associated dataset to the hub if it does not yet exist. Make sure to specify an existing dataset or upload the dataset first.</p> <p>Parameters:</p> Name Type Description Default <code>benchmark</code> <code>BenchmarkV1Specification | BenchmarkV2Specification</code> <p>The benchmark to upload.</p> required <code>access</code> <code>AccessType</code> <p>Grant public or private access to result</p> <code>'private'</code> <code>owner</code> <code>HubOwner | str | None</code> <p>Which Hub user or organization owns the artifact. Takes precedence over <code>benchmark.owner</code>.</p> <code>None</code>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.get_competition","title":"get_competition","text":"<pre><code>get_competition(artifact_id: str) -&gt; CompetitionSpecification\n</code></pre> <p>Load a competition from the Polaris Hub.</p> <p>Parameters:</p> Name Type Description Default <code>artifact_id</code> <code>str</code> <p>The artifact identifier for the competition</p> required <p>Returns:</p> Type Description <code>CompetitionSpecification</code> <p>A <code>CompetitionSpecification</code> instance, if it exists.</p>"},{"location":"api/hub.client.html#polaris.hub.client.PolarisHubClient.submit_competition_predictions","title":"submit_competition_predictions","text":"<pre><code>submit_competition_predictions(\n    competition: CompetitionSpecification,\n    competition_predictions: CompetitionPredictions,\n)\n</code></pre> <p>Submit predictions for a competition to the Polaris Hub. The Hub will evaluate them against the secure test set and store the result.</p> <p>Parameters:</p> Name Type Description Default <code>competition</code> <code>CompetitionSpecification</code> <p>The competition to evaluate the predictions for.</p> required <code>competition_predictions</code> <code>CompetitionPredictions</code> <p>The predictions and associated metadata to be submitted to the Hub.</p> required"},{"location":"api/hub.external_client.html","title":"External Auth Client","text":""},{"location":"api/hub.external_client.html#polaris.hub.external_client.ExternalAuthClient","title":"polaris.hub.external_client.ExternalAuthClient","text":"<pre><code>ExternalAuthClient(\n    settings: PolarisHubSettings, cache_auth_token: bool = True, **kwargs: dict\n)\n</code></pre> <p>               Bases: <code>OAuth2Client</code></p> <p>This authentication client is used to obtain OAuth 2 tokens from Polaris's external OAuth2 server. These can in turn be used to obtain Polaris Hub tokens.</p> Internal use <p>This class is intended for internal use by the <code>PolarisHubClient</code> class, and you should not have to interact with it directly.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>PolarisHubSettings</code> <p>A <code>PolarisHubSettings</code> instance.</p> required <code>cache_auth_token</code> <code>bool</code> <p>Whether to cache the auth token to a file.</p> <code>True</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to the authlib <code>OAuth2Client</code> constructor.</p> <code>{}</code>"},{"location":"api/hub.external_client.html#polaris.hub.external_client.ExternalAuthClient.user_info","title":"user_info  <code>property</code>","text":"<pre><code>user_info: dict\n</code></pre> <p>Get information about the currently logged-in user through the OAuth2 User Info flow.</p>"},{"location":"api/hub.external_client.html#polaris.hub.external_client.ExternalAuthClient.interactive_login","title":"interactive_login","text":"<pre><code>interactive_login(overwrite: bool = False, auto_open_browser: bool = True)\n</code></pre> <p>Login to the Polaris Hub using an interactive flow, through a Web browser.</p> Headless authentication <p>It is currently not possible to log in to the Polaris Hub without a browser. See this GitHub issue for more info.</p> <p>Parameters:</p> Name Type Description Default <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the current token if the user is already logged in.</p> <code>False</code> <code>auto_open_browser</code> <code>bool</code> <p>Whether to automatically open the browser to visit the authorization URL.</p> <code>True</code>"},{"location":"api/hub.polarisfs.html","title":"PolarisFileSystem","text":""},{"location":"api/hub.polarisfs.html#polaris.hub.polarisfs.PolarisFileSystem","title":"polaris.hub.polarisfs.PolarisFileSystem","text":"<pre><code>PolarisFileSystem(\n    polaris_client: PolarisHubClient,\n    dataset_owner: str,\n    dataset_name: str,\n    **kwargs: dict,\n)\n</code></pre> <p>               Bases: <code>AbstractFileSystem</code></p> <p>A file system interface for accessing datasets on the Polaris platform.</p> <p>This class extends <code>fsspec.AbstractFileSystem</code> and provides methods to list objects within a Polaris dataset and fetch the content of a file from the dataset.</p> Zarr Integration <p>This file system can be used with Zarr to load multidimensional array data stored in a Dataset from the Polaris infrastructure. This class is needed because we otherwise cannot generate signed URLs for folders and Zarr is a folder based data-format.</p> <pre><code>fs = PolarisFileSystem(...)\nstore = zarr.storage.FSStore(..., fs=polaris_fs)\nroot = zarr.open(store, mode=\"r\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>polaris_client</code> <code>PolarisHubClient</code> <p>The Polaris Hub client used to make API requests.</p> required <code>dataset_owner</code> <code>str</code> <p>The owner of the dataset.</p> required <code>dataset_name</code> <code>str</code> <p>The name of the dataset.</p> required"},{"location":"api/hub.polarisfs.html#polaris.hub.polarisfs.PolarisFileSystem.is_polarisfs_path","title":"is_polarisfs_path  <code>staticmethod</code>","text":"<pre><code>is_polarisfs_path(path: str) -&gt; bool\n</code></pre> <p>Check if the given path is a PolarisFS path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the path is a PolarisFS path; otherwise, False.</p>"},{"location":"api/hub.polarisfs.html#polaris.hub.polarisfs.PolarisFileSystem.ls","title":"ls","text":"<pre><code>ls(\n    path: str,\n    detail: bool = False,\n    timeout: Optional[TimeoutTypes] = None,\n    **kwargs: dict,\n) -&gt; Union[List[str], List[Dict[str, Any]]]\n</code></pre> <p>List objects in the specified path within the Polaris dataset.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path within the dataset to list objects.</p> required <code>detail</code> <code>bool</code> <p>If True, returns detailed information about each object.</p> <code>False</code> <code>timeout</code> <code>Optional[TimeoutTypes]</code> <p>Maximum time (in seconds) to wait for the request to complete.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[List[str], List[Dict[str, Any]]]</code> <p>A list of dictionaries if detail is True; otherwise, a list of object names.</p>"},{"location":"api/hub.polarisfs.html#polaris.hub.polarisfs.PolarisFileSystem.cat_file","title":"cat_file","text":"<pre><code>cat_file(\n    path: str,\n    start: Union[int, None] = None,\n    end: Union[int, None] = None,\n    timeout: Optional[TimeoutTypes] = None,\n    **kwargs: dict,\n) -&gt; bytes\n</code></pre> <p>Fetches and returns the content of a file from the Polaris dataset.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the file within the dataset.</p> required <code>start</code> <code>Union[int, None]</code> <p>The starting index of the content to retrieve.</p> <code>None</code> <code>end</code> <code>Union[int, None]</code> <p>The ending index of the content to retrieve.</p> <code>None</code> <code>timeout</code> <code>Optional[TimeoutTypes]</code> <p>Maximum time (in seconds) to wait for the request to complete.</p> <code>None</code> <code>kwargs</code> <code>dict</code> <p>Extra arguments passed to <code>fsspec.open()</code></p> <code>{}</code> <p>Returns:</p> Type Description <code>bytes</code> <p>The content of the requested file.</p>"},{"location":"api/hub.polarisfs.html#polaris.hub.polarisfs.PolarisFileSystem.rm","title":"rm","text":"<pre><code>rm(path: str, recursive: bool = False, maxdepth: Optional[int] = None) -&gt; None\n</code></pre> <p>Remove a file or directory from the Polaris dataset.</p> <p>This method is provided for compatibility with the Zarr storage interface. It may be called by the Zarr store when removing a file or directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the file or directory to be removed.</p> required <code>recursive</code> <code>bool</code> <p>If True, remove directories and their contents recursively.</p> <code>False</code> <code>maxdepth</code> <code>Optional[int]</code> <p>The maximum depth to recurse when removing directories.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Note <p>This method currently it does not perform any removal operations and is included as a placeholder that aligns with the Zarr interface's expectations.</p>"},{"location":"api/hub.polarisfs.html#polaris.hub.polarisfs.PolarisFileSystem.pipe_file","title":"pipe_file","text":"<pre><code>pipe_file(\n    path: str,\n    content: Union[bytes, str],\n    timeout: Optional[TimeoutTypes] = None,\n    **kwargs: dict,\n) -&gt; None\n</code></pre> <p>Pipes the content of a file to the Polaris dataset.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the file within the dataset.</p> required <code>content</code> <code>Union[bytes, str]</code> <p>The content to be piped into the file.</p> required <code>timeout</code> <code>Optional[TimeoutTypes]</code> <p>Maximum time (in seconds) to wait for the request to complete.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api/hub.storage.html","title":"Hub.storage","text":""},{"location":"api/hub.storage.html#polaris.hub.storage.StorageSession","title":"polaris.hub.storage.StorageSession","text":"<pre><code>StorageSession(hub_client, scope: Scope, resource: ArtifactUrn)\n</code></pre> <p>               Bases: <code>OAuth2Client</code></p> <p>A context manager for managing a storage session, with token exchange and token refresh capabilities. Each session is associated with a specific scope and resource.</p>"},{"location":"api/hub.storage.html#polaris.hub.storage.StorageSession.fetch_token","title":"fetch_token","text":"<pre><code>fetch_token(**kwargs) -&gt; dict[str, Any]\n</code></pre> <p>Error handling for token fetching.</p>"},{"location":"api/hub.storage.html#polaris.hub.storage.StorageSession.ensure_active_token","title":"ensure_active_token","text":"<pre><code>ensure_active_token(token: OAuth2Token | None = None) -&gt; bool\n</code></pre> <p>Override the active check to trigger a re-fetch of the token if it is not active.</p>"},{"location":"api/hub.storage.html#polaris.hub.storage.StorageSession.set_file","title":"set_file","text":"<pre><code>set_file(path: str, value: bytes | bytearray)\n</code></pre> <p>Set a value at the given path.</p>"},{"location":"api/hub.storage.html#polaris.hub.storage.StorageSession.get_file","title":"get_file","text":"<pre><code>get_file(path: str) -&gt; bytes | bytearray\n</code></pre> <p>Get the value at the given path.</p>"},{"location":"api/hub.storage.html#polaris.hub.storage.S3Store","title":"polaris.hub.storage.S3Store","text":"<pre><code>S3Store(\n    path: str | PurePath,\n    access_key: str,\n    secret_key: str,\n    token: str,\n    endpoint_url: str,\n    part_size: int = 10 * 1024 * 1024,\n    content_type: str = \"application/octet-stream\",\n)\n</code></pre> <p>               Bases: <code>Store</code></p> <p>A Zarr store implementation using a S3 bucket as the backend storage.</p> <p>It supports multipart uploads for large objects and handles S3-specific exceptions.</p>"},{"location":"api/hub.storage.html#polaris.hub.storage.S3Store.copy_to_destination","title":"copy_to_destination","text":"<pre><code>copy_to_destination(\n    destination: Store,\n    if_exists: ZarrConflictResolution = \"replace\",\n    log: Callable = lambda: None,\n) -&gt; tuple[int, int, int]\n</code></pre> <p>Copy the content of this store to the destination store.</p> <p>We leverage the internal knowledge of this store to make the operation more efficient than <code>zarr.copy_store</code>:     - Parallel, concurrent <code>getitems</code> operations using a thread pool</p> <p>Zarr V3 supports partial writes, that would allow us to stream the response back into the store. Unfortunately, it's not supported right now by the library version we use, so we'll have to read the whole response into memory and write it back to the destination.</p> <p>Parameters:</p> Name Type Description Default <code>destination</code> <code>Store</code> <p>destination store</p> required <code>if_exists</code> <code>ZarrConflictResolution</code> <p>behavior if the destination key already exists</p> <code>'replace'</code> <code>log</code> <code>Callable</code> <p>optional logging function</p> <code>lambda: None</code>"},{"location":"api/hub.storage.html#polaris.hub.storage.S3Store.copy_from_source","title":"copy_from_source","text":"<pre><code>copy_from_source(\n    source: Store,\n    if_exists: ZarrConflictResolution = \"replace\",\n    log: Callable = lambda: None,\n) -&gt; tuple[int, int, int]\n</code></pre> <p>Copy the content of the source store to this store.</p> <p>We leverage the internal knowledge of this store to make the operation more efficient than <code>zarr.copy_store</code>:     - Conditional <code>put_object</code> operation for the \"skip\" conflict resolution     - Parallel, concurrent <code>put_object</code> operations using a thread pool</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Store</code> <p>source store</p> required <code>if_exists</code> <code>ZarrConflictResolution</code> <p>behavior if the destination key already exists</p> <code>'replace'</code> <code>log</code> <code>Callable</code> <p>optional logging function</p> <code>lambda: None</code>"},{"location":"api/hub.storage.html#polaris.hub.storage.S3Store.listdir","title":"listdir","text":"<pre><code>listdir(path: str = '') -&gt; Generator[str, None, None]\n</code></pre> <p>For a given path, list all the \"subdirectories\" and \"files\" for that path. The returned paths are relative to the input path.</p> <p>Uses pagination and return a generator to handle very large number of keys. Note: This might not help with some Zarr operations that materialize the whole sequence.</p>"},{"location":"api/hub.storage.html#polaris.hub.storage.S3Store.getitems","title":"getitems","text":"<pre><code>getitems(\n    keys: Sequence[str], *, contexts: Mapping[str, Context]\n) -&gt; dict[str, Any]\n</code></pre> <p>More efficient implementation of getitems using concurrent fetching through multiple threads.</p> <p>The default implementation uses contains to check existence before fetching the value, which doubles the number of requests.</p>"},{"location":"api/hub.storage.html#polaris.hub.storage.S3Store.getsize","title":"getsize","text":"<pre><code>getsize(key: str) -&gt; int\n</code></pre> <p>Return the size (in bytes) of the object at the given key.</p>"},{"location":"api/hub.storage.html#polaris.hub.storage.S3Store.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key: str) -&gt; bytes\n</code></pre> <p>Retrieves the value for the given key from the store.</p> <p>Makes no provision to handle overly large values returned.</p>"},{"location":"api/hub.storage.html#polaris.hub.storage.S3Store.__setitem__","title":"__setitem__","text":"<pre><code>__setitem__(key: str, value: bytes | bytearray | memoryview) -&gt; None\n</code></pre> <p>Persists the given value in the store.</p> <p>Based on value size, will use multipart upload for large files, or a single put_object call.</p>"},{"location":"api/hub.storage.html#polaris.hub.storage.S3Store.__delitem__","title":"__delitem__","text":"<pre><code>__delitem__(key: str) -&gt; None\n</code></pre> <p>Removing a key from the store is not supported.</p>"},{"location":"api/hub.storage.html#polaris.hub.storage.S3Store.__contains__","title":"__contains__","text":"<pre><code>__contains__(key: str) -&gt; bool\n</code></pre> <p>Checks the existence of a key in the store.</p> <p>If the intent is to download the value after this check, it is more efficient to attempt tp retrieve it and handle the KeyError from a non-existent key.</p>"},{"location":"api/hub.storage.html#polaris.hub.storage.S3Store.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; Generator[str, None, None]\n</code></pre> <p>Iterate through all the keys in the store.</p>"},{"location":"api/hub.storage.html#polaris.hub.storage.S3Store.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Number of keys in the store.</p>"},{"location":"api/load.html","title":"Load","text":""},{"location":"api/load.html#polaris.load_dataset","title":"polaris.load_dataset","text":"<pre><code>load_dataset(\n    path: str, verify_checksum: ChecksumStrategy = \"verify_unless_zarr\"\n) -&gt; DatasetV1\n</code></pre> <p>Loads a Polaris dataset.</p> <p>In Polaris, a dataset is a tabular data structure that stores data-points in a row-wise manner. A dataset can have multiple modalities or targets, can be sparse and can be part of one or multiple benchmarks.</p> <p>The Polaris dataset can be loaded from the Hub or from a local or remote directory.</p> <ul> <li>Hub (recommended): When loading the dataset from the Hub, you can simply     provide the <code>owner/name</code> slug. This can be easily copied from the relevant dataset     page on the Hub.</li> <li>Directory: When loading the dataset from a directory, you should provide the path     as returned by <code>Dataset.to_json</code>.     The path can be local or remote.</li> </ul>"},{"location":"api/load.html#polaris.load_benchmark","title":"polaris.load_benchmark","text":"<pre><code>load_benchmark(\n    path: str, verify_checksum: ChecksumStrategy = \"verify_unless_zarr\"\n)\n</code></pre> <p>Loads a Polaris benchmark.</p> <p>In Polaris, a benchmark wraps a dataset with additional meta-data to specify the evaluation logic.</p> <p>The Polaris benchmark can be loaded from the Hub or from a local or remote directory.</p> Dataset is automatically loaded <p>The dataset underlying the benchmark is automatically loaded when loading the benchmark.</p> <ul> <li>Hub (recommended): When loading the benchmark from the Hub, you can simply     provide the <code>owner/name</code> slug. This can be easily copied from the relevant benchmark     page on the Hub.</li> <li>Directory: When loading the benchmark from a directory, you should provide the path     as returned by <code>BenchmarkSpecification.to_json</code>.     The path can be local or remote.</li> </ul>"},{"location":"api/subset.html","title":"Subset","text":""},{"location":"api/subset.html#polaris.dataset.Subset","title":"polaris.dataset.Subset","text":"<p>The <code>Subset</code> class provides easy access to a single partition of a split dataset.</p> No need to create this class manually <p>You should not have to create this class manually. In most use-cases, you can create a <code>Subset</code> through the <code>get_train_test_split</code> method of a <code>BenchmarkSpecification</code> object.</p> Featurize your inputs <p>Not all datasets are already featurized. For example, a small-molecule task might simply provide the SMILES string. To easily featurize the inputs, you can pass or set a transformation function. For example:</p> <pre><code>import datamol as dm\n\nbenchmark.get_train_test_split(..., featurization_fn=dm.to_fp)\n</code></pre> <p>This should be the starting point for any framework-specific (e.g. PyTorch, Tensorflow) data-loader implementation. How the data is loaded in Polaris can be non-trivial, so this class is provided to abstract away the details. To easily build framework-specific data-loaders, a <code>Subset</code> supports various styles of accessing the data:</p> <ol> <li>In memory: Loads the entire dataset in memory and returns a single array with all datapoints,     this style is accessible through the <code>subset.targets</code> and <code>subset.inputs</code> properties.</li> <li>List: Index the subset like a list, this style is accessible through the <code>subset[idx]</code> syntax.</li> <li>Iterator: Iterate over the subset, this style is accessible through the <code>iter(subset)</code> syntax.</li> </ol> <p>Examples:</p> <p>The different styles of accessing the data:</p> <pre><code>import polaris as po\n\nbenchmark = po.load_benchmark(...)\ntrain, test = benchmark.get_train_test_split()\n\n# Load the entire dataset in memory, useful for e.g. scikit-learn.\nX = train.inputs\ny = train.targets\n\n# Access a single datapoint as with a list, useful for e.g. PyTorch.\nx, y = train[0]\n\n# Iterate over the dataset, useful for very large datasets.\nfor x, y in train:\n    ...\n</code></pre> <p>Raises:</p> Type Description <code>TestAccessError</code> <p>When trying to access the targets of the test set (specified by the <code>hide_targets</code> attribute).</p>"},{"location":"api/utils.types.html","title":"Types","text":""},{"location":"api/utils.types.html#polaris.utils.types.SplitIndicesType","title":"SplitIndicesType  <code>module-attribute</code>","text":"<pre><code>SplitIndicesType: TypeAlias = list[int]\n</code></pre> <p>A split is defined by a sequence of integers.</p>"},{"location":"api/utils.types.html#polaris.utils.types.SplitType","title":"SplitType  <code>module-attribute</code>","text":"<pre><code>SplitType: TypeAlias = tuple[\n    SplitIndicesType, Union[SplitIndicesType, dict[str, SplitIndicesType]]\n]\n</code></pre> <p>A split is a pair of which the first item is always assumed to be the train set. The second item can either be a single test set or a dictionary with multiple, named test sets.</p>"},{"location":"api/utils.types.html#polaris.utils.types.ListOrArrayType","title":"ListOrArrayType  <code>module-attribute</code>","text":"<pre><code>ListOrArrayType: TypeAlias = list | ndarray\n</code></pre> <p>A list of numbers or a numpy array. Predictions can be provided as either a list or a numpy array.</p>"},{"location":"api/utils.types.html#polaris.utils.types.IncomingPredictionsType","title":"IncomingPredictionsType  <code>module-attribute</code>","text":"<pre><code>IncomingPredictionsType: TypeAlias = (\n    ListOrArrayType | dict[str, ListOrArrayType | dict[str, ListOrArrayType]]\n)\n</code></pre> <p>The type of the predictions that are ingested into the Polaris BenchmarkPredictions object. Can be one of the following:</p> <ul> <li>A single array (single-task, single test set)</li> <li>A dictionary of arrays (single-task, multiple test sets)</li> <li>A dictionary of dictionaries of arrays (multi-task, multiple test sets)</li> </ul>"},{"location":"api/utils.types.html#polaris.utils.types.PredictionsType","title":"PredictionsType  <code>module-attribute</code>","text":"<pre><code>PredictionsType: TypeAlias = dict[str, dict[str, ndarray]]\n</code></pre> <p>The normalized format for predictions for internal use. Predictions are accepted in a generous variety of representations and normalized into this standard format, a dictionary of dictionaries that looks like {\"test_set_name\": {\"target_name\": np.ndarray}}.</p>"},{"location":"api/utils.types.html#polaris.utils.types.DatapointType","title":"DatapointType  <code>module-attribute</code>","text":"<pre><code>DatapointType: TypeAlias = tuple[DatapointPartType, DatapointPartType]\n</code></pre> <p>A datapoint has:</p> <ul> <li>A single input or multiple inputs (either as dict or tuple)</li> <li>No target, a single target or a multiple targets (either as dict or tuple)</li> </ul>"},{"location":"api/utils.types.html#polaris.utils.types.SlugStringType","title":"SlugStringType  <code>module-attribute</code>","text":"<pre><code>SlugStringType: TypeAlias = Annotated[\n    str, StringConstraints(pattern=\"^[a-z0-9-]+$\", min_length=4, max_length=64)\n]\n</code></pre> <p>A URL-compatible string that can serve as slug on the hub.</p>"},{"location":"api/utils.types.html#polaris.utils.types.SlugCompatibleStringType","title":"SlugCompatibleStringType  <code>module-attribute</code>","text":"<pre><code>SlugCompatibleStringType: TypeAlias = Annotated[\n    str,\n    StringConstraints(pattern=\"^[A-Za-z0-9_-]+$\", min_length=4, max_length=64),\n]\n</code></pre> <p>A URL-compatible string that can be turned into a slug by the hub.</p> <p>Can only use alpha-numeric characters, underscores and dashes. The string must be at least 4 and at most 64 characters long.</p>"},{"location":"api/utils.types.html#polaris.utils.types.Md5StringType","title":"Md5StringType  <code>module-attribute</code>","text":"<pre><code>Md5StringType: TypeAlias = Annotated[\n    str, StringConstraints(pattern=\"^[a-f0-9]{32}$\")\n]\n</code></pre> <p>A string that represents an MD5 hash.</p>"},{"location":"api/utils.types.html#polaris.utils.types.HubUser","title":"HubUser  <code>module-attribute</code>","text":"<pre><code>HubUser: TypeAlias = SlugCompatibleStringType\n</code></pre> <p>A user on the Polaris Hub is identified by a username, which is a <code>SlugCompatibleStringType</code>.</p>"},{"location":"api/utils.types.html#polaris.utils.types.HttpUrlString","title":"HttpUrlString  <code>module-attribute</code>","text":"<pre><code>HttpUrlString: TypeAlias = Annotated[\n    str, BeforeValidator(lambda v: validate_python(v) and v)\n]\n</code></pre> <p>A validated HTTP URL that will be turned into a string. This is useful for interactions with httpx and authlib, who have their own URL types.</p>"},{"location":"api/utils.types.html#polaris.utils.types.AnyUrlString","title":"AnyUrlString  <code>module-attribute</code>","text":"<pre><code>AnyUrlString: TypeAlias = Annotated[\n    str, BeforeValidator(lambda v: validate_python(v) and v)\n]\n</code></pre> <p>A validated generic URL that will be turned into a string. This is useful for interactions with other libraries that expect a string.</p>"},{"location":"api/utils.types.html#polaris.utils.types.DirectionType","title":"DirectionType  <code>module-attribute</code>","text":"<pre><code>DirectionType: TypeAlias = float | Literal['min', 'max']\n</code></pre> <p>The direction of any variable to be sorted. This can be used to sort the metric score, indicate the optmization direction of endpoint.</p>"},{"location":"api/utils.types.html#polaris.utils.types.AccessType","title":"AccessType  <code>module-attribute</code>","text":"<pre><code>AccessType: TypeAlias = Literal['public', 'private']\n</code></pre> <p>Type to specify access to a dataset, benchmark or result in the Hub.</p>"},{"location":"api/utils.types.html#polaris.utils.types.TimeoutTypes","title":"TimeoutTypes  <code>module-attribute</code>","text":"<pre><code>TimeoutTypes = Union[Tuple[int, int], Literal['timeout', 'never']]\n</code></pre> <p>Timeout types for specifying maximum wait times.</p>"},{"location":"api/utils.types.html#polaris.utils.types.IOMode","title":"IOMode  <code>module-attribute</code>","text":"<pre><code>IOMode: TypeAlias = Literal['r', 'r+', 'a', 'w', 'w-']\n</code></pre> <p>Type to specify the mode for input/output operations (I/O) when interacting with a file or resource.</p>"},{"location":"api/utils.types.html#polaris.utils.types.SupportedLicenseType","title":"SupportedLicenseType  <code>module-attribute</code>","text":"<pre><code>SupportedLicenseType: TypeAlias = Literal[\n    \"CC-BY-4.0\",\n    \"CC-BY-SA-4.0\",\n    \"CC-BY-NC-4.0\",\n    \"CC-BY-NC-SA-4.0\",\n    \"CC0-1.0\",\n    \"MIT\",\n]\n</code></pre> <p>Supported license types for dataset uploads to Polaris Hub</p>"},{"location":"api/utils.types.html#polaris.utils.types.ZarrConflictResolution","title":"ZarrConflictResolution  <code>module-attribute</code>","text":"<pre><code>ZarrConflictResolution: TypeAlias = Literal['raise', 'replace', 'skip']\n</code></pre> <p>Type to specify which action to take when encountering existing files within a Zarr archive.</p>"},{"location":"api/utils.types.html#polaris.utils.types.ChecksumStrategy","title":"ChecksumStrategy  <code>module-attribute</code>","text":"<pre><code>ChecksumStrategy: TypeAlias = Literal['verify', 'verify_unless_zarr', 'ignore']\n</code></pre> <p>Type to specify which action to take to verify the data integrity of an artifact through a checksum.</p>"},{"location":"api/utils.types.html#polaris.utils.types.ArtifactUrn","title":"ArtifactUrn  <code>module-attribute</code>","text":"<pre><code>ArtifactUrn: TypeAlias = Annotated[\n    str, StringConstraints(pattern=\"^urn:polaris:\\\\w+:\\\\w+:\\\\w+$\")\n]\n</code></pre> <p>A Uniform Resource Name (URN) for an artifact on the Polaris Hub.</p>"},{"location":"api/utils.types.html#polaris.utils.types.DatasetIndex","title":"DatasetIndex  <code>module-attribute</code>","text":"<pre><code>DatasetIndex: TypeAlias = RowIndex | tuple[RowIndex, ColumnIndex]\n</code></pre> <p>To index a dataset using square brackets, we have a few options:</p> <ul> <li>A single row, e.g. dataset[0]</li> <li>Specify a specific value, e.g. dataset[0, \"col1\"]</li> </ul> <p>There are more exciting options we could implement, such as slicing,  but this gets complex.</p>"},{"location":"api/utils.types.html#polaris.utils.types.PredictionKwargs","title":"PredictionKwargs  <code>module-attribute</code>","text":"<pre><code>PredictionKwargs: TypeAlias = Literal['y_pred', 'y_prob', 'y_score']\n</code></pre> <p>The type of predictions expected by the metric interface.</p>"},{"location":"api/utils.types.html#polaris.utils.types.ColumnName","title":"ColumnName  <code>module-attribute</code>","text":"<pre><code>ColumnName: TypeAlias = str\n</code></pre> <p>A column name in a dataset.</p>"},{"location":"api/utils.types.html#polaris.utils.types.HubOwner","title":"HubOwner","text":"<p>               Bases: <code>BaseModel</code></p> <p>An owner of an artifact on the Polaris Hub</p> <p>The slug is most important as it is the user-facing part of this data model. The externalId and type are added to be consistent with the model returned by the Polaris Hub .</p>"},{"location":"api/utils.types.html#polaris.utils.types.HubOwner.normalize","title":"normalize  <code>staticmethod</code>","text":"<pre><code>normalize(owner: str | Self) -&gt; Self\n</code></pre> <p>Normalize a string or <code>HubOwner</code> instance to a <code>HubOwner</code> instance.</p>"},{"location":"api/utils.types.html#polaris.utils.types.TargetType","title":"TargetType","text":"<p>               Bases: <code>Enum</code></p> <p>The high-level classification of different targets.</p>"},{"location":"api/utils.types.html#polaris.utils.types.TaskType","title":"TaskType","text":"<p>               Bases: <code>Enum</code></p> <p>The high-level classification of different tasks.</p>"},{"location":"tutorials/basics.html","title":"The Basics","text":"In\u00a0[2]: Copied! <pre>import polaris as po\nfrom polaris.hub.client import PolarisHubClient\n</pre> import polaris as po from polaris.hub.client import PolarisHubClient In\u00a0[\u00a0]: Copied! <pre>client = PolarisHubClient()\nclient.login()\n</pre> client = PolarisHubClient() client.login() <p>Instead of through the Python API, you could also use the Polaris CLI. See:</p> <pre>polaris login --help\n</pre> In\u00a0[4]: Copied! <pre>dataset = po.load_dataset(\"polaris/hello-world\")\nbenchmark = po.load_benchmark(\"polaris/hello-world-benchmark\")\n</pre> dataset = po.load_dataset(\"polaris/hello-world\") benchmark = po.load_benchmark(\"polaris/hello-world-benchmark\") <pre>2024-06-26 09:52:08.706 | INFO     | polaris._artifact:_validate_version:66 - The version of Polaris that was used to create the artifact (0.0.0) is different from the currently installed version of Polaris (0.0.2.dev191+g82e7db2).\n2024-06-26 09:52:10.327 | INFO     | polaris._artifact:_validate_version:66 - The version of Polaris that was used to create the artifact (0.0.0) is different from the currently installed version of Polaris (0.0.2.dev191+g82e7db2).\n2024-06-26 09:52:10.338 | INFO     | polaris._artifact:_validate_version:66 - The version of Polaris that was used to create the artifact (0.0.0) is different from the currently installed version of Polaris (0.0.2.dev191+g82e7db2).\n</pre> In\u00a0[5]: Copied! <pre>train, test = benchmark.get_train_test_split()\n</pre> train, test = benchmark.get_train_test_split() <p>The created objects support various flavours to access the data.</p> <ul> <li>The objects are iterable;</li> <li>The objects can be indexed;</li> <li>The objects have properties to access all data at once.</li> </ul> In\u00a0[6]: Copied! <pre>for x, y in train:\n    pass\n</pre> for x, y in train:     pass In\u00a0[7]: Copied! <pre>for i in range(len(train)):\n    x, y = train[i]\n</pre> for i in range(len(train)):     x, y = train[i] In\u00a0[8]: Copied! <pre>x = train.inputs\ny = train.targets\n</pre> x = train.inputs y = train.targets <p>To avoid accidental access to the test targets, the test object does not expose the labels and will throw an error if you try access them explicitly.</p> In\u00a0[9]: Copied! <pre>for x in test:\n    pass\n</pre> for x in test:     pass In\u00a0[10]: Copied! <pre>for i in range(len(test)):\n    x = test[i]\n</pre> for i in range(len(test)):     x = test[i] In\u00a0[11]: Copied! <pre>x = test.inputs\n\n# NOTE: The below will throw an error!\n# y = test.targets\n</pre> x = test.inputs  # NOTE: The below will throw an error! # y = test.targets In\u00a0[12]: Copied! <pre>import datamol as dm\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Load the benchmark (automatically loads the underlying dataset as well)\nbenchmark = po.load_benchmark(\"polaris/hello-world-benchmark\")\n\n# Get the split and convert SMILES to ECFP fingerprints by specifying an featurize function.\ntrain, test = benchmark.get_train_test_split(featurization_fn=dm.to_fp)\n\n# Define a model and train\nmodel = RandomForestRegressor(max_depth=2, random_state=0)\nmodel.fit(train.X, train.y)\n</pre> import datamol as dm from sklearn.ensemble import RandomForestRegressor  # Load the benchmark (automatically loads the underlying dataset as well) benchmark = po.load_benchmark(\"polaris/hello-world-benchmark\")  # Get the split and convert SMILES to ECFP fingerprints by specifying an featurize function. train, test = benchmark.get_train_test_split(featurization_fn=dm.to_fp)  # Define a model and train model = RandomForestRegressor(max_depth=2, random_state=0) model.fit(train.X, train.y) <pre>2024-06-26 09:52:12.003 | INFO     | polaris._artifact:_validate_version:66 - The version of Polaris that was used to create the artifact (0.0.0) is different from the currently installed version of Polaris (0.0.2.dev191+g82e7db2).\n2024-06-26 09:52:12.014 | INFO     | polaris._artifact:_validate_version:66 - The version of Polaris that was used to create the artifact (0.0.0) is different from the currently installed version of Polaris (0.0.2.dev191+g82e7db2).\n</pre> Out[12]: <pre>RandomForestRegressor(max_depth=2, random_state=0)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0RandomForestRegressor?Documentation for RandomForestRegressoriFitted<pre>RandomForestRegressor(max_depth=2, random_state=0)</pre> <p>To evaluate a model within Polaris, you should use the <code>evaluate()</code> endpoint. This requires you to just provide the predictions. The targets of the test set are automatically extracted so that the chance of the user accessing the test labels is minimal</p> In\u00a0[13]: Copied! <pre>predictions = model.predict(test.X)\n</pre> predictions = model.predict(test.X) In\u00a0[14]: Copied! <pre>results = benchmark.evaluate(predictions)\nresults\n</pre> results = benchmark.evaluate(predictions) results Out[14]: nameNonedescriptiontagsuser_attributesownerNonepolaris_version0.0.2.dev191+g82e7db2benchmark_namehello-world-benchmarkbenchmark_ownerslugpolarisexternal_idorg_2gtoaJIVrgRqiIR8Qm5BnpFCbxutypeorganizationgithub_urlNonepaper_urlNonecontributorsNoneartifact_idNonebenchmark_artifact_idpolaris/hello-world-benchmarkresultsTest setTarget labelMetricScoretestSOLmean_squared_error2.6875139821testSOLmean_absolute_error1.2735690161 <p>Before uploading the results to the Hub, you can provide some additional information about the results that will be displayed on the Polaris Hub.</p> In\u00a0[15]: Copied! <pre># For a complete list of meta-data, check out the BenchmarkResults object\nresults.name = \"hello-world-result\"\nresults.github_url = \"https://github.com/polaris-hub/polaris-hub\"\nresults.paper_url = \"https://polarishub.io/\"\nresults.description = \"Hello, World!\"\n</pre> # For a complete list of meta-data, check out the BenchmarkResults object results.name = \"hello-world-result\" results.github_url = \"https://github.com/polaris-hub/polaris-hub\" results.paper_url = \"https://polarishub.io/\" results.description = \"Hello, World!\" <p>Finally, let's upload the results to the Hub! The result will be private, but visiting the link in the logs you can decide to make it public through the Hub.</p> In\u00a0[16]: Copied! <pre>client.upload_results(results, owner=\"cwognum\")\nclient.close()\n</pre> client.upload_results(results, owner=\"cwognum\") client.close() <p>That's it! Just like that you have partaken in your first Polaris benchmark. In next tutorials, we will consider more advanced use cases of Polaris, such as creating and uploading your own datasets and benchmarks.</p> <p>The End.</p>"},{"location":"tutorials/basics.html#the-basics","title":"The Basics\u00b6","text":"<p>In short</p> <p>This tutorial walks you through the basic usage of Polaris. We will first login to the hub and will then see how easy it is to load a dataset or benchmark from it. Finally, we will train a simple baseline to submit a first set of results!</p> <p>Polaris is designed to standardize the process of constructing datasets, specifying benchmarks and evaluating novel machine learning techniques within the realm of drug discovery.</p> <p>While the Polaris library can be used independently from the Polaris Hub, the two were designed to seamlessly work together. The hub provides various pre-made, high quality datasets and benchmarks to develop and evaluate novel ML methods. In this tutorial, we will see how easy it is to load and use these datasets and benchmarks.</p>"},{"location":"tutorials/basics.html#login","title":"Login\u00b6","text":"<p>To be able to complete this step, you will require a Polaris Hub account. Go to https://polarishub.io/ to create one. You only have to log in once at the start or when you haven't used your account in a while.</p>"},{"location":"tutorials/basics.html#load-from-the-hub","title":"Load from the Hub\u00b6","text":"<p>Both datasets and benchmarks are identified by a <code>owner/name</code> id. You can easily find and copy these through the Hub. Once you have the id, loading a dataset or benchmark is incredibly easy.</p>"},{"location":"tutorials/basics.html#use-the-benchmark","title":"Use the benchmark\u00b6","text":"<p>The polaris library is designed to make it easy to participate in a benchmark. In just a few lines of code, we can get the train and test partition, access the associated data in various ways and evaluate our predictions. There's two main API endpoints.</p> <ul> <li><code>get_train_test_split()</code>: For creating objects through which we can access the different dataset partitions.</li> <li><code>evaluate()</code>: For evaluating a set of predictions in accordance with the benchmark protocol.</li> </ul>"},{"location":"tutorials/basics.html#partake-in-the-benchmark","title":"Partake in the benchmark\u00b6","text":"<p>To complete our example, let's participate in the benchmark. We will train a simple random forest model on the ECFP representation through scikit-learn and datamol.</p>"},{"location":"tutorials/competition.participate.html","title":"Participating in a Competition","text":"In\u00a0[\u00a0]: Copied! <pre>import polaris as po\nfrom polaris.hub.client import PolarisHubClient\n\n# Don't forget to add your Polaris Hub username below!\nMY_POLARIS_USERNAME = \"\"\n\nclient = PolarisHubClient()\nclient.login()\n</pre> import polaris as po from polaris.hub.client import PolarisHubClient  # Don't forget to add your Polaris Hub username below! MY_POLARIS_USERNAME = \"\"  client = PolarisHubClient() client.login() In\u00a0[10]: Copied! <pre>competition_id = \"polaris/hello-world-competition\"\ncompetition = po.load_competition(competition_id)\n</pre> competition_id = \"polaris/hello-world-competition\" competition = po.load_competition(competition_id) In\u00a0[\u00a0]: Copied! <pre>train, test = competition.get_train_test_split()\n</pre> train, test = competition.get_train_test_split() <p>Similar to benchmarks, the created test and train objects support various flavours to access the data.</p> In\u00a0[\u00a0]: Copied! <pre># The objects are iterable\nfor x, y in train:\n    pass\n\n# The objects can be indexed\nfor i in range(len(train)):\n    x, y = train[i]\n\n# The objects have properties to access all data at once. Use this with\n# caution if the underlying dataset is large!\nx = train.inputs\ny = train.targets\n</pre> # The objects are iterable for x, y in train:     pass  # The objects can be indexed for i in range(len(train)):     x, y = train[i]  # The objects have properties to access all data at once. Use this with # caution if the underlying dataset is large! x = train.inputs y = train.targets <p>Now, let's create some predictions against the imaginary <code>hello-world-competition</code>. Let's assume we train a simple random forest model on the ECFP representation through scikit-learn and datamol, and then we submit our results for secure evaluation by the Polaris Hub.</p> In\u00a0[\u00a0]: Copied! <pre>import datamol as dm\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Load the competition (automatically loads the underlying dataset as well)\ncompetition = po.load_competition(\"polaris/hello-world-benchmark\")\n\n# Get the split and convert SMILES to ECFP fingerprints by specifying a featurize function.\ntrain, test = competition.get_train_test_split(featurization_fn=dm.to_fp)\n\n# Define a model and train\nmodel = RandomForestRegressor(max_depth=2, random_state=0)\nmodel.fit(train.X, train.y)\n\npredictions = model.predict(test.X)\n</pre> import datamol as dm from sklearn.ensemble import RandomForestRegressor  # Load the competition (automatically loads the underlying dataset as well) competition = po.load_competition(\"polaris/hello-world-benchmark\")  # Get the split and convert SMILES to ECFP fingerprints by specifying a featurize function. train, test = competition.get_train_test_split(featurization_fn=dm.to_fp)  # Define a model and train model = RandomForestRegressor(max_depth=2, random_state=0) model.fit(train.X, train.y)  predictions = model.predict(test.X) <p>Now that we have created some predictions, we can construct a <code>CompetitionPredictions</code> object that will prepare our predictions for evaluation by the Polaris Hub. Here, you can also add metadata to your predictions to better describe your results and how you achieved them.</p> In\u00a0[\u00a0]: Copied! <pre>from polaris.evaluate import CompetitionPredictions\n\ncompetition_predictions = CompetitionPredictions(\n    name=\"hello-world-result\",\n    predictions=predictions,\n    target_labels=competition.target_cols,\n    test_set_labels=competition.test_set_labels,\n    test_set_sizes=competition.test_set_sizes,\n    github_url=\"https://github.com/polaris-hub/polaris-hub\",\n    paper_url=\"https://polarishub.io/\",\n    description=\"Hello, World!\",\n)\n</pre> from polaris.evaluate import CompetitionPredictions  competition_predictions = CompetitionPredictions(     name=\"hello-world-result\",     predictions=predictions,     target_labels=competition.target_cols,     test_set_labels=competition.test_set_labels,     test_set_sizes=competition.test_set_sizes,     github_url=\"https://github.com/polaris-hub/polaris-hub\",     paper_url=\"https://polarishub.io/\",     description=\"Hello, World!\", ) <p>Once your <code>CompetitionPredictions</code> object is created, you're ready to submit them for evaluation! This will automatically save your result to the Polaris Hub, but it will be private until the competition closes.</p> In\u00a0[\u00a0]: Copied! <pre>results = competition.evaluate(competition_predictions)\n\nclient.close()\n</pre> results = competition.evaluate(competition_predictions)  client.close() <p>That's it! Just like that you have partaken in your first Polaris competition. Keep an eye on that leaderboard when it goes public and best of luck in your future competitions!</p> <p>The End.</p>"},{"location":"tutorials/competition.participate.html#participating-in-a-competition","title":"Participating in a Competition\u00b6","text":"<p>In short</p> <p>This tutorial walks you through how to fetch an active competition from Polaris, prepare your predictions and then submit them for secure evaluation by the Polaris Hub.</p> <p>Participating in a competition on Polaris is very similar to participating in a standard benchmark. The main difference lies in how predictions are prepared and how they are evaluated. We'll touch on each of these topics later in the tutorial.</p> <p>Before continuing, please ensure you are logged into Polaris.</p>"},{"location":"tutorials/competition.participate.html#fetching-a-competition","title":"Fetching a Competition\u00b6","text":"<p>As with standard benchmarks, Polaris provides simple APIs that allow you to quickly fetch a competition from the Polaris Hub. All you need is the unique identifier for the competition which follows the format of <code>competition_owner</code>/<code>competition_name</code>.</p>"},{"location":"tutorials/competition.participate.html#participate-in-the-competition","title":"Participate in the Competition\u00b6","text":"<p>The Polaris library is designed to make it easy to participate in a competition. In just a few lines of code, we can get the train and test partition, access the associated data in various ways and evaluate our predictions. There's two main API endpoints.</p> <ul> <li><code>get_train_test_split()</code>: For creating objects through which we can access the different dataset partitions.</li> <li><code>submit_predictions()</code>: For submitting the predictions to an active competition.</li> </ul>"},{"location":"tutorials/custom_dataset_benchmark.html","title":"Data Models","text":"<p>In short</p> <p>This tutorial walks you through the dataset and benchmark data-structures. After creating our own custom dataset and benchmark, we will learn how to upload it to the Hub!</p> <p>We have already seen how easy it is to load a benchmark or dataset from the Polaris Hub. Let's now learn a bit more about the underlying data model by creating our own dataset and benchmark!</p> In\u00a0[3]: Copied! <pre>import pandas as pd\n\nPATH = (\n    \"https://raw.githubusercontent.com/molecularinformatics/Computational-ADME/main/ADME_public_set_3521.csv\"\n)\ntable = pd.read_csv(PATH)\ntable.head(5)\n</pre> import pandas as pd  PATH = (     \"https://raw.githubusercontent.com/molecularinformatics/Computational-ADME/main/ADME_public_set_3521.csv\" ) table = pd.read_csv(PATH) table.head(5) Out[3]: Internal ID Vendor ID SMILES CollectionName LOG HLM_CLint (mL/min/kg) LOG MDR1-MDCK ER (B-A/A-B) LOG SOLUBILITY PH 6.8 (ug/mL) LOG PLASMA PROTEIN BINDING (HUMAN) (% unbound) LOG PLASMA PROTEIN BINDING (RAT) (% unbound) LOG RLM_CLint (mL/min/kg) 0 Mol1 317714313 CNc1cc(Nc2cccn(-c3ccccn3)c2=O)nn2c(C(=O)N[C@@H... emolecules 0.675687 1.493167 0.089905 0.991226 0.518514 1.392169 1 Mol2 324056965 CCOc1cc2nn(CCC(C)(C)O)cc2cc1NC(=O)c1cccc(C(F)F)n1 emolecules 0.675687 1.040780 0.550228 0.099681 0.268344 1.027920 2 Mol3 304005766 CN(c1ncc(F)cn1)[C@H]1CCCNC1 emolecules 0.675687 -0.358806 NaN 2.000000 2.000000 1.027920 3 Mol4 194963090 CC(C)(Oc1ccc(-c2cnc(N)c(-c3ccc(Cl)cc3)c2)cc1)C... emolecules 0.675687 1.026662 1.657056 -1.158015 -1.403403 1.027920 4 Mol5 324059015 CC(C)(O)CCn1cc2cc(NC(=O)c3cccc(C(F)(F)F)n3)c(C... emolecules 0.996380 1.010597 NaN 1.015611 1.092264 1.629093 <p>While not required, a good dataset will specify additional meta-data to give further explanations on the data is contained within the dataset. This can be done on both the column level and on the dataset level.</p> In\u00a0[4]: Copied! <pre>from polaris.dataset import ColumnAnnotation\n\n# Additional meta-data on the column level\n# Of course, for a real dataset we should annotate all columns.\nannotations = {\n    \"LOG HLM_CLint (mL/min/kg)\": ColumnAnnotation(\n        desription=\"Microsomal stability\",\n        user_attributes={\"unit\": \"mL/min/kg\"},\n    ),\n    \"SMILES\": ColumnAnnotation(desription=\"Molecule SMILES string\", modality=\"molecule\"),\n}\n</pre> from polaris.dataset import ColumnAnnotation  # Additional meta-data on the column level # Of course, for a real dataset we should annotate all columns. annotations = {     \"LOG HLM_CLint (mL/min/kg)\": ColumnAnnotation(         desription=\"Microsomal stability\",         user_attributes={\"unit\": \"mL/min/kg\"},     ),     \"SMILES\": ColumnAnnotation(desription=\"Molecule SMILES string\", modality=\"molecule\"), } In\u00a0[5]: Copied! <pre>from polaris.dataset import Dataset\nfrom polaris.utils.types import HubOwner\n\ndataset = Dataset(\n    # The table is the core data-structure required to construct a dataset\n    table=table,\n    # Additional meta-data on the dataset level.\n    name=\"Fang_2023_DMPK\",\n    description=\"120 prospective data sets, collected over 20 months across six ADME in vitro endpoints\",\n    source=\"https://doi.org/10.1021/acs.jcim.3c00160\",\n    annotations=annotations,\n    tags=[\"DMPK\", \"ADME\"],\n    owner=HubOwner(user_id=\"cwognum\", slug=\"cwognum\"),\n    license=\"CC-BY-4.0\",\n    user_attributes={\"year\": \"2023\"},\n)\n</pre> from polaris.dataset import Dataset from polaris.utils.types import HubOwner  dataset = Dataset(     # The table is the core data-structure required to construct a dataset     table=table,     # Additional meta-data on the dataset level.     name=\"Fang_2023_DMPK\",     description=\"120 prospective data sets, collected over 20 months across six ADME in vitro endpoints\",     source=\"https://doi.org/10.1021/acs.jcim.3c00160\",     annotations=annotations,     tags=[\"DMPK\", \"ADME\"],     owner=HubOwner(user_id=\"cwognum\", slug=\"cwognum\"),     license=\"CC-BY-4.0\",     user_attributes={\"year\": \"2023\"}, ) In\u00a0[6]: Copied! <pre>import tempfile\n\ntemp_dir = tempfile.TemporaryDirectory().name\n</pre> import tempfile  temp_dir = tempfile.TemporaryDirectory().name In\u00a0[7]: Copied! <pre>import datamol as dm\n\nsave_dir = dm.fs.join(temp_dir, \"dataset\")\n</pre> import datamol as dm  save_dir = dm.fs.join(temp_dir, \"dataset\") In\u00a0[8]: Copied! <pre>path = dataset.to_json(save_dir)\n</pre> path = dataset.to_json(save_dir) <p>Looking at the save destination, we see this created two files: A JSON with all the meta-data and a <code>.parquet</code> file with the tabular data.</p> In\u00a0[9]: Copied! <pre>fs = dm.fs.get_mapper(save_dir).fs\nfs.ls(save_dir)\n</pre> fs = dm.fs.get_mapper(save_dir).fs fs.ls(save_dir) Out[9]: <pre>['/var/folders/1y/1v1blh6x56zdn027g5g9bwph0000gr/T/tmpe_g26lrl/dataset/table.parquet',\n '/var/folders/1y/1v1blh6x56zdn027g5g9bwph0000gr/T/tmpe_g26lrl/dataset/dataset.json']</pre> <p>Loading the dataset can be done through this JSON file.</p> In\u00a0[10]: Copied! <pre>import polaris as po\n\ndataset = po.load_dataset(path)\n</pre> import polaris as po  dataset = po.load_dataset(path) <p>We can also upload the dataset to the hub!</p> In\u00a0[11]: Copied! <pre># from polaris.hub.client import PolarisHubClient\n\n# NOTE: Commented out to not flood the DB\n# with PolarisHubClient() as client:\n#     client.upload_dataset(dataset=dataset)\n</pre> # from polaris.hub.client import PolarisHubClient  # NOTE: Commented out to not flood the DB # with PolarisHubClient() as client: #     client.upload_dataset(dataset=dataset) In\u00a0[12]: Copied! <pre>import numpy as np\nfrom polaris.benchmark import SingleTaskBenchmarkSpecification\n\n# For the sake of simplicity, we use a very simple, ordered split\nsplit = (np.arange(3000).tolist(), (np.arange(521) + 3000).tolist())  # train  # test\n\nbenchmark = SingleTaskBenchmarkSpecification(\n    dataset=dataset,\n    target_cols=\"LOG SOLUBILITY PH 6.8 (ug/mL)\",\n    input_cols=\"SMILES\",\n    split=split,\n    metrics=\"mean_absolute_error\",\n)\n</pre> import numpy as np from polaris.benchmark import SingleTaskBenchmarkSpecification  # For the sake of simplicity, we use a very simple, ordered split split = (np.arange(3000).tolist(), (np.arange(521) + 3000).tolist())  # train  # test  benchmark = SingleTaskBenchmarkSpecification(     dataset=dataset,     target_cols=\"LOG SOLUBILITY PH 6.8 (ug/mL)\",     input_cols=\"SMILES\",     split=split,     metrics=\"mean_absolute_error\", ) <p>Metrics should be supported in the polaris framework.</p> <p>For more information, see the <code>Metric</code> class.</p> <p>To support the vast flexibility in specifying a benchmark, we have different classes that correspond to different types of benchmarks. Each of these subclasses makes the data-model or logic more specific to a particular case. For example, trying to create a multitask benchmark with the same arguments as we used above will throw an error as there is just a single target column specified.</p> In\u00a0[14]: Copied! <pre>from polaris.benchmark import MultiTaskBenchmarkSpecification\n\nbenchmark = MultiTaskBenchmarkSpecification(\n    dataset=dataset,\n    target_cols=\"LOG SOLUBILITY PH 6.8 (ug/mL)\",\n    input_cols=\"SMILES\",\n    split=split,\n    metrics=\"mean_absolute_error\",\n)\n</pre> from polaris.benchmark import MultiTaskBenchmarkSpecification  benchmark = MultiTaskBenchmarkSpecification(     dataset=dataset,     target_cols=\"LOG SOLUBILITY PH 6.8 (ug/mL)\",     input_cols=\"SMILES\",     split=split,     metrics=\"mean_absolute_error\", ) <pre>\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\n/Users/cas.wognum/Documents/repositories/polaris/docs/tutorials/custom_dataset_benchmark.ipynb Cell 25 line 3\n      &lt;a href='vscode-notebook-cell:/Users/cas.wognum/Documents/repositories/polaris/docs/tutorials/custom_dataset_benchmark.ipynb#X33sZmlsZQ%3D%3D?line=0'&gt;1&lt;/a&gt; from polaris.benchmark import MultiTaskBenchmarkSpecification\n----&gt; &lt;a href='vscode-notebook-cell:/Users/cas.wognum/Documents/repositories/polaris/docs/tutorials/custom_dataset_benchmark.ipynb#X33sZmlsZQ%3D%3D?line=2'&gt;3&lt;/a&gt; benchmark = MultiTaskBenchmarkSpecification(\n      &lt;a href='vscode-notebook-cell:/Users/cas.wognum/Documents/repositories/polaris/docs/tutorials/custom_dataset_benchmark.ipynb#X33sZmlsZQ%3D%3D?line=3'&gt;4&lt;/a&gt;     dataset=dataset,\n      &lt;a href='vscode-notebook-cell:/Users/cas.wognum/Documents/repositories/polaris/docs/tutorials/custom_dataset_benchmark.ipynb#X33sZmlsZQ%3D%3D?line=4'&gt;5&lt;/a&gt;     target_cols=\"LOG SOLUBILITY PH 6.8 (ug/mL)\",\n      &lt;a href='vscode-notebook-cell:/Users/cas.wognum/Documents/repositories/polaris/docs/tutorials/custom_dataset_benchmark.ipynb#X33sZmlsZQ%3D%3D?line=5'&gt;6&lt;/a&gt;     input_cols=\"SMILES\",\n      &lt;a href='vscode-notebook-cell:/Users/cas.wognum/Documents/repositories/polaris/docs/tutorials/custom_dataset_benchmark.ipynb#X33sZmlsZQ%3D%3D?line=6'&gt;7&lt;/a&gt;     split=split,\n      &lt;a href='vscode-notebook-cell:/Users/cas.wognum/Documents/repositories/polaris/docs/tutorials/custom_dataset_benchmark.ipynb#X33sZmlsZQ%3D%3D?line=7'&gt;8&lt;/a&gt;     metrics=\"mean_absolute_error\",\n      &lt;a href='vscode-notebook-cell:/Users/cas.wognum/Documents/repositories/polaris/docs/tutorials/custom_dataset_benchmark.ipynb#X33sZmlsZQ%3D%3D?line=8'&gt;9&lt;/a&gt; )\n\nFile ~/micromamba/envs/polaris/lib/python3.11/site-packages/pydantic/main.py:164, in BaseModel.__init__(__pydantic_self__, **data)\n    162 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    163 __tracebackhide__ = True\n--&gt; 164 __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)\n\nValidationError: 1 validation error for MultiTaskBenchmarkSpecification\ntarget_cols\n  Value error, A multi-task benchmark should specify at least two target columns [type=value_error, input_value='LOG SOLUBILITY PH 6.8 (ug/mL)', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error</pre> In\u00a0[15]: Copied! <pre>save_dir = dm.fs.join(temp_dir, \"benchmark\")\n</pre> save_dir = dm.fs.join(temp_dir, \"benchmark\") In\u00a0[16]: Copied! <pre>path = benchmark.to_json(save_dir)\n</pre> path = benchmark.to_json(save_dir) In\u00a0[17]: Copied! <pre>fs = dm.fs.get_mapper(save_dir).fs\nfs.ls(save_dir)\n</pre> fs = dm.fs.get_mapper(save_dir).fs fs.ls(save_dir) Out[17]: <pre>['/var/folders/1y/1v1blh6x56zdn027g5g9bwph0000gr/T/tmpe_g26lrl/benchmark/table.parquet',\n '/var/folders/1y/1v1blh6x56zdn027g5g9bwph0000gr/T/tmpe_g26lrl/benchmark/benchmark.json',\n '/var/folders/1y/1v1blh6x56zdn027g5g9bwph0000gr/T/tmpe_g26lrl/benchmark/dataset.json']</pre> <p>This created three files. Two <code>json</code> files and a single <code>parquet</code> file. The <code>parquet</code> file saves the tabular structure at the base of the <code>Dataset</code> class, whereas the <code>json</code> files save all the meta-data for the <code>Dataset</code> and <code>BenchmarkSpecification</code>.</p> <p>As before, loading the benchmark can be done through the JSON file.</p> In\u00a0[18]: Copied! <pre>benchmark = po.load_benchmark(path)\n</pre> benchmark = po.load_benchmark(path) <p>And as before, we can also upload the benchmark directly to the hub.</p> In\u00a0[19]: Copied! <pre># NOTE: Commented out to not flood the DB\n# with PolarisHubClient() as client:\n#     client.upload_benchmark(dataset=dataset)\n</pre> # NOTE: Commented out to not flood the DB # with PolarisHubClient() as client: #     client.upload_benchmark(dataset=dataset) <p>The End.</p>"},{"location":"tutorials/custom_dataset_benchmark.html#create-the-dataset","title":"Create the dataset\u00b6","text":"<p>A dataset in Polaris is at its core a tabular data-structure in which each row stores a single datapoint. For this example, we will process a multi-task DMPK dataset from Fang et al.. For the sake of simplicity, we don't do any curation and will just download the dataset as-is from their Github.</p> <p>The importance of curation</p> <p>While we do not address it in this tutorial, data curation is essential to an impactful benchmark. Because of this, we have not just made several high-quality benchmarks readily available on the Polaris Hub, but also open-sourced some of the tools we've built to curate these datasets.</p>"},{"location":"tutorials/custom_dataset_benchmark.html#save-and-load-the-dataset","title":"Save and load the dataset\u00b6","text":"<p>We can now save the dataset either to a local path or directly to the hub!</p>"},{"location":"tutorials/custom_dataset_benchmark.html#create-the-benchmark-specification","title":"Create the benchmark specification\u00b6","text":"<p>A benchmark is represented by the <code>BenchmarkSpecification</code>, which wraps a <code>Dataset</code> with additional data to produce a benchmark.</p> <p>It specifies:</p> <ol> <li>Which dataset to use (see Dataset);</li> <li>Which columns are used as input and which columns are used as target;</li> <li>Which metrics should be used to evaluate performance on this task;</li> <li>A predefined, static train-test split to use during evaluation.</li> </ol>"},{"location":"tutorials/custom_dataset_benchmark.html#save-and-load-the-benchmark","title":"Save and load the benchmark\u00b6","text":"<p>Saving the benchmark is easy and can be done with a single line of code.</p>"},{"location":"tutorials/dataset_pdb.html","title":"PDB Datasets","text":"<p>In short</p> <p>This tutorial shows how to create datasets with PDBs through the .zarr format.</p> <p>This feature is still very new.</p> <p>The features we will show in this tutorial are still experimental. We would love to learn from the community how we can make it easier to create datasets.</p> In\u00a0[1]: Copied! <pre>import platformdirs\n\nimport datamol as dm\n\nfrom polaris.dataset import DatasetFactory\nfrom polaris.dataset.converters import PDBConverter\n\nSAVE_DIR = dm.fs.join(platformdirs.user_cache_dir(appname=\"polaris-tutorials\"), \"dataset_pdb\")\n</pre> import platformdirs  import datamol as dm  from polaris.dataset import DatasetFactory from polaris.dataset.converters import PDBConverter  SAVE_DIR = dm.fs.join(platformdirs.user_cache_dir(appname=\"polaris-tutorials\"), \"dataset_pdb\") In\u00a0[\u00a0]: Copied! <pre>import biotite.database.rcsb as rcsb\n\npdb_path = rcsb.fetch(\"6s89\", \"pdb\", SAVE_DIR)\nprint(pdb_path)\n</pre> import biotite.database.rcsb as rcsb  pdb_path = rcsb.fetch(\"6s89\", \"pdb\", SAVE_DIR) print(pdb_path) In\u00a0[14]: Copied! <pre>save_dst = dm.fs.join(SAVE_DIR, \"tutorial_pdb.zarr\")\n\nfactory = DatasetFactory(zarr_root_path=save_dst)\nfactory.reset(save_dst)\n\nfactory.register_converter(\"pdb\", PDBConverter(pdb_column=\"pdb\"))\nfactory.add_from_file(pdb_path)\n\n# Build the dataset\ndataset = factory.build()\n</pre> save_dst = dm.fs.join(SAVE_DIR, \"tutorial_pdb.zarr\")  factory = DatasetFactory(zarr_root_path=save_dst) factory.reset(save_dst)  factory.register_converter(\"pdb\", PDBConverter(pdb_column=\"pdb\")) factory.add_from_file(pdb_path)  # Build the dataset dataset = factory.build() In\u00a0[15]: Copied! <pre>dataset\n</pre> dataset Out[15]: nameNonedescriptiontagsuser_attributesownerNonepolaris_version0.7.10.dev22+g8edf177.d20240814default_adapterspdbARRAY_TO_PDBzarr_root_path/Users/lu.zhu/Library/Caches/polaris-tutorials/002/tutorial_pdb.zarrreadmeannotationspdbis_pointerTruemodalityPROTEIN_3DdescriptionNoneuser_attributesdtypeobjectsourceNonelicenseNonecuration_referenceNonecache_dir/Users/lu.zhu/Library/Caches/polaris/datasets/b0895f92-5a11-4e48-953f-3f969c6a9ca6md5sum66f3c7774e655bc6d48c907100d6912fartifact_idNonen_rows1n_columns1 In\u00a0[16]: Copied! <pre>dataset.table\n</pre> dataset.table Out[16]: pdb 0 pdb/6s89 In\u00a0[\u00a0]: Copied! <pre>dataset.get_data(0, \"pdb\")\n</pre> dataset.get_data(0, \"pdb\") In\u00a0[7]: Copied! <pre>pdb_paths = rcsb.fetch([\"1l2y\", \"4i23\"], \"pdb\", SAVE_DIR)\nprint(pdb_paths)\n</pre> pdb_paths = rcsb.fetch([\"1l2y\", \"4i23\"], \"pdb\", SAVE_DIR) print(pdb_paths) <pre>['/Users/lu.zhu/Library/Caches/polaris-tutorials/002/1l2y.pdb', '/Users/lu.zhu/Library/Caches/polaris-tutorials/002/4i23.pdb']\n</pre> In\u00a0[8]: Copied! <pre>factory = DatasetFactory(SAVE_DIR.join(\"pdbs.zarr\"))\n\nconverter = PDBConverter()\nfactory.register_converter(\"pdb\", converter)\n\nfactory.add_from_files(pdb_paths, axis=0)\ndataset = factory.build()\n</pre> factory = DatasetFactory(SAVE_DIR.join(\"pdbs.zarr\"))  converter = PDBConverter() factory.register_converter(\"pdb\", converter)  factory.add_from_files(pdb_paths, axis=0) dataset = factory.build() In\u00a0[9]: Copied! <pre>dataset.table\n</pre> dataset.table Out[9]: pdb 0 pdb/1l2y 1 pdb/4i23 In\u00a0[\u00a0]: Copied! <pre>dataset.get_data(1, \"pdb\")\n</pre> dataset.get_data(1, \"pdb\") <p>The process of completing the dataset's metadata and uploading it to the hub follows the same steps as outlined in the tutorial dataset_zarr.ipynb</p> <p>The End.</p>"},{"location":"tutorials/dataset_pdb.html#dummy-pdb-example","title":"Dummy PDB example\u00b6","text":""},{"location":"tutorials/dataset_pdb.html#fetch-pdb-files-from-rcsb-pdb","title":"Fetch PDB files from RCSB PDB\u00b6","text":""},{"location":"tutorials/dataset_pdb.html#create-dataset-from-pdb-file","title":"Create dataset from PDB file\u00b6","text":""},{"location":"tutorials/dataset_pdb.html#check-the-dataset","title":"Check the dataset\u00b6","text":""},{"location":"tutorials/dataset_pdb.html#check-data-table","title":"Check data table\u00b6","text":""},{"location":"tutorials/dataset_pdb.html#get-pdb-data-from-specific-row","title":"Get PDB data from specific row\u00b6","text":"<p>A array of list of <code>biotite.Atom</code> will be returned. See more details at fastpdb and Atom.</p>"},{"location":"tutorials/dataset_pdb.html#create-dataset-from-multiple-pdb-files","title":"Create dataset from multiple PDB files\u00b6","text":""},{"location":"tutorials/dataset_sdf.html","title":"SDF Datasets","text":"<p>In short</p> <p>This tutorial shows how we can create more complicated datasets with SDF file by leveraging the dataset factory in Polaris.</p> <p>This feature is still very new.</p> <p>The features we will show in this tutorial are still experimental. We would love to learn from the community how we can make it easier to create datasets.</p> In\u00a0[2]: Copied! <pre>import platformdirs\nimport datamol as dm\n</pre> import platformdirs import datamol as dm In\u00a0[3]: Copied! <pre>SAVE_DIR = dm.fs.join(platformdirs.user_cache_dir(appname=\"polaris-tutorials\"), \"dataset_sdf\")\n</pre> SAVE_DIR = dm.fs.join(platformdirs.user_cache_dir(appname=\"polaris-tutorials\"), \"dataset_sdf\") In\u00a0[4]: Copied! <pre># Let's generate a toy dataset with a single molecule\nsmiles = \"Cn1cnc2c1c(=O)n(C)c(=O)n2C\"\nmol = dm.to_mol(smiles)\n\n# We will generate 3D conformers for this molecule with some conformers\nmol = dm.conformers.generate(mol, align_conformers=True)\n\n# Let's also set a molecular property\nmol.SetProp(\"my_property\", \"my_value\")\n\nmol\n</pre> # Let's generate a toy dataset with a single molecule smiles = \"Cn1cnc2c1c(=O)n(C)c(=O)n2C\" mol = dm.to_mol(smiles)  # We will generate 3D conformers for this molecule with some conformers mol = dm.conformers.generate(mol, align_conformers=True)  # Let's also set a molecular property mol.SetProp(\"my_property\", \"my_value\")  mol Out[4]: my_propertymy_value In\u00a0[5]: Copied! <pre>path = dm.fs.join(SAVE_DIR, \"caffeine.sdf\")\ndm.to_sdf(mol, path)\n</pre> path = dm.fs.join(SAVE_DIR, \"caffeine.sdf\") dm.to_sdf(mol, path) <p>This being a toy example, it is a very small dataset. However, for many real-world datasets SDF files can quickly get large, at which point it is no longer efficient to store everything directly in the Pandas DataFrame. This is why Polaris supports pointer columns to store large data outside of the DataFrame in a Zarr archive. But... How to convert from SDF to Zarr?</p> <p>There are a lot of considerations here:</p> <ul> <li>You want read and write operations to be quick.</li> <li>You want to reduce the storage requirements.</li> <li>You want the conversion to be lossless.</li> </ul> <p>Chances are you've no in-depth understanding of how Zarr works, making it a big investment to convert your SDF dataset to Zarr.</p> <p><code>DatasetFactory</code> to the rescue!</p> In\u00a0[6]: Copied! <pre>from polaris.dataset import DatasetFactory\nfrom polaris.dataset.converters import SDFConverter\n\n# Create a new factory object\nsave_dst = dm.fs.join(SAVE_DIR, \"data.zarr\")\nfactory = DatasetFactory(zarr_root_path=save_dst)\n\n# Register a converter for the SDF file format\nfactory.register_converter(\"sdf\", SDFConverter())\n\n# Process your SDF file\nfactory.add_from_file(path)\n\n# Build the dataset\ndataset = factory.build()\n</pre> from polaris.dataset import DatasetFactory from polaris.dataset.converters import SDFConverter  # Create a new factory object save_dst = dm.fs.join(SAVE_DIR, \"data.zarr\") factory = DatasetFactory(zarr_root_path=save_dst)  # Register a converter for the SDF file format factory.register_converter(\"sdf\", SDFConverter())  # Process your SDF file factory.add_from_file(path)  # Build the dataset dataset = factory.build() <p>That's all! Let's take a closer look at what this has actually done.</p> In\u00a0[7]: Copied! <pre>dataset.annotations\n</pre> dataset.annotations Out[7]: <pre>{'smiles': ColumnAnnotation(is_pointer=False, modality=&lt;Modality.MOLECULE: 'molecule'&gt;, description=None, user_attributes={}, dtype=dtype('O')),\n 'my_property': ColumnAnnotation(is_pointer=False, modality=&lt;Modality.UNKNOWN: 'unknown'&gt;, description=None, user_attributes={}, dtype=dtype('O')),\n 'molecule': ColumnAnnotation(is_pointer=True, modality=&lt;Modality.MOLECULE_3D: 'molecule_3D'&gt;, description=None, user_attributes={}, dtype=dtype('O'))}</pre> In\u00a0[8]: Copied! <pre>dataset.get_data(row=0, col=\"molecule\")\n</pre> dataset.get_data(row=0, col=\"molecule\") Out[8]: <p>We can see that Polaris has:</p> <ul> <li>Saved the molecule in an external Zarr archive and set the column annotations accordingly.</li> <li>Has extracted the molecule-level properties as additional columns.</li> <li>Has added an additional column with the SMILES.</li> <li>Effortlessly saves and loads the molecule object from the Zarr.</li> </ul> In\u00a0[9]: Copied! <pre>from polaris.dataset import create_dataset_from_file\n\ndataset = create_dataset_from_file(path, save_dst)\ndataset.get_data(row=0, col=\"molecule\")\n</pre> from polaris.dataset import create_dataset_from_file  dataset = create_dataset_from_file(path, save_dst) dataset.get_data(row=0, col=\"molecule\") Out[9]: <p>The <code>DatasetFactory</code> is based on the factory design pattern. That way, you can easily create and add your own file converters. However, the defaults are set to be a good option for most people.</p> <p>Let's consider two cases that show the power of the <code>DatasetFactory</code> design.</p> In\u00a0[10]: Copied! <pre>save_dst = dm.fs.join(SAVE_DIR, \"data2.zarr\")\nfactory.reset(save_dst)\n\n# Configure the converter\nconverter = SDFConverter(mol_prop_as_cols=False)\n\n# Overwrite the converter for SDF files\nfactory.register_converter(\"sdf\", converter)\n\n# Process the SDF file again\nfactory.add_from_file(path)\n\n# Build the dataset\ndataset = factory.build()\n</pre> save_dst = dm.fs.join(SAVE_DIR, \"data2.zarr\") factory.reset(save_dst)  # Configure the converter converter = SDFConverter(mol_prop_as_cols=False)  # Overwrite the converter for SDF files factory.register_converter(\"sdf\", converter)  # Process the SDF file again factory.add_from_file(path)  # Build the dataset dataset = factory.build() <pre>2024-03-26 13:16:43.897 | INFO     | polaris.dataset._factory:register_converter:112 - You are overwriting the converter for the sdf extension.\n</pre> <p>And voila! The property is saved to the Zarr instead of to a separate column.</p> In\u00a0[11]: Copied! <pre>dataset.get_data(row=0, col=\"molecule\")\n</pre> dataset.get_data(row=0, col=\"molecule\") Out[11]: my_propertymy_value In\u00a0[12]: Copied! <pre>dataset.table\n</pre> dataset.table Out[12]: smiles molecule 0 CN1C=NC2=C1C(=O)N(C)C(=O)N2C molecule#0 In\u00a0[13]: Copied! <pre>save_dst = dm.fs.join(SAVE_DIR, \"data3.zarr\")\nfactory.reset(save_dst)\n\n# Let's pretend these are two different SDF files\nfactory.register_converter(\"sdf\", SDFConverter(mol_column=\"molecule1\", smiles_column=None))\nfactory.add_from_file(path)\n\n# We change the configuration between files\nfactory.register_converter(\"sdf\", SDFConverter(mol_column=\"molecule2\", mol_prop_as_cols=False))\nfactory.add_from_file(path)\n\ndataset = factory.build()\n</pre> save_dst = dm.fs.join(SAVE_DIR, \"data3.zarr\") factory.reset(save_dst)  # Let's pretend these are two different SDF files factory.register_converter(\"sdf\", SDFConverter(mol_column=\"molecule1\", smiles_column=None)) factory.add_from_file(path)  # We change the configuration between files factory.register_converter(\"sdf\", SDFConverter(mol_column=\"molecule2\", mol_prop_as_cols=False)) factory.add_from_file(path)  dataset = factory.build() <pre>2024-03-26 13:16:43.938 | INFO     | polaris.dataset._factory:register_converter:112 - You are overwriting the converter for the sdf extension.\n2024-03-26 13:16:43.945 | INFO     | polaris.dataset._factory:register_converter:112 - You are overwriting the converter for the sdf extension.\n</pre> In\u00a0[14]: Copied! <pre>dataset.table\n</pre> dataset.table Out[14]: my_property molecule1 smiles molecule2 0 my_value molecule1#0 CN1C=NC2=C1C(=O)N(C)C(=O)N2C molecule2#0 <p>The End.</p>"},{"location":"tutorials/dataset_sdf.html#dataset-factory","title":"Dataset Factory\u00b6","text":"<p>Datasets in Polaris are expected to be saved in a very specific format. This format has been carefully designed to be as universal and performant as possible. Nevertheless, we expect very few datasets to be readily available in this format. We therefore provide the <code>DatasetFactory</code> as a way to more easily convert datasets to the Polaris specific format.</p> <p>Let's assume we have a dataset in the SDF format.</p>"},{"location":"tutorials/dataset_sdf.html#factory-design-pattern","title":"Factory Design Pattern\u00b6","text":"<p>If you've been dilligently going through the tutorials, you might remember that there is a function that seems to be doing something similar. And you would be right!</p>"},{"location":"tutorials/dataset_sdf.html#configuring-the-converter","title":"Configuring the converter\u00b6","text":"<p>Let's assume we do not want to extract the properties as separate columns, but rather keep them in the RDKit object. We cannot do this with the default converter, but we can configure its behavior to achieve this.</p>"},{"location":"tutorials/dataset_sdf.html#merging-data-from-different-sources","title":"Merging data from different sources\u00b6","text":"<p>Another case is when you want to merge data from multiple sources. Maybe you have two different SDF files.</p>"},{"location":"tutorials/dataset_zarr.html","title":"Zarr Datasets","text":"<p>In short</p> <p>This tutorial shows how to create datasets with more advanced data-modalities through the .zarr format.</p> In\u00a0[2]: Copied! <pre>import zarr\nimport platformdirs\n\nimport numpy as np\nimport datamol as dm\nimport pandas as pd\n\nSAVE_DIR = dm.fs.join(platformdirs.user_cache_dir(appname=\"polaris-tutorials\"), \"dataset_zarr\")\n</pre> import zarr import platformdirs  import numpy as np import datamol as dm import pandas as pd  SAVE_DIR = dm.fs.join(platformdirs.user_cache_dir(appname=\"polaris-tutorials\"), \"dataset_zarr\") <pre>/mnt/ps/home/CORP/lu.zhu/miniconda3/envs/po_datasets/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[3]: Copied! <pre># Create two images and save them to a Zarr archive\nbase_path = dm.fs.join(SAVE_DIR, \"data.zarr\")\ninp_col_name = \"images\"\n\nimages = np.random.random((2, 64, 64, 3))\nroot = zarr.open(base_path, \"w\")\nroot.array(inp_col_name, images)\n</pre> # Create two images and save them to a Zarr archive base_path = dm.fs.join(SAVE_DIR, \"data.zarr\") inp_col_name = \"images\"  images = np.random.random((2, 64, 64, 3)) root = zarr.open(base_path, \"w\") root.array(inp_col_name, images) Out[3]: <pre>&lt;zarr.core.Array '/images' (2, 64, 64, 3) float64&gt;</pre> In\u00a0[4]: Copied! <pre># Consolidate the dataset for efficient loading from the cloud bucket\nzarr.consolidate_metadata(base_path)\n</pre> # Consolidate the dataset for efficient loading from the cloud bucket zarr.consolidate_metadata(base_path) Out[4]: <pre>&lt;zarr.hierarchy.Group '/'&gt;</pre> In\u00a0[5]: Copied! <pre># For performance reasons, Polaris expects all data related to a column to be saved in a single Zarr array.\n# To index a specific element in that array, the pointer path can have a suffix to specify the index.\ntrain_path = f\"{inp_col_name}#0\"\ntest_path = f\"{inp_col_name}#1\"\n</pre> # For performance reasons, Polaris expects all data related to a column to be saved in a single Zarr array. # To index a specific element in that array, the pointer path can have a suffix to specify the index. train_path = f\"{inp_col_name}#0\" test_path = f\"{inp_col_name}#1\" In\u00a0[6]: Copied! <pre>tgt_col_name = \"target\"\n\ntable = pd.DataFrame(\n    {\n        inp_col_name: [train_path, test_path],  # Instead of the content, we specify paths\n        tgt_col_name: np.random.random(2),\n    }\n)\n</pre> tgt_col_name = \"target\"  table = pd.DataFrame(     {         inp_col_name: [train_path, test_path],  # Instead of the content, we specify paths         tgt_col_name: np.random.random(2),     } ) In\u00a0[7]: Copied! <pre>from polaris.dataset import Dataset, ColumnAnnotation\n\ndataset = Dataset(\n    table=table,\n    # To indicate that we are dealing with a pointer column here,\n    # we need to annotate the column.\n    annotations={\"images\": ColumnAnnotation(is_pointer=True)},\n    # We also need to specify the path to the root of the Zarr archive\n    zarr_root_path=base_path,\n)\n</pre> from polaris.dataset import Dataset, ColumnAnnotation  dataset = Dataset(     table=table,     # To indicate that we are dealing with a pointer column here,     # we need to annotate the column.     annotations={\"images\": ColumnAnnotation(is_pointer=True)},     # We also need to specify the path to the root of the Zarr archive     zarr_root_path=base_path, ) <p>Note how the table does not contain the image data, but rather stores a path relative to the root of the Zarr.</p> In\u00a0[8]: Copied! <pre>dataset.table.loc[0, \"images\"]\n</pre> dataset.table.loc[0, \"images\"] Out[8]: <pre>'images#0'</pre> <p>To load the data that is being pointed to, you can simply use the <code>Dataset.get_data()</code> utility method.</p> In\u00a0[9]: Copied! <pre>dataset.get_data(col=\"images\", row=0).shape\n</pre> dataset.get_data(col=\"images\", row=0).shape Out[9]: <pre>(64, 64, 3)</pre> <p>Creating a benchmark and the associated <code>Subset</code> objects will automatically do so!</p> In\u00a0[10]: Copied! <pre>from polaris.benchmark import SingleTaskBenchmarkSpecification\n\nbenchmark = SingleTaskBenchmarkSpecification(\n    dataset=dataset,\n    input_cols=inp_col_name,\n    target_cols=tgt_col_name,\n    metrics=\"mean_absolute_error\",\n    split=([0], [1]),\n)\n</pre> from polaris.benchmark import SingleTaskBenchmarkSpecification  benchmark = SingleTaskBenchmarkSpecification(     dataset=dataset,     input_cols=inp_col_name,     target_cols=tgt_col_name,     metrics=\"mean_absolute_error\",     split=([0], [1]), ) In\u00a0[11]: Copied! <pre>train, test = benchmark.get_train_test_split()\n\nfor x, y in train:\n    # At this point, the content is loaded from the path specified in the table\n    print(x.shape)\n</pre> train, test = benchmark.get_train_test_split()  for x, y in train:     # At this point, the content is loaded from the path specified in the table     print(x.shape) <pre>(64, 64, 3)\n</pre> In\u00a0[12]: Copied! <pre># Let's first create some dummy dataset with 1000 64x64 \"images\"\nimages = np.random.random((1000, 64, 64, 3))\n</pre> # Let's first create some dummy dataset with 1000 64x64 \"images\" images = np.random.random((1000, 64, 64, 3)) In\u00a0[13]: Copied! <pre>path = dm.fs.join(SAVE_DIR, \"zarr\", \"data.zarr\")\n\nwith zarr.open(path, \"w\") as root:\n    root.array(inp_col_name, images)\n</pre> path = dm.fs.join(SAVE_DIR, \"zarr\", \"data.zarr\")  with zarr.open(path, \"w\") as root:     root.array(inp_col_name, images) <p>To create a dataset from a Zarr archive, we can use the convenience function <code>create_dataset_from_file()</code>.</p> In\u00a0[14]: Copied! <pre>from polaris.dataset import create_dataset_from_file\n\n# Because Polaris might restructure the Zarr archive,\n# we need to specify a location to save the Zarr file to.\ndataset = create_dataset_from_file(path, zarr_root_path=dm.fs.join(SAVE_DIR, \"zarr\", \"processed.zarr\"))\n\n# The path refers to the original zarr directory we created in the above code block\ndataset.table.iloc[0][inp_col_name]\n</pre> from polaris.dataset import create_dataset_from_file  # Because Polaris might restructure the Zarr archive, # we need to specify a location to save the Zarr file to. dataset = create_dataset_from_file(path, zarr_root_path=dm.fs.join(SAVE_DIR, \"zarr\", \"processed.zarr\"))  # The path refers to the original zarr directory we created in the above code block dataset.table.iloc[0][inp_col_name] Out[14]: <pre>'images#0'</pre> In\u00a0[15]: Copied! <pre>dataset.get_data(col=inp_col_name, row=0).shape\n</pre> dataset.get_data(col=inp_col_name, row=0).shape Out[15]: <pre>(64, 64, 3)</pre> In\u00a0[16]: Copied! <pre>savedir = dm.fs.join(SAVE_DIR, \"json\")\njson_path = dataset.to_json(savedir)\n</pre> savedir = dm.fs.join(SAVE_DIR, \"json\") json_path = dataset.to_json(savedir) <pre>2024-07-21 13:11:49.273 | INFO     | polaris._mixins:md5sum:27 - Computing the checksum. This can be slow for large datasets.\nFinding all files in the Zarr archive:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 79/131 [00:00&lt;00:00, 375.21it/s]</pre> <pre>Finding all files in the Zarr archive: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 131/131 [00:00&lt;00:00, 396.17it/s]\n2024-07-21 13:11:49.616 | INFO     | polaris.dataset._dataset:to_json:431 - Copying Zarr archive to /mnt/ps/home/CORP/lu.zhu/.cache/polaris-tutorials/002/json/data.zarr. This may take a while.\n</pre> In\u00a0[17]: Copied! <pre>fs = dm.fs.get_mapper(path).fs\nfs.ls(SAVE_DIR)\n</pre> fs = dm.fs.get_mapper(path).fs fs.ls(SAVE_DIR) Out[17]: <pre>['/mnt/ps/home/CORP/lu.zhu/.cache/polaris-tutorials/002/json',\n '/mnt/ps/home/CORP/lu.zhu/.cache/polaris-tutorials/002/data.zarr',\n '/mnt/ps/home/CORP/lu.zhu/.cache/polaris-tutorials/002/zarr']</pre> <p>Besides the <code>table.parquet</code> and <code>dataset.yaml</code>, we can now also see a <code>data</code> folder which stores the content for the additional content from the pointer columns.</p> In\u00a0[18]: Copied! <pre>Dataset.from_json(json_path)\n</pre> Dataset.from_json(json_path) <pre>2024-07-21 13:12:16.485 | INFO     | polaris._mixins:md5sum:27 - Computing the checksum. This can be slow for large datasets.\nFinding all files in the Zarr archive:  17%|\u2588\u258b        | 22/131 [00:00&lt;00:00, 211.62it/s]</pre> <pre>Finding all files in the Zarr archive: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 131/131 [00:00&lt;00:00, 246.81it/s]\n</pre> Out[18]: nameNonedescriptiontagsuser_attributesownerNonepolaris_versiondevdefault_adapterszarr_root_path/mnt/ps/home/CORP/lu.zhu/.cache/polaris-tutorials/002/zarr/processed.zarrreadmeannotationsimagesis_pointerTruemodalityUNKNOWNdescriptionNoneuser_attributesdtypeobjectsourceNonelicenseNonecuration_referenceNonecache_dir/mnt/ps/home/CORP/lu.zhu/.cache/polaris/datasets/97d642a2-001c-40aa-ac98-0e24353005d2md5sumb7c52acfbda1f9bba47ae218e9c4717fartifact_idNonen_rows1000n_columns1 In\u00a0[22]: Copied! <pre># Define the zarr dataset metadata before uploading\ndataset.name = \"tutorial_zarr\"\ndataset.license = \"CC-BY-4.0\"\ndataset.source = \"https://github.com/polaris-hub/polaris\"\n</pre> # Define the zarr dataset metadata before uploading dataset.name = \"tutorial_zarr\" dataset.license = \"CC-BY-4.0\" dataset.source = \"https://github.com/polaris-hub/polaris\" In\u00a0[23]: Copied! <pre>dataset.upload_to_hub(owner=\"polaris\")\n</pre> dataset.upload_to_hub(owner=\"polaris\") <pre>\u2819 Uploading dataset... </pre> <pre>\u2826 Uploading dataset... </pre> <pre>2024-07-21 13:19:12.188 | INFO     | polaris.hub.client:upload_dataset:602 - Copying Zarr archive to the Hub. This may take a while.\n</pre> <pre>\u2705 SUCCESS: Your dataset has been successfully uploaded to the Hub. View it here: https://polarishub.io/datasets/polaris/tutorial_zarr\n \n</pre> <pre>/mnt/ps/home/CORP/lu.zhu/miniconda3/envs/po_datasets/lib/python3.12/site-packages/yaspin/core.py:228: UserWarning: color, on_color and attrs are not supported when running in jupyter\n  self._color = self._set_color(value) if value else value\n</pre> <p>The End.</p>"},{"location":"tutorials/dataset_zarr.html#pointer-columns","title":"Pointer columns\u00b6","text":"<p>Not all data might fit the tabular format, e.g. images or conformers. In that case, we have pointer columns. Pointer columns do not contain the data itself, but rather store a reference to an external file from which the content can be loaded.</p> <p>For now, we only support <code>.zarr</code> files as references. To learn more about <code>.zarr</code>, visit their documentation. Their tutorial specifically is a good read to better understand the main features.</p>"},{"location":"tutorials/dataset_zarr.html#dummy-example","title":"Dummy example\u00b6","text":"<p>For the sake of simplicity, let's assume we have just two datapoints. We will use this to demonstrate the idea behind pointer columns.</p>"},{"location":"tutorials/dataset_zarr.html#creating-datasets-from-zarr-arrays","title":"Creating datasets from <code>.zarr</code> arrays\u00b6","text":"<p>While the above example works, creating the table with all paths from scratch is time-consuming when datasets get large. Instead, you can also automatically parse a Zarr archive into the expected tabular data structure.</p> <p>A Zarr archive can contain groups and arrays, where each group can again contain groups and arrays. Within Polaris, we expect the root to be a flat hierarchy that contains a single array per column.</p>"},{"location":"tutorials/dataset_zarr.html#a-single-array-for-all-datapoints","title":"A single array for all datapoints\u00b6","text":"<p>Polaris expects a flat zarr hierarchy, with a single array per pointer column:</p> <pre><code>/\n  column_a\n</code></pre> <p>Which will get parsed into a table like:</p> column_a column_a/array#1 column_a/array#2 ... column_a/array#N <p>Note</p> <p>Notice the # suffix in the path, which indicates the index at which the data-point is stored within the big array. </p>"},{"location":"tutorials/dataset_zarr.html#saving-the-dataset","title":"Saving the dataset\u00b6","text":"<p>We can still easily save the dataset. All the pointer columns will be automatically updated.</p>"},{"location":"tutorials/dataset_zarr.html#load-the-dataset","title":"Load the dataset\u00b6","text":""},{"location":"tutorials/dataset_zarr.html#upload-zarr-dataset-to-hub","title":"Upload zarr dataset to Hub\u00b6","text":""},{"location":"tutorials/optimization.html","title":"Optimization","text":"<p>In short</p> <p>This tutorial shows how to optimize a Polaris dataset to improve its efficiency.</p> <p>No magic bullet</p> <p>What works best really depends on the specific dataset you're using and you will benefit from trying out different ways of storing the data.</p> In\u00a0[2]: Copied! <pre>import numpy as np\nimport pandas as pd\n</pre> import numpy as np import pandas as pd In\u00a0[3]: Copied! <pre># Let's create a dummy dataset with two columns\nrng = np.random.default_rng(0)\ncol_a = rng.choice(list(range(100)), 10000)\ncol_b = rng.random(10000)\ntable = pd.DataFrame({\"A\": col_a, \"B\": col_b})\n</pre> # Let's create a dummy dataset with two columns rng = np.random.default_rng(0) col_a = rng.choice(list(range(100)), 10000) col_b = rng.random(10000) table = pd.DataFrame({\"A\": col_a, \"B\": col_b}) <p>By default, Pandas (and NumPy) use the largest dtype available.</p> In\u00a0[4]: Copied! <pre>table.dtypes\n</pre> table.dtypes Out[4]: <pre>A      int64\nB    float64\ndtype: object</pre> In\u00a0[5]: Copied! <pre>table.memory_usage().sum()\n</pre> table.memory_usage().sum() Out[5]: <pre>160132</pre> <p>However, we know that column A only has values between 0 and 99, so we won't need the full <code>int64</code> dtype. The <code>np.int16</code> is already more appropriate!</p> In\u00a0[6]: Copied! <pre>table[\"A\"] = table[\"A\"].astype(np.int16)\ntable.memory_usage().sum()\n</pre> table[\"A\"] = table[\"A\"].astype(np.int16) table.memory_usage().sum() Out[6]: <pre>100132</pre> <p>We managed to reduce the number of bytes by ~60k (or 60KB). That's 37.5% less!</p> <p>Now imagine we would be talking about gigabyte-sized dataset!</p> In\u00a0[7]: Copied! <pre>import os\nimport zarr\nfrom tempfile import mkdtemp\n\ntmpdir = mkdtemp()\n\n# For the ones familiar with Zarr, this is not optimized at all.\n# If you wouldn't want to convert to NumPy, you would want to\n# optimize the chunking / compression.\n\npath = os.path.join(tmpdir, \"data.zarr\")\nroot = zarr.open(path, \"w\")\nroot.array(\"A\", rng.random(10000))\nroot.array(\"B\", rng.random(10000));\n</pre> import os import zarr from tempfile import mkdtemp  tmpdir = mkdtemp()  # For the ones familiar with Zarr, this is not optimized at all. # If you wouldn't want to convert to NumPy, you would want to # optimize the chunking / compression.  path = os.path.join(tmpdir, \"data.zarr\") root = zarr.open(path, \"w\") root.array(\"A\", rng.random(10000)) root.array(\"B\", rng.random(10000)); In\u00a0[8]: Copied! <pre>from polaris.dataset import create_dataset_from_file\n\nroot_path = os.path.join(tmpdir, \"data\", \"data.zarr\")\ndataset = create_dataset_from_file(path, zarr_root_path=root_path)\n</pre> from polaris.dataset import create_dataset_from_file  root_path = os.path.join(tmpdir, \"data\", \"data.zarr\") dataset = create_dataset_from_file(path, zarr_root_path=root_path) In\u00a0[9]: Copied! <pre>from polaris.dataset import Subset\n\nsubset = Subset(dataset, np.arange(len(dataset)), \"A\", \"B\")\n</pre> from polaris.dataset import Subset  subset = Subset(dataset, np.arange(len(dataset)), \"A\", \"B\") <p>For the sake of this example, we will use PyTorch.</p> In\u00a0[10]: Copied! <pre>from torch.utils.data import DataLoader\n\ndataloader = DataLoader(subset, batch_size=64, shuffle=True)\n</pre> from torch.utils.data import DataLoader  dataloader = DataLoader(subset, batch_size=64, shuffle=True) <p>Let's see how fast this is!</p> In\u00a0[11]: Copied! <pre>%%timeit\nfor batch in dataloader:\n    pass\n</pre> %%timeit for batch in dataloader:     pass <pre>1.45 s \u00b1 22 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</pre> <p>That's pretty slow... Let's see if Polaris its optimization helps.</p> In\u00a0[12]: Copied! <pre>dataset.load_to_memory()\n</pre> dataset.load_to_memory() In\u00a0[13]: Copied! <pre>%%timeit\nfor batch in dataloader:\n    pass\n</pre> %%timeit for batch in dataloader:     pass <pre>99.4 ms \u00b1 2.45 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</pre> <p>That's a lot faster!</p> <p>Now all that's left to do, is to clean up the temporary directory.</p> In\u00a0[14]: Copied! <pre>from shutil import rmtree\n\nrmtree(tmpdir)\n</pre> from shutil import rmtree  rmtree(tmpdir) <p>The End.</p>"},{"location":"tutorials/optimization.html#datasets-that-fit-in-memory","title":"Datasets that fit in memory\u00b6","text":"<p>Through the Polaris <code>Subset</code> class, we aim to provide a general purpose data loader that serves as a good default for a variety of use cases.</p> <p>As a dataset creator, it is important to be mindful of some design decisions you can make to improve performance for your downstream users. These design decisions are most impactful!</p> <p>As a dataset user, we provide the <code>Dataset.load_to_memory()</code> method to load the uncompressed dataset into memory. This is limited though, because there is only so much we can do automatically without risking data integrity.</p> <p>Despite our best efforts to provide a data loader that is as efficient as possible, you will always be able to optimize things further for a specific use case if needed.</p>"},{"location":"tutorials/optimization.html#without-zarr","title":"Without Zarr\u00b6","text":"<p>Without pointer columns, the best way to optimize your dataset's performance is by making sure you use the appropriate dtype. A smaller memory footprint not only reduces storage requirements, but also speeds up moving data around (e.g. to the GPU or to create <code>torch.Tensor</code> objects).</p>"},{"location":"tutorials/optimization.html#with-zarr","title":"With Zarr\u00b6","text":"<p>If part of the dataset is stored in a Zarr archive - and that Zarr archive fits in memory (remember to optimize the <code>dtype</code>) - the most efficient thing to do is to just convert from Zarr to a NumPy array. Zarr is not built to support this use case specifically and NumPy is optimized for it. For more information, see e.g. this Github issue.</p> <p>Luckily, you don't have to do this yourself. You can use Polaris its <code>Dataset.load_to_memory()</code>.</p> <p>Let's again start by creating a dummy dataset!</p>"},{"location":"tutorials/optimization.html#datasets-that-fit-on-a-local-disk","title":"Datasets that fit on a local disk\u00b6","text":"<p>For datasets that don't fit in memory, but that can be stored on a local disk, the most impactful design decision is how the dataset is chunked.</p> <p>Zarr datasets are chunked. When you try to load one piece of data, the entire chunk that data is part of has to be loaded into memory and decompressed. Remember that in ML, data access is typically random, which is a terrible access pattern because you are likely to reload chunks into memory.</p> <p>Most efficient is thus to chunk the data such that each chunk only contains a single data point.</p> <ul> <li>Benefit: No longer induce a performance penalty due to loading additional data into memory that it might not need.</li> <li>Downside: You might be able to compress the data more if you can consider similarities across data points while compressing.</li> </ul> <p>A note on rechunking: Within Polaris, you do not have control over how a dataset on the Hub is chunked. In that case, rechunking is needed. This can induce a one-time, but nevertheless big performance penalty (see also the Zarr docs). I don\u2019t expect this to be an issue in the short-term given the size of the dataset we will be working with, but Zarr recommends using the rechunker Python package to improve performance.</p>"},{"location":"tutorials/optimization.html#remote-datasets","title":"Remote Datasets\u00b6","text":"<p>In this case, you really benefit from improving memory storage by trying different compressors.</p> <p>See also this article.</p>"}]}